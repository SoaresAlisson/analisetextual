[["objetivos-deste-manual.html", "Introdução Análise Textual aplicada à Sociologia Objetivos deste manual Plano do livro", " Introdução Análise Textual aplicada à Sociologia Alisson Soares Primeira versão em 22 de abril de 2021. Última atualização em 16 maio de 2021 Objetivos deste manual Sou Alisson Soares, sociólogo, e comecei há pouco tempo no campo das humanidades digitais. Achei por bem montar este manual como uma forma de compartilhar o que venho aprendendo. Algumas ferramentas, softwares serão de modo gráfico, mas por vezes vamos recorrer à programação. A principal linguagem aqui será o R, mas o uso de Python não está completamente excluído. O objetivo deste manual é: Ser um livro em progresso constante. Ser introdutório, mas também sempre indicar bons materiais para maior aprofundamento dos temas tratados. Sempre que possível, oferecer material em português, apesar de nem sempre isto ser possível. Oferecer uma introdução às ferramentas computacionais utilizadas em pesquisa das ciências sociais. Usar preferencialmente ferramentas gratuitas e de código aberto. Sempre que possível, apresentar exemplos de pesquisa e ciência social utilizando estas ferramentas. Plano do livro Os capítulos planejados até o momento, muito já em elaboração: História da Análise Textual Rápida introdução à programação (conceitos básicos como variável, loops, condicionais if-else, função, etc.) Normalização e expressões regulares. Rápida introdução ao R (tipos de dados, estrutura de dados, importação de dados) Introdução à análise textual via computador Frequência de termos (bag of words, n-grams, skipgrams, TF-IDF) dendogramas Correlação, tipos de distâncias Introdução à análise de redes. Inteligência Artificial: clusterização; topic modelling Análise de sentimentos. "],["intro.html", "Introdução O termo “humanidade digitais”", " Introdução “the best digital theory-building of the past decade stems from social and computational origins. As such, it is increasingly apparent that digital sociologists need to develop a computational as well as a sociological imagination.” O termo “humanidade digitais” Segundo um estudo do LinkedIn em 15 países, prevendo 150 milhões de vagas nos próximos 5 anos, das áreas de trabalhos digitais em alta e 2021, duas se relacionam a Humanidades Digitais: Big data Analytics (segundo lugar) e “Text, image and voice processing” (6º lugar). Profissionalmente, pode ser interessante entrar nesta área. Existem diversos termos correlatos a “Humanidades digitais” (“Digital humanities”), como “E-humanities,”Cultural Analytics“,”ciências sociais computacionais“,”sociologia digital“,”história digital“,”etnografia digital“,”etnografia quantitativa“,”Análise cultural quantitativa\" (quantitative cultural analysis) “cultural analytics”, “humanities data science”, “humanities data analysis” “humanities computing”, “distant reading”, “computational social science”, etc. Humanidades digitais inclui a análise de: sons/música, imagens. textos. Nosso objetivo aqui valores &lt;- c(12, 34, 13) # atribuindo nomes ao vetor acima nomes &lt;- c(&quot;banana&quot;, &quot;uva&quot;, &quot;abacate&quot;) # criando o vetor names(valores) &lt;- nomes names ## function (x) .Primitive(&quot;names&quot;) "],["história-da-análise-textual.html", "Capítulo 1 História da Análise Textual 1.1 Linha do tempo da história da Análise Textual", " Capítulo 1 História da Análise Textual “May we hope that when things come to such a crisis, human labor of the literary sort may be in part superseded by machinery? Machinery has done wonders, and when we think of what literature is becoming, it is certainly to be wished that we could read it by machinery, and by machinery digest it” (Andrew Stauffer In London’s Daily News. 15 de Setembro de 1869 apud Catherine DeRose.) A análise textual abarca campos do conhecimento bem variados, como psicologia, ciências da computação, ciências da informação, linguística, ciência política, sociologia, etc. Apresentamos aqui uma linha do tempo com alguns dos principais eventos relacionados à análise textual, bem como ao seu uso com computadores. De modo resumido, podemos pensar nos primeiros desenvolvimentos ao final do século XIX, a introdução do computador e mais recentemente, a introdução da inteligência artificial como pontos marcantes nesta cronologia. Fizemos aqui uma breve cronologia, que não pretende ser extensiva, com todos eventos importantes, mas apenas demarcar alguns pontos interessantes, para ajudar a dar alguma ideia àqueles que iniciam no campo das humanidades digitais e na sociologia digital. 1.1 Linha do tempo da história da Análise Textual 1887 Medenhall. Analisa o comprimento de palavras: MENDENHALL, T. C. . The characteristic curves of composition. Science.Vol ns-9, Issue 214S. 11 March 1887. baixar pdf Wincenty Lutoslawski. Análise de palavras raras na obra de Platão. Cunhou o termo “estilometria” Lutoslawski’s Origin and Growth of Plato’s Logic - The Origin and Growth of Plato’s Logic, 1888 Benjamin Bourdon (1860-1943, psicólogo e professor da Université de Rennes): Ao pesquisar sobre a expressão de emoções através de palavras, analisou o livro “Exodus” da Bíblia e calculou frequências, classificou e eliminou as stopwords. “In 1888, in a research on the expression of emotions through words, Benjamin Bourdon analysed the Exodus of the Bible and calculated the frequencies by rearranging and classifying them, eliminating the stop words” fonte. 1888 Friedrich Kaeding (1855 - 1929), cria índices de frequência para estruturação de sistemas estenográficos (sistema de escrita por abreviações para que a escrita seja tão rápida como a fala). 1934 Harold Laswell (1902-1978, cientista político) produz a primeira contagem de palavras chave. 1934 Vygostky produz a primeira análise quantitativa de narrativa 1949 Robert Busa (padre jesuíta) junto à IBM com o projeto Index Thomisticus, que levou 34 anos, envolveu cerca de 70 pessoas, sobre as obras de São Tomás de Aquino. “The IBM… considered this first enterprise of using a computer for linguistic and lexicographic goals as a pilot-project” fonte, indexando mais de 10 milhões de palavras. Busa, R. (1980). “The Annals of Humanities Computing: The Index Thomisticus”. Computers and the Humanities. 14 (2): 83–90. doi:10.1007/BF02403798. ISSN 0010-4817. ROCKWELL, Geoffrey; PASSAROTTI, Marco (2019-05-27). “The Index Thomisticus as a Big Data Project”. Umanistica Digitale (5). doi:10.6092/issn.2532-8816/8575. 1950 Gottschalk usa Content Analysis para rastrear temas freudianos. GOTTSCHALK, Louis A. The Measurement of Psychological States Through the Content Analysis of Verbal Behavior. University of California Press. 1969. 317p. Gottschalk-Gleser Content Analysis Method of Measuring the Magnitude of Psychological Dimensions 1950 Alan Turing aplica Inteligência Artificial a textos. 1952 Bereleson publica o primeiro manual de análise de conteúdo. BERELSON, B. (1952). Content analysis in communication research. New York: Hafner. 1954 Primeira tradução automática de texto (Georgetown–IBM experiment) do russo ao inglês. HUTCHINS, John. The first public demonstration of machine translation: the Georgetown-IBM system, 7th January 1954. 2006. Press Release da IBM 1963 Mosteller e Wallace analisam a autoria dos Federalist Papers. MOSTELLER, F., WALLACE, D. L.. 1963. Inference in an authorship problem. Journal of the American Statistical Association 58:275–309. 1965 Tomashevsky formaliza análise quantitativa de narrativa. TOMASHEVSKY, B. (1965). Thematics. In L T. Lemon &amp; M. I. Reis (Eds. &amp; Trans.), Russian formalist criticism: Four essays (pp. 61-95). Lincoln: University of Nebraska Press. (Original de 1925) 1966 Stones e Bales usam computador para medir propriedades psicométricas de textos na RAND. 1980 Declínio do formalismo chomskyano; nascimento do Processamento de Linguagem Natural (PLN). 1980 Aplicação de Aprendizado de máquinas (Machine Learning) ao Processamento de Linguagem Natural 1981 Walter Weintraub e a contagem de parts-of-speech. WEINTRAUB, Walter. Verbal Behavior: adaptation and psychopathology. Springer:NY. 1981. SOBEL, Dava. Language patterns reveal problems in personality. NYT. Oct. 27,1981. 1985 Schrodt introduz codificação automática de eventos (Automated Event Coding). SCHRODT, Philip A. Automated Coding Of International Event Data Using Sparse Parsing Techniques. 2000. 1986 James W. Pennebaker desenvolve LIWC (Linguistic Inquiry and Word Count). 1989 Roberto Franzosi (perfil no Research Gate) (sociólogo) traz a análise de narrativa quantitativa (quantitative Narrative Analysis) para as Ciências Sociais. 1998 Primeiro desenvolvimento de Topic Models. 1998 John W Mohr conduz a primeira análise quantitativa de visões de mundo. 1999 Peter Bearman (sociólogo) et al. aplicam métodos de rede a narrativas “Narrative network”. 2001 David M. Blei et al desenvolvem a LDA (Latent Dirichlet Allocation). David M. Blei, Andrew Y. Ng, Michael I. Jordan. Latent Dirichlet Allocation. Journal of Machine Learning Research 3 (2003) 993-1022. github Blei Lab. 2003 MALLET (MAchine Learning for LanguagE Toolkit), um dos primeiros sistemas de topic models, é criado. “MALLET is a Java-based package for statistical natural language processing, document classification, clustering, topic modeling, information extraction, and other machine learning applications to text. MALLET includes sophisticated tools for document classification: efficient routines for converting text to”features“, a wide variety of algorithms (including Naïve Bayes, Maximum Entropy, and Decision Trees), and code for evaluating classifier performance using several commonly used metrics.” 2005 Quin et al analisam discursos políticos usando topic models. Kevin M Quinn, Burt L Monroe, Michael Colaresi,Michael H Crespin, and Dragomir R Radev. 2010. How to analyze political attention with minimal assumptions and costs. American Journal of Political Science54(1):209–228 2010 Gary King e Daniel Hopkins trazem Topic Models ao mainstream. Hopkins, Daniel, and Gary King. 2010. “A Method of Auto-mated Nonparametric Content Analysis for Social Science.”American Journal of Political Science54(1): 229–47. Fonte: Versão ampliada, baseado parcialmente em: “SICSS 2018 - History of Quantitative Text Analysis” slides, video. Pretende-se posteriormente expandir esta seção, explicando em mais detalhes alguns dos exemplos acima "],["exemplos-de-pesquisas-em-humanidades-digitais.html", "Capítulo 2 Exemplos de Pesquisas em Humanidades Digitais 2.1 Bibliometria / cientometria / cienciometria 2.2 Exemplo: Google Trends como Proxy para epidemias 2.3 Exemplo: Como as fake news sobre a pandemia de Covid-19 se assemelham/divergem entre os países? 2.4 Exemplo: Mudança de significado de palavras 2.5 Exemplo: Análise de complexidade musical 2.6 Exemplo: Polarização 2.7 Exemplo de integração quali-quanti: Complementando dados qualitativos 2.8 Exemplo de integração quali-quanti: Depurando dados quantitativos", " Capítulo 2 Exemplos de Pesquisas em Humanidades Digitais Para entender os potenciais das humanidades digitais para pesquisa, nada melhor que observarmos exemplos de pesquisas. Aqui seguem alguns de exemplos, selecionados pelo potencial de integração entre humanidades e métodos digitais, mais que quanto ao possível mérito/demérito científico. 2.1 Bibliometria / cientometria / cienciometria O estudo das citações, das rede de citações em artigos científicos foi talvez um dos pioneiros no uso de algumas das técnicas aqui descritas, existindo já há décadas. Chamada de “bibliometria”, “cientometria” ou “cienciometria”, ela conta citações de determinados autores em artigos científicos e tenta avaliar o quão influentes estes são. 2.1.1 Exemplo: Filósofos da ciência na Sociologia Por exemplo, HEDSTRÖM et al (1998), buscando saber a influência dos principais filósofos da ciência (Hempel, Kuhn, Popper e Wittgenstein) na sociologia em diferentes países e regiões (países nórdicos, EUA, Grã Bretanha, Alemanha e França), analisou o número de artigos nas principais revistas sociológicas que os citaram. (Fonte: HEDSTRÖM et al. 1998. p. 343) Pelos dados ali apresentados, Popper seria o filósofo mais influente na Europa, principalmente nos países de língua alemã, ao passo que Kuhn seria mais dominante nos EUA. HEDSTRÖM, Peter; SWEDBERG; and UDÉHN, Lars. Popper’s Situational Analysis and Contemporary Sociology. Philosophy of the Social Sciences 1998; 28; 339-64] 2.1.2 Exemplo: A teoria dos sistemas sociais de Niklas Luhmann Stephen Roth analisou a chamada diferenciação funcional dos subsistemas da sociedade mundial, isto é, como os subssitemas como política, economia, religião, ciência, direito, meios de comunicação de massa, etc. se autonomizam em relação aos outros, entre os anos de 1800 e 2000, e para tal utilizou dados do Google Ngram viewer, que por sua vez se baseia no Google Books (mais detalhes sobre estas ferramentes na seção sobre frequência de palavras). Ele encontrou, por exemplo, declínio da presença relativa (isto é, proporcional a cada ano) da palavra “Deus” (god) nos livros em inglês ao longo do tempo. Fonte: Roth (2014, p.46). E se examinarmos os termos relacionados aos Meios de Comunicação de Massa, podemos ver a importância relativa do termo “imprensa” (press) aumentando ao longo do tempo. Fonte: Roth(2014, p.47). Roth, Steffen. 2014. “Fashionable Functions: A Google Ngram View of Trends in Functional Differentiation (1800-2000)” International Journal of Technology and Human Interaction, 34–58. Uma versão posterior, um pouco mais elaborada em: Steffen Roth, Carlton Clark, Nikolay Trofimov, Artur Mkrtichyan, Markus Heidingsfelder, Laura Appignanesi, Miguel Pérez-Valls, Jan Berkel, Jari Kaivo-oja. Futures of a distributed memory. A global brain wave measurement (1800–2000). Technological Forecasting &amp; Social Change 118 (2017) 307–323 2.1.3 Exemplo: Tendência de termos chave da Sociologia Além de contar citação nas referências, pode-se contar as palavras mais frequentes no corpo do texto e compará-las. Bernau (2018), por exemplo, coletou dados do JSTOR’s Data for Research e plotou um gráfico longitudinal (ao longo do tempo) de frequência de palavas com termos chaves da sociologia, como “classe”, “raça” e “gênero” da revista American Sociological Review. Ele também disponibilizou o script em R que desenvolveu para esta análise. BERNAU, John A. Text Analysis with JSTOR Archives. November 12, 2018. https://doi.org/10.1177/2378023118809264 2.1.4 Exemplo: Tendências da filosofia Inspirado neste trabalho, Brian Weatherson usou também o pacote jstor_dfr, baixou dados da filosofia, e os clusterizou através de topic modeling. Podemos ver as tendências gerais de vários ramos da filosofia ao longo do tempo: Tendencias na história da filosofia O resultado e mais informações estão em seu livro online: WEATHERSON, Brian . A History of Philosophy Journals. Volume 1: Evidence from Topic Modeling, 1876-2013) 2.2 Exemplo: Google Trends como Proxy para epidemias A busca no Google por certos sintomas de doenças, ou melhor, a variação na busca por certas doenças e sintomas correlatos pode indicar que variação real da doença. Isso acontece, por exemplo, com sintomas de gripe. Um pico no aumento das buscas pelos sintomas indica um prenúncio do aumento das infecções, a ser checado/validado posteriormente. No artigo Google Trends: A Web-Based Tool for Real-Time Surveillance of Disease Outbreaks., os autores explicam que a ferramenta lançada em 2018: &gt; “Google Flu Trends can detect regional outbreaks of influenza 7–10 days before conventional Centers for Disease Control and Prevention surveillance systems” Para funcionar, há certas pré-condições sociais. Vale para gripe, e a ferramenta também prevê aumentos da Covid-19. Na reportagem da Piaui “No carnaval, buscas por “sintomas covid” voltaram a subir; sete dias depois, número de novos casos bateu recorde” de 09 de março de 2021 compara as buscas no google com casos reais. Gráfico: buscas por “sintomas Covid no Google” versus casos reais ver também “Sintomas Covid” en Google trends:.Un indicador alternativo para el seguimiento de la incidencia de casos. com exemplos da Espanha, México, Chile e Argentina. No entanto, a ferramenta que parecia promissora falhou em prever o pico de gripe de 2013, sobrestimando por 140%. A empresa achou melhor terminar o projeto, conforme um artigo da Wired de 2015. Mais detalhes podem lidos no artigo The Parable of Google Flu: Traps in Big Data Analysis. O Instituto de Ciências Cognitvas de Osnabrück leva a ideia adiante, com modelo mais complexo, utilizando dados de redes sociais como Twitter e através do Watson da IBM.(site do projeto). 2.3 Exemplo: Como as fake news sobre a pandemia de Covid-19 se assemelham/divergem entre os países? O relatório de 23/11/2020 sobre isolamento científico, intitulado “scientific [self] isolation” do Laut, Centro de Análise da Liberdade e do Autoritarismo, cruzou as checagens de fake news de 129 países diferentes (há uma plataforma que traduz as reportagens de fact checking de todo mundo para o inglês) e investigou a distribuição de notícias falsas sobre os tratamentos da Covid19. Os pesquisadores procederam então uma distribuição num plano das discussões nos países conforme sua semelhança. Quanto mais próximos, mais semelhantes os debates ao redor do tema. Encontraram então que o Brasil é o país mais isolado em sua discussão envolvendo certos medicamentos, no canto superior direito. grafico 2.4 Exemplo: Mudança de significado de palavras Kulkarni et al (2015) mostraram como através de ferramentas computacionais é possível identificar a mudança de significado de termos, seja ao longo de um século (com dados do Google NGram), seja em dinâmicas mais rápidas, como no twitter. Um dos termos analisado foi o “gay”: Linguistic Change da palavra “gay” Várias outras palavras foram analisadas, como “tape” que significava “fita adesiva”, mas passou a significar também “fita cassete” nos anos 1970; ou “apple” e “windows” que ganharam novo significado com a indústria da computação. KULKARNI, V., Al-Rfou, R; PEROZZI, B e SKIENA, S. Statistically Significant Detection of Linguistic Change. WWW 2015, May 18–22, 2015. http://dx.doi.org/10.1145/2736277.2741627 . 2.5 Exemplo: Análise de complexidade musical A reportagem da Folha de São Paulo Música brasileira foi simplificada ao longo das décadas, diz pesquisa cita o trabalho do cientista de dados Leonardo Sales (blog do autor), que analisou os acordes e vocabulário das letras, com base em 44 mil cifras e 102 mil letras raspadas de sites como cifras.com.br e letras.com.br em uma série de postagens: parte 1, parte 2, parte 3 sobre as letras, parte 4. Os códigos para raspagem de dados destes sites estão disponíveis em Python. grafico Como quase toda notação, há vantagens e desvantagens. Uma desvantagem, que levantou críticas, é que a análise se baseia em cifra, muito utilizada em músicas mais populares, mas inadequada para estilos mais complexos, como jazz. 2.6 Exemplo: Polarização Uma boa parte de pesquisas em política com métodos digitais se dedicou a analisar o fenômeno da polarização política. Christopher A. Bail, Lisa P. Argyle, Taylor W. Brown, John P. Bumpus, Haohan Chen, M. B. Fallin Hunzaker, Jaemin Lee, Marcus Mann, Friedolin Merhout, and Alexander Volfovsky. 2018. Exposure to opposing views on social media can increase political polarization. Proceedings of the National Academy of Sciences 115, 37 (Sept. 2018), 9216–9221. https://doi.org/10.1073/pnas.1804840115 Publisher: National Academy of Sciences Section: Social Sciences. Parte do trabalho de Franzosi descrevemos na seção sobre dados estruturados. Roberto P. Franzosi. Sociology, narrative, and the quality versus quantity debate (Goethe versus Newton): Can computer-assisted story grammars help us understand the rise of Italian fascism (1919–1922)? . Theor Soc (2010) 39:593–629 DOI 10.1007/s11186-010-9131-3 2.7 Exemplo de integração quali-quanti: Complementando dados qualitativos Uma dica de integração quali-quanti (qualitativo e quantitativo) usando análise textual vem do PEW Research. Aqui explicam como a partir de uma análise de grupos focais feita em 2019 com grupos dos EUA e da Grã Bretanha sobre atitudes frente a globalização/nacionalismo, complementaram com pesquisa quantitativa de análise textual, analisando as diferenças entre os grupos de cada país. Usaram técnicas como frequência de palavras, correlação de palavras e Topic modelling. Através destas análises, encontraram tópicos que se mostraram relevantes a serem incorporados em surveys futuros. DEVLIN, Kat.“How quantitative methods can supplement a qualitative approach when working with focus groups”. medium. Dec 18, 2020. 2.8 Exemplo de integração quali-quanti: Depurando dados quantitativos Nem sempre uma tabela de dados estruturados tem tudo estruturado, do modo que a sua pergunta de pesquisa necessite. Dentro de um campo específico de uma tabela pode-se precisar de desmembrar ainda mais os dados. Eis aqui um exemplo. Os pesquisadores do Dadoscope queriam investigar se houve aumento na abertura de igrejas evangélicas durante os anos Lula e Dilma. Os pesquisadores baixaram dados da Receita Federal referente ao Cadastro Nacional de Pessoa Jurídica e filtraram por “94.91–0–00 — Atividades de organizações religiosas ou filosóficas” no campo “Classificação Nacional de Atividades Econômicas”. O problema é que isto agrega não só igrejas evangélicas, como também católicas e de outras religiões e até agremiações filosóficas e institutos de psicanálise. Aqui entra a integração: Para tentarmos realizar a classificação das 150 mil igrejas evangélicas de maneira semi-supervisionada nós usamos Snorkel, uma biblioteca escrita em Python… foi preciso treinar um algoritmo de classificação usando uma amostra dos dados. De forma sucinta, os dados são separados em amostras que são usadas para treino, teste e validação da classificação. Para classificar os mais de 150 mil nomes únicos presentes na amostra de treino, criamos funções que classificam de forma grosseira as igrejas (e.g., se a palavra “assembleia” estiver presente, classificar a igreja como “evangélica”). Depois de escrever dezenas dessas funções, comparamos sua acurácia com uma amostra de teste de 5% dos nomes únicos, manualmente classificados por dois pesquisadores. Feito isso, usamos uma rede neural que combina em camadas estas funções e voilá: 91.8% de acurácia. Sabemos que este resultado não é perfeito, mas ele torna o trabalho de classificação viável. O artigo completo “Exclusivo: Igrejas evangélicas pentecostais tiveram boom de crescimento nos governos Lula e Dilma” pode ser lido aqui: artigo na Forum e o mesmo artigo na Medium. Caderno de notas no dadoscope para entender melhor como o processo foi realizado. Sobre a ferramenta utilizada, a Snorkel, ver a página do Github, introdução (em inglês) ao Snorkel. Por fim, outra dica para pensar a integração de dados quantitativos e qualitativos é a palestra de Dr. Christof Schöch: The Convergence of Quantitative and Qualitative Approaches, ocorrida no 1st Summerschool of Digital Humanities: Distant Reading - Potentials and Applications, em inglês. "],["organização-dos-dados-quanto-a-sua-estrutura.html", "Capítulo 3 Organização dos dados quanto a sua estrutura 3.1 Dados estruturados 3.2 Dados não estruturados 3.3 Dados semi-estruturados", " Capítulo 3 Organização dos dados quanto a sua estrutura Objetivos do capítulo: 1) Apresentar a distinção entre dados estruturados, dados não estruturados e dados semi estruturados 2) Apresentar alguns formatos de arquivos frequentes na análise textual. Podemos pensar a organização de dados quanto à sua estrutura de três formas: dados estruturados, dados semi estruturados e dados não estruturados. 3.1 Dados estruturados Formatos de arquivos estruturados são csv,xml, json, xls, xlsx, etc. Muitos destes possuem formato de tabela, o que torna bastante fácil encontrar a informação buscada. 3.1.1 Os formatos csv (comma separeted values) e tsv. O formato csv (comma separeted values ou “valores separados por vírgula”) é um dos mais simples, consiste de arquivo de texto simples, com valores separados por um caractere (ou conjunto de caracteres) que separam os valores em cada linha, sendo geralmente vírgula ou ponto e vírgula ou tabulação (tecla tab). Qualquer caractere ou conjunto de caracteres pode ser usado como separador de campos. Na imensa maioria dos casos cada linha é separada pela quebra de linha. Por exemplo, a seguinte tabela: Estado sigla capital região Acre AC Rio Branco Norte Alagoas AL Maceió Nordeste Amapá AP Macapá Norte Amazonas AM Manaus Norte Bahia BA Salvador Nordeste Ceará CE Fortaleza Nordeste Em abrirmos o csv no bloco de notas (notepad): Estado;sigla;capital;região; Acre;AC;Rio Branco;Norte; Alagoas;AL;Maceió;Nordeste; Amapá;AP;Macapá;Norte; Amazonas;AM;Manaus;Norte; Bahia;BA;Salvador;Nordeste; Ceará;CE;Fortaleza;Nordeste; O separador de campo neste arquivo CSV é o ponto e vírgula ;. Ao pedirmos ao computador para localizar qual a designação da sigla “AP”, ele saberá buscar facilmente esta informação. No caso ali, a vírgula é o separador de campos, mas qualquer outro caractere pode ser usado como separador. O formato .tsv, por exemplo, é separado por tabulação - ou o símbolo \\t. Mas é possível encontrar arquivo csv, porém com separador tipo “ ou”;\". 3.2 Dados não estruturados Os dados não estruturados são a forma como encontramos em livros impressos, artigos, são a forma como humanos lêem textos. “Algum tempo hesitei se devia abrir estas memorias pelo principio ou pelo fim, isto é, se poria em primeiro logar o meu nascimento ou a minha morte. Supposto o uso vulgar seja começar pelo nascimento, duas considerações me levaram a adoptar differente methodo: a primeira é que eu não sou propriamente um autor defunto, mas um defunto autor, para quem a campa foi outro berço; a segunda é que o escripto ficaria assim mais galante e mais novo. Moysés, que tambem contou a sua morte, não a poz no introito, mas no cabo: differença radical entre este livro e o Pentateuco….” Este tipo de texto, não estruturado, é alvo do Processamento de linguagem natural (PLN)/ Natural Language Process (NLP) 3.3 Dados semi-estruturados Dados semi-estruturados são um meio termo entre os estruturados e os semi estruturados. Por vezes são chamados de “auto-descritivos”. 3.3.1 Exemplos de dados semi-estruturados 3.3.1.1 O formato Json O Json (“JavaScript Object Notation”, isto é “Notação de Objetos JavaScript”), é organizado no esquema de pares nome/valor. Por exemplo, ao separarmos primeiro nome firstName de sobrenome lastName no Json: {&quot;employees&quot;:[ { &quot;firstName&quot;:&quot;João&quot;, &quot;lastName&quot;:&quot;da Silva&quot; }, { &quot;firstName&quot;:&quot;Ana&quot;, &quot;lastName&quot;:&quot;Maria&quot; }, { &quot;firstName&quot;:&quot;Joaquim&quot;, &quot;lastName&quot;:&quot;Xavier&quot; } ]} O arquivo json inicia e termina com colchetes [] Todo Json é delimitado por chaves {}, os dados são representados no esquema nome/valor `\"nome\": \"valor\". estes são separados por vírgula. Caso queira mais detalhes sobre este formato: Um video introdutório sobre o formato Json do canal Código Fonte TV JSON // Dicionário do Programador. Um video introdutório mais prático sobre Json, JSON em 6 minutos do canal “Canal TI”. Para ver as regras de sintaxe do Json. 3.3.1.2 Markup Códigos especiais, ou linguagem “markup” é uma notação de documento que tem duas apresentações, uma simplificada como texto normal para humanos, e outra com os “markup” para que o computador entenda. 3.3.1.2.1 O formato Markdown Um exemplo bem simples de markup é o Mardown, usado na escrita rápida de textos. Exemplo de markdown 3.3.1.3 O formato YAML O YAML é um padrão de serialização de dados que prima por ser “human friendly”, isto é, de fácil leittura também para humanos. Em arquivos markdown tem-se usado o yaml como cabeçalho, com informações para a renderização do pdf, como título, subtítulo, resumo, palavras chave, etc. Ao converter markdown para o formato final, o computador irá interpretar estas informações. Um exemplo de yaml no arquivo markdown: --- title: &quot;Título do meu pdf&quot; subtitle: subtitulo qualquer author: Fulano de Tal # comentário qualquer fontsize: 12pt urlcolor: blue geometry: margin=2.5cm abstract: &gt; meu resumo bla bla bla bla --- # Titulo Texto texto texto texto texto texto texto ## Subtitulo Texto texto texto texto texto texto texto O cabeçalho em yaml é delimitado no seu início e fim por três traços consecutivos ---. Repare que o símbolo tralha # dentro do yml é interepretado como comentário, já no markdown, indica capítulo. 3.3.1.4 O Formato LaTex O LaTex é uma linguagem usada na confecção, principalmente de textos (livros, artigos) acadêmicos, bem como apresentações. O formato LaTex permite grande flexibilidade, e é muito usado para escrever fórmulas matemáticas e gerar as referências bibliográficas automaticamente. Por isso, o LaTex é muito usado no contexto acadêmico. O seu formato mínimo pode ser visto assim: \\documentclass{article} \\begin{document} Olá Mundo \\end{document} Exemplo simples de texto em LaTex e sua renderização Ou em um exemplo um pouco mais elaborado: Exemplo de LaTex com o software Gummi no Linux Perceba que antes de \\begin{document}, isto é, no cabeçalho do documento temos várias informações, entre elas o título do artigo na linha 5 em title{}, e em \\author{}, nas linhas de 6 a 8, temos os autores. Temos também delimitados os capítulos ou seções, no caso ali em section{}. 3.3.1.5 O formato BibTex Um formato “irmão” do LaTex e markdown é o BibTex, um formato estruturado, com dados bibliográficos usado como fonte para gerar automaticamente a bibliografia ao final do texto renderizado em formatos como Tex, LaTex e markdown. As referências nesse formato ficam salvos num grande arquivo .bib. Um exemplo de citação dentro do bib: @book{Coleman:IntroMathSociology, address = {New York}, pages = {570}, publisher = {The Free Press of Glencoe Collie, Macmillan Limited}, title = {Introduction to Mathematical Sociology}, year = {1964} } O @book indica o tipo, podendo ser também, por exemplo, @article para artigos, @inbook para parte de um livro, @phdthesis para tese de phd (há mais opções). Para uma lista completa, ver The 14 BibTeX entry types. O Coleman:IntroMathSociology é o ID, a identificação única, que é também usado na citação do LaTex (Por exemplo, usando \\cite{Coleman:IntroMathSociology} dentro do Tex) ou do Markdown (usando [@IntroMathSociology] dentro do texto) para que o compilador saiba qual texto está sendo citado no texto. Exemplo de citação usando bibtex no LaTex Este arquivo bib que contém as referências bibliográficas, como é texto puro, pode ser editado num editor de texto comum, como o notepad, Gedit, etc. Mas o mais indicado é usar um software gestor de bibliografia, como o JabRef ou o KBibTex. O KBibTex possui menos recursos que o JabRef mas dá plenamente conta do recado, sendo inclusive o gerenciador que utilizo. Além de ser usado para gerar pdfs com as referências, o formato também pode ser usado em pesquisas bibliométricas. 3.3.1.6 Os formatos xml e html No caso, nome seria “FirstName” e seu valor seria “João”, nome seria “lastName” e seu valor “da Silva” E esses mesmos dados no formato xml: &lt;employees&gt; &lt;employee&gt; &lt;firstName&gt;João&lt;/firstName&gt; &lt;lastName&gt;da Silva&lt;/lastName&gt; &lt;/employee&gt; &lt;employee&gt; &lt;firstName&gt;Ana&lt;/firstName&gt; &lt;lastName&gt;Maria&lt;/lastName&gt; &lt;/employee&gt; &lt;employee&gt; &lt;firstName&gt;Joaquim&lt;/firstName&gt; &lt;lastName&gt;Xavier&lt;/lastName&gt; &lt;/employee&gt; &lt;/employees&gt; Algumas linguagens usadas em texto são chamadas de markup, onde o que é mostrado na tela, para humanos lerem, difere do que o computador “entende”. Exemplo é o xml acima, o html ou ainda linguagens como markdown e LaTex. O html tem por base o xml. O html possui basicamente a seguinte estrutura &lt;!doctype html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Titulo da pagina&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;Título do capítulo&lt;/h1&gt; &lt;p&gt;Texto texto texto&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; Se você copiar o conteúdo acima e salvar num arquivo com o nome, digamos teste.html e abrí-lo com seu navegador de internet (Firefox, Chrome, Opera, etc.), verá como funciona esta ideia de markup. No caso do html, os valores são delimitados por tags, por exemplo: &gt; &lt;ALGO&gt;conteudo&lt;/ALGO&gt; onde &lt;/ indica que estamos fechando a tag. Assim: &lt;h1&gt;Título do capítulo&lt;/h1&gt; O texto “Título do capítulo” está entre a tag “h1”. Temos duas partes no html: Entre &lt;head&gt; e seu fechamento, &lt;/head&gt; ficam os metadados, como título, data, etc. Entre &lt;body&gt; e &lt;/body&gt; fica o conteúdo da página que aparece dentro do navegador. Um outro exemplo: &lt;name&gt;Joaquim José da Silva Xavier&lt;/name&gt;, o Tiradentes (&lt;local&gt;Fazenda do Pombal&lt;/local&gt;, batizado em &lt;data&gt;12 de novembro de 1746&lt;/data&gt; — &lt;local&gt;Rio de Janeiro&lt;/local&gt;, &lt;data&gt;21 de abril de 1792&lt;/data&gt;), foi um &lt;profissao&gt;dentista&lt;/profissao&gt;, &lt;profissao&gt;tropeiro&lt;/profissao&gt;, &lt;profissao&gt;minerador&lt;/profissao&gt;, &lt;profissao&gt;comerciante&lt;/profissao&gt;, &lt;profissao&gt;militar&lt;/profissao&gt; e &lt;profissao&gt;ativista político&lt;/profissao&gt; &lt;gentilico&gt;brasileiro&lt;/gentilico&gt;, que atuou nas capitanias de &lt;local&gt;Minas Gerais&lt;/local&gt; e &lt;local&gt;Rio de Janeiro&lt;/local&gt;. Onde podemos ver tags como &lt;name&gt;, &lt;local&gt;, &lt;data&gt;, etc. ao redor de certas informações, o que torna possível ao computador encontrar estas informações. Grande parte do trabalho em análise textual trata-se passar do formato não-estruturado para o semi-estruturado ou estruturado, para que possamos trabalhar programaticamente. Por exemplo, Franzosi (2010) ao fazer análise da narrativa de jornais italianos da época de ascensão do Fascismo, passou textos não estruturados como este: Republicans plunged in Bissone di S. Cristina around 10pm of this month at the pub Prati. A guy, who went by the name of “captain,” took out a list of names and did the roll call loudly. Para o seguinte formato: [Semantic triplet 1: [Participant: [Actor: republicans]] [[Process: [[Verb: plunge] [Circumstances: [Space: [City: Bissone di S. Cristina] [[Location: pub] [Name: Prati]]]] [[Time: [Date: 05/07/1921] [Hour: 10pm]]]]] [Semantic triplet 2: [Participant: [Actor: captain]] [Process: [[Verb: does roll call] [Circumstances: [Type of action: loudly] [Instrument: list]]] [Participant: [Actor: workers]] Para tal, Franzosi desenvolveu um software para análise de narrativas textuais, o PC-ACE (Program for Computer-Assisted Coding of Events) e pôde ter uma noção da violência cotidiana na época: Tabela de Franzosi. List of daily occurences of triplets of violence in the Avanti! database (FRANZOSI, p.607) Frequency distribution of triplets of violence in the Avanti! database (FRANZOSI, p.607) E ainda fez um “mapa de calor” (“heat map”) com a localização da violência fascista na Itália (FRANZOSI, p.609) Referência: FRANZOSI, Roberto P.. Sociology, narrative, and the quality versus quantity debate (Goethe versus Newton): Can computer-assisted story grammars help us understand the rise of Italian fascism (1919–1922)?. Theor Soc (2010) 39:593–629. DOI 10.1007/s11186-010-9131-3 Os dados semi-estruturados não tem, portanto, formato de tabela, mas contêm indicações de informações mais abstratas, através de tags ou outras marcações. Com base nestas informações que faremos análises de texto. Este processo de transformação de dados não estruturados em estruturados é chamado de “datificação”. "],["análise-de-redes-sociais.html", "Capítulo 4 Análise de Redes Sociais 4.1 O pacote Igraph 4.2 Os pacotes ggraph e tidygraph . Construindo grafos com o tidyverse. 4.3 Redes de palavras 4.4 Redes de citação 4.5 Gráfico de centralidade 4.6 Comunidades 4.7 Sugestões de links", " Capítulo 4 Análise de Redes Sociais O R possui diversos pacotes para análise de rede, como o igraph, statnet, e do tidyverse temos tydygraph e ggraph. 4.1 O pacote Igraph Instalando o pacote igraph: install.packages(&quot;igraph&quot;) # instalando o pacote chamando o pacote já instalado library(igraph) # chamando o pacote já instalado ## ## Attaching package: &#39;igraph&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## decompose, spectrum ## The following object is masked from &#39;package:base&#39;: ## ## union É possível ainda plotar com os comandos tkplot(g) e rglplot(g). Rode no seu console e veja a diferença nos gráficos. Mas vamos a algo mais prático, pegando o famoso poema de Drummond: “João amava Teresa que amava Raimundo que amava Maria que amava Joaquim que amava Lili que não amava ninguém” E o transformando em um grafo (os “gráficos” em análise de rede recebem este nome): library(igraph) g &lt;- graph.empty(directed=TRUE) # &quot;directed&quot; implica distinguir entre &quot;de&quot; e &quot;para&quot; na relação entre os nós. # Adicionando os vértices. g &lt;- g + vertex(&quot;João&quot;) g &lt;- g + vertex(&quot;Teresa&quot;) g &lt;- g + vertex(&quot;Raimundo&quot;) g &lt;- g + vertex(&quot;Maria&quot;) g &lt;- g + vertex(&quot;Joaquim&quot;) g &lt;- g + vertex(&quot;Lili&quot;) # Especificando as relações entres os vértices, os edges g &lt;- g + edges(&quot;João&quot;, &quot;Teresa&quot;) g &lt;- g + edges(&quot;Teresa&quot;, &quot;Raimundo&quot;) g &lt;- g + edges(&quot;Raimundo&quot;, &quot;Maria&quot;) g &lt;- g + edges(&quot;Maria&quot;, &quot;Joaquim&quot;) g &lt;- g + edges(&quot;Joaquim&quot;, &quot;Lili&quot;) plot.igraph(g) # plotando o grafo Neste caso, poderíamos ter feito este mesmo gráfico com código mais compacto: library(igraph) g &lt;- graph.empty(directed=TRUE) # Adicionando os vértices. g &lt;- g + vertex(c(&quot;João&quot;, &quot;Teresa&quot;, &quot;Raimundo&quot;, &quot;Maria&quot;, &quot;Joaquim&quot;, &quot;Lili&quot;)) # Adicionando os edges em pares g &lt;- g + edges(c(&quot;João&quot;, &quot;Teresa&quot;, &quot;Teresa&quot;, &quot;Raimundo&quot;, &quot;Raimundo&quot;, &quot;Maria&quot;,&quot;Maria&quot;, &quot;Joaquim&quot;,&quot;Joaquim&quot;, &quot;Lili&quot;)) plot.igraph(g) Se o grafo sobe ou desce, pouco importa para nós aqui, importa as pessoas e as relações entre elas. Repare que os edges são entendidos aos pares. Se fizéssemos um vetor sem as devidas repetições, teríamos um gráfico errado das relações: g &lt;- graph.empty(directed=TRUE) g &lt;- g + vertex(c(&quot;João&quot;, &quot;Teresa&quot;, &quot;Raimundo&quot;, &quot;Maria&quot;, &quot;Joaquim&quot;, &quot;Lili&quot;)) g &lt;- g + edges(c(&quot;João&quot;, &quot;Teresa&quot;, &quot;Raimundo&quot;, &quot;Maria&quot;,&quot;Joaquim&quot;, &quot;Lili&quot;)) plot.igraph(g) Ou de modo mais econômico ainda: g &lt;- graph.formula( João --+ Teresa --+ Raimundo --+ Maria --+ Joaquim --+ Lili ) plot(g) 4.1.1 Clusterização Em elaboração 4.2 Os pacotes ggraph e tidygraph . Construindo grafos com o tidyverse. O pacote ggraph é um pacote elaborado por Thomas Lin Pedersen, o mesmo do ggplot2, e pretende ser uma extensão deste, usando a mesma gramática de gráficos, o que nos dá grande flexibilidade visual. Com o ggraph é possível construir graficamente redes, mas ele vai além dos grafos, construindo também dendogramas, diferentes tipos de árvores, matrizes, gráficos hierárquicos, diagrama de arc, sunburst, etc. Para inserir os dados no ggraph, é necessário colocá-lo no formato “tidy” do “tidyverse”, e fazemos isso com o pacote tidygraph: Para instalar, usamos os comandos: install.packages(&#39;ggraph&#39;) install.packages(&#39;tidygraph&#39;) Carregando os pacotes library(ggraph) ## Loading required package: ggplot2 library(tidygraph) ## ## Attaching package: &#39;tidygraph&#39; ## The following object is masked from &#39;package:igraph&#39;: ## ## groups ## The following object is masked from &#39;package:stats&#39;: ## ## filter Seguindo o exemplo na página do criador do GGraph Thomas Lin Pedersen, vamos usar o dataset highschool do ggraph, que contém dados sobre a evolução da amizade entre garotos numa escola do ensino médio no Illinois, nos anos de 1957 e 1958, que responderam à pergunta: “Com que colegas desta escola você anda mais frequentemente?”. Esta pesquisa apareceu originalmente nos livros “Introduction to Mathematical Sociology” e “The Adolescent Society”, ambos do sociólogo James Coleman. Para obter mais informações sobre este dataset, basta digitar no console: help(highschool). str(highschool) # observando a estrutura do data frame ## &#39;data.frame&#39;: 506 obs. of 3 variables: ## $ from: num 1 1 1 1 1 2 2 3 3 4 ... ## $ to : num 14 15 21 54 55 21 22 9 15 5 ... ## $ year: num 1957 1957 1957 1957 1957 ... head(highschool,10) # observado as primeiras linhas do data frame ## from to year ## 1 1 14 1957 ## 2 1 15 1957 ## 3 1 21 1957 ## 4 1 54 1957 ## 5 1 55 1957 ## 6 2 21 1957 ## 7 2 22 1957 ## 8 3 9 1957 ## 9 3 15 1957 ## 10 4 5 1957 Vemos que há na coluna 1 de (“from”) pessoa número X para (“to”) para pessoa Y no ano (“year”). Assim, a pessoa 1 teve contato com as pessoas 14, 15, 21, 54 e 55 em 1957. A pessoa 2 teve contato com as pessoas 21 e 22, e assim por diante. Vamos preparar os dados para plotar o grafo com ggraph. Antes, é necessário colocá-lo no formato “tidy” do “tidyverse”, e fazemos isso com o pacote tidygraph. A função as_tbl_graph() do pacote tidygraph funciona como a função grouped_df(), que agrupa nós (nodes) e arestas (edges). as_tbl_graph(highschool) ## # A tbl_graph: 70 nodes and 506 edges ## # ## # A directed multigraph with 1 component ## # ## # Node Data: 70 x 1 (active) ## name ## &lt;chr&gt; ## 1 1 ## 2 2 ## 3 3 ## 4 4 ## 5 5 ## 6 6 ## # … with 64 more rows ## # ## # Edge Data: 506 x 3 ## from to year ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 13 1957 ## 2 1 14 1957 ## 3 1 20 1957 ## # … with 503 more rows Vamos criar uma lista chamada “grafo”, adicionando um campo novo com mutate(), de nome “Popularidade” e que vai medir a centralidade de grau através da função centrality_degree() do tidygraph. Centralidade de grau é a medida mais simples de centralidade, que conta o número de conexões, as arestas (“edges”) de cada nó. grafo &lt;- as_tbl_graph(highschool) %&gt;% mutate(Popularidade = centrality_degree(mode = &#39;in&#39;)) grafo ## # A tbl_graph: 70 nodes and 506 edges ## # ## # A directed multigraph with 1 component ## # ## # Node Data: 70 x 2 (active) ## name Popularidade ## &lt;chr&gt; &lt;dbl&gt; ## 1 1 2 ## 2 2 0 ## 3 3 0 ## 4 4 4 ## 5 5 5 ## 6 6 2 ## # … with 64 more rows ## # ## # Edge Data: 506 x 3 ## from to year ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 13 1957 ## 2 1 14 1957 ## 3 1 20 1957 ## # … with 503 more rows Plotando o grafo: ggraph(grafo, layout = &#39;kk&#39;) + geom_edge_fan(aes(colour = stat(index)), show.legend = FALSE) + geom_node_point(aes(size = Popularidade, colour= Popularidade), show.legend = TRUE) + scale_colour_gradient(low = &quot;steelblue&quot;, high = &quot;black&quot;) + facet_edges(~year) + theme_graph(foreground = &#39;steelblue&#39;, fg_text_colour = &#39;yellow&#39;) + labs(title = &quot;Evolução da amizade entre adolescentes de uma escola no Illinóis&quot;, caption = &quot;Fonte: Elaboração própria a partir dos dados de Coleman(1961)&quot;) layout define como os nós serão alocados. O layout do ggraph possui os mesmos do igraph e outros mais como, hive plots, treemaps e circle packing. kk indica que está sendo usado o algortimo Kamada-Kawai para dispersar os nós e facilitar nossa visualização. geom_edge_fan() desenha os laços de modo curvo. Há diversas outras opções possíveis neste caso, como: geom_edge_arc, geom_edge_bend, geom_edge_diagonal,geom_edge_elbow, geom_edge_fan, geom_edge_hive, geom_edge_link, geom_edge_parallel. colour indica que a intensidade das ligações será por um gradiente de cor, stat() indica que segue ali uma informação estatística. geom_node_point() msostra os nós como pontos/círculos e permite que sejam plotados em diferentes tamanhos, cores e formas em aes(). size = Popularidade indica que o tamanho dos nós é controlado pela variável “Popularidade” que criamos. colour = Popularidade indica que além do tamanho, a cor também vaira conforme a popularidade. Para mais opções, digite ?geom_node_point() no console ou consulte a documentação do ggraph. scale_colour_gradient(low = \"steelblue\", high = \"black\") é opcional, podendo ser retirada. Especifica o gradiente de cores do comando anterior (no caso, nós), qual o valor mais baixo até o mais elevado. Sua informação é redundante com relação ao tamanho dos nós, mas ajuda a reforçar tal informação. facet_edges() função de “faceting”, de criar facetas, gráficos multiplos, e o símbolo de til ~ seguido de year indica que o critério aqui são as categorias dentro de ano, que no caso, são 1957 e 1958. Os nós são repetidos em cada painel. theme_graph especifica as cores das legendas nas facetas labs(title = \" indica o título e caption o rodapé. Veja também cheatsheet/folha de dicas do ggraph Para ver os diferentes layouts possíves do ggraph e seus respectivos códigos, para além dos grafos clique aqui CAPÍTULO A SER EXPANDIDO 4.3 Redes de palavras EM CONSTRUÇÃO 4.4 Redes de citação EM CONSTRUÇÃO 4.5 Gráfico de centralidade EM CONSTRUÇÃO 4.6 Comunidades EM CONSTRUÇÃO 4.7 Sugestões de links AQUINO, Jackson A. “Análise de redes sociais”, capítulo 12 de ___. R para cientistas sociais. Ilhéus, BA: EDITUS, 2014. 157 p. ISBN: 978-85-7455-369-6. (PDF gratuito bem introdutório com R) HIGGINS, Silvio S.; RIBEIRO, Antônio Carlos. Análise de redes em Ciências Sociais. Brasília: ENAP. 2018. 229p. (PDF Gratuito de livro. Bom para aprender os conceitos/teorias básicos de análise de rede). Manual online do ipgraph para R; PDF do Manual do igraph para R (ambos em inglês) ggraph Documentation no Cran. d’ANDRÉA, Carlos Frederico de Brito. Pesquisando plataformas online: conceitos e métodos. EDUFBA. 2020. A obra visa introduzir os Estudos de Plataforma, um campo de estudos que, desde o início da década de 2010, discute as especificidades políticas e materiais das mídias sociais e de outras plataformas online. Datificação, algoritmos, governança e os modelos de negócio das plataformas são algumas das dimensões sintetizadas no livro. De modo didático, o autor apresenta um conjunto de leituras e de experimentações metodológicas conduzidas com um diversificado grupo de colaboradoras(es) no país e no exterior. (ebook PDF e Epub gratuitos) RECUERO, Raquel. Introdução à análise de redes sociais online. EDUFBA.2017. A Análise de Redes Sociais (ARS) é uma abordagem de pesquisa cuja popularidade tem aumentado nos últimos anos, principalmente, entre os pesquisadores da área de Comunicação. É nesse âmbito que várias obras, entre artigos e livros, vêm surgindo e introduzindo o estudo dessas estruturas a partir da análise de redes e da compreensão da representação dessas redes sociais na internet. Este livro é uma pequena compilação dos principais conceitos e elementos para a compreensão e a aplicação da ARS. É baseado em uma breve apresentação e histórico do paradigma, os principais conceitos, suas métricas e, finalmente, suas formas de representação e visualização. (ebook PDF e Epub gratuitos). LIZARDO Omar; JILBERT Isaac. Social Networks: An Introduction. 2021. (ebook online) "],["links-úteis.html", "Capítulo 5 Links úteis 5.1 Humanidades digitais 5.2 Análise de redes sociais 5.3 Textos sobre análise textual 5.4 Processamento Linguagem Natural (PLN ou NLP) 5.5 Revistas / Journals acadêmicos 5.6 Links sobre Dados Abertos 5.7 Vídeos 5.8 Sites / Blogs 5.9 Organizações 5.10 Links de Podcasts sobre Digital Humanities 5.11 Links Programação 5.12 Links de Cursos Online 5.13 Grupos de discussão/Forum 5.14 Links de folhas de dicas (Cheat-sheets)", " Capítulo 5 Links úteis Segue aqui uma lista com links para livros, artigos, sites, manutais, tutoriais, vídeos e canais de vídeos. A maioria e a prioridade é de material gratuito, sobre temas gerais relacionados a humanidades digitais e em especial, à sociologia. Há também sugestões mais focadas nos assuntos específicos nos capítulos. O critério de seleção aqui foi o de contribuição para esclarecer o potencial das humanidades digitais para quem inicia na área de humanidades digitais/sociologia digital. Os itens não foram selecionados tanto pela contribuição científica, mas com base no critério de exemplos que podem ajudar a esclarecer o potencial das humanidades digitais para quem inicia. 5.1 Humanidades digitais Introduction to Digital Humanities Textbook A digital textbook designed for UCLA’s Introduction to Digital Humanities course website. (ebook disponível gratuito online) EDMOND, Jennifer Digital Technology and the Practices of Humanities Research. (Livro gratuito) The Data Journalism Handbook. Towards a Critical Data Practice. 415p. 2021. ISBN 9789048542079. (Livro GRATUITO). BAUER, Paul C. Computational Social Science: Theory &amp; Application. (livro online grátis). 5.2 Análise de redes sociais Ver sugestões no capítulo “Análise de Redes Sociais”. 5.3 Textos sobre análise textual GRIMMER, Justin.STEWART, Brandom. Text as Data: The Promise and Pitfalls of Automatic ContentAnalysis Methods for Political Texts. Political Analysis(2013) pp. 1–31. doi:10.1093/pan/mps028. CASTELFRANCHI, Yurij. A análise de textos auxiliada pelo computador: um laboratório a céu aberto para as ciências sociais. Journal of Science Communication 16(02)(2017)C04 TREADWELL, Donald. Content Analysis: Understanding Text and Image in Numbers. Understanding Text and Image in Numbers. In __ Introducing Communication Research: paths of Inquiry. Sage. 2014. (Capítulo sobre análise de conteúdo) 5.3.1 Sociologia Digital KATEMBERA, Serge. “Sociologia digital ou sociologia do digital?” V. 2 N. 1 (2020): Dossiê Ambiente e Sociedade. (Artigo) NASCIMENTO, Leonardo F. Sociologia digital : uma breve introdução. EDUFBA. 2020. (Livro gratuito em PDF e epub) Podcast “New Work in Digital Humanities”. Episódio: Neil Selwyn, “What is Digital Sociology?” (Polity, 2019) BERNAU, John A. Text Analysis with JSTOR Archives. November 12, 2018 https://doi.org/10.1177/2378023118809264. (Aplicação de métodos de humanidades digitais, usando a base do Jstor, para uma rápida análise de variação temática em revistas sociológicas). FUSSEY, Pete e ROTH, Silke (ed.) Digitizing Sociology. edição da revista “Sociology” da British Sociological Association. 5.3.1.1 Netnografia, etnografia digital MARKHAM, Annette N. 2013. Fieldwork in Social Media. Qualitative Communication Research 2, 4 (Dec. 2013), 434–446. https://doi.org/10.1525/qcr.2013.2.4.434 SHAFFER, David Williamson. Quantitative Etnography. Boswell Press. 2017. “This is a book about understanding why, in the digital age, the old distinctions between qualitative and quantitative research methods, between the sciences and humanities, and between numbers and understanding, limit the kinds of questions we can ask, in some cases, and lead us accept superficial answers in others. Quantitative Ethnography is a research method that goes beyond those distinctions to help us understand how to make sense of our increasingly data-rich world…\". (PDF da Introdução disponível gratuitamente) 5.4 Processamento Linguagem Natural (PLN ou NLP) JURAFSKY, D.; MARTIN, J. Speech and language processing: An introduction to speech recognition, computational linguistics and natural language processing. Upper Saddle River, NJ: Prentice Hall, 2020. link pdf dos capítulos individuais, link livro completo. (Um manual bastante extenso e mais teórico sobre PLN) BIRD, S.; KLEIN, E.; LOPER., E. Natural language processing with Python – analyzing text with the natural language toolkit. (Livro online gratuito, baseado em Python 3.) Playlist no Youtube das aulas da Stanford University sobre PLN 5.5 Revistas / Journals acadêmicos SIGCAS Computers and Society da Association for Computing Machinery Digital Scholarship in the Humanities. Revista da Universidade de Oxford. Sessão com artigos gratuitos Journal: Digital humanities Quartely International Journal of Digital Humanities Socius: Sociological Research for a Dynamic World. Special Collection: Data Visualization. Jornal de acesso livre International Journal of Humanities and Arts Computing (Universidade de Edimburgo) magazén International Journal for Digital and Public Humanities. (open access). Revista do Dipartimento di Studi Umanistici da Università Ca’ Foscari de Veneza Journal of Digital Social Research. Open Access Reviews in Digital Humanities. (O Review já não publica desde 2014) Journal of Digital Humanities. (O Journal of DH tem uma sessão de resenha de novas ferramentas disponíveis). Journal of Cultural Analytics. Department of Languages, Literatures, and Cultures at McGill University, Canada. open-access journal dedicated to the computational study of culture 5.6 Links sobre Dados Abertos 5 estrelas dos dados abertos (Tim Berners-Lee, criador do termo “dados abertos”. Site explica o que são dados abertos e seus 5 níveis. Em português) Busca do Google por base de dados Abertos Dados abertos. (Se não sabe onde encontrar algum dado específico que você procura, veja aqui) SHIKIDA, Claudio D., MONASTERIO, Leonardo, NER, Pedro Fernando. Guia Brasileiro de Análise de Dados: Armadilhas e Soluções. Brasília. 2021. ( Tópicos: Causalidade, Pobreza e Desigualdade, Análise de dados em Saúde, Educação, Crimes e Violência, Macroeconomia, Mercado e Trabalho e Opinião Pública.) Dados Abertos: Fórum de discussão. Workshop do Henrique Xavier, no canal “Base dos Dados” sobre como explorar os dados do Diário Oficial da União. 5.7 Vídeos Vídeos do 1º Summerschool of Digital Humanities da Universidade de Heidelberg, Alemanha, ocorrido em 2017. link. (Em inglês) 5.8 Sites / Blogs Text analysis info Textual Analysis - University of Notre Dame Site Digital Humanities Now. (Agrega oportunidade de emprego, notícias, bolsas de pesquisa). site Go Digital Humanities. (Novidades sobre humanidades digitais, como eventos). Blog Digital Society Blog. (Blog do Institut für Internet und Gesellschaft da Alexander von Humboldt). 5.9 Organizações Alliance of Digital Humanities Organizations (ADHO). Global Network of Internet and Society Research Centers (NoC) i. (Catálogo com grupos de pesquisa sobre internet e sociedade ao redor do mundo). 5.10 Links de Podcasts sobre Digital Humanities Complexity: Peter Dodds on Text-Based Timeline Analysis &amp; New Instruments for The Science of Stories New Work in Digital Humanities. New Books Network. Interviews with digital humanists about their new work. Lista com mais podcasts dedicados às digital humanities 5.11 Links Programação 5.11.1 R introdução LENTE, Caio. Zen do R. “O objetivo deste livro é ensinar ao leitor que não costuma programar algumas formas simples de melhorar a organização de seus projetos de análise de dados em R”. (Livro online gratuito). AQUINO, Jackson A.. R para cientistas sociais. Ilhéus, BA: EDITUS, 2014. 157 p. ISBN: 978-85-7455-369-6. (PDF gratuito) FERREIRA, E.B.;. de OLIVEIRA, M.S. Introdução à Estatística com R. Unifal. 2020. (PDF gratuito). Livro Curso-R (em construção) Blog: Curso-R KUBRUSLY, Jessica. Uma Introdução à Programação com o R. (ebook online em português) FREIRE, Sergio Miranda. Introdução ao R. (Livro online, no formato bookdown) WICKHAM, Hadley. Advanced R. 2nd edition. (Livro online gratuito, no formato bookdown, em inglês, para entender melhor os conceitos do R). 5.11.2 R - tópicos específicos WICKHAM, H. GROLEMUND, Garret. R for Data Science. O’Reilly. 2017. (Livro Online, em inglês). IRIZARRY, Rafael A. Introduction to Data Science. Livro Online, em inglês, feito como notas de aulas da HarvardX Data Science Series CLARK, Michael. R &amp; Social Science. Getting Started with applied use of R in the Social Sciences. (pequeno manual em PDF da Universidade de Notre Dame) ggplot2 on-line version of work-in-progress 3rd edition of “ggplot2: elegant graphics for data analysis” SILGE, Julia, ROBINSON, David Text Mining with R. (ebook online) RYDBERG-COX, Jeff. Statistical Methods for Studying Literature Using R da Universidade Missouri-Kansas City. R-tutor- An R Introduction to Statistics. Site com tutoriais diversos sobre R. Em Inglês. Diversos links para livros gratuitos de R no bookdown. 5.11.3 Links Python HEINOLD, Brian. A Practical Introduction to Python Programming. 2012. 263 p. (PDF de introdução ao Python) Jake VanderPlas. Python Data Science Handbook: Essential Tools for Working with data. (livro online). 5.11.4 Python - tópicos específicos KARSDORP, Folgert. Python Programming for the Humanities. Livro online. Pyhton Humanities. Site com introdução às DH em Python, com alguns tutoriais. Constellate What do you want to learn today?. (Projeto Constellate da JStor junto a diversas universidades. Conta com sessão com tutoriais sobre análise textual, a maioria em Python. 5.12 Links de Cursos Online EdX, Cursos online de Universidades como Harvard, MIT, etc. gratuitos como ouvinte. Paga-se pelo certificado. Há cursos grautuitos, como “Introducing Text Analytics and Natural Language Processing with Python”, “Introduction to Digital Humanities”, “Data Science: Visualization”, “Using Python for Research”. DataCamp. Cursos mais práticos, em R, Python, SQL, com exercícios, totalmente online (não é necessário instalar nada em seu computador). As partes iniciais dos cursos costumam ser gratuitas, mas há parte paga com anuidades. Udemy Possui cursos gratuitos e cursos pagos. É possível encontrar cursos pagos por volta de R$20,00. Coursera. Possui parceria com mais de 200 universidades e empresas como o Google e IBM. Cognitive Class Da IBM, Cursos gratuitos em data science, alguns gratuitos com certificado. Cursos em inglês e espanhol. Possui cursos como “Data Visualization with R”. Big Data University. Cursos em Portugues. Curso gratuito de Estatística e probabilidade da Khan Academy, em português. Inclui teste de hipótese, regressão. 5.13 Grupos de discussão/Forum 5.13.1 Telegram Processamento de Linguagem Natural em Português PT-Br Data Science - Python R Brasil R humanidades Análise Textual-Humanidades Digitais 5.14 Links de folhas de dicas (Cheat-sheets) “Cheat sheets” seriam a tradução para “cola”, aquela feita para consulta em exames na escola. Em programação, refere-se a uma tabela muito bem resumida, com o que há de essencial em determinado assunto. Text Analysis Glossary Um glossário de termos usados na análise textual, em inglês, do projeto Constellate (JStor e diversas universidades) Lista com mais de 100 cheat-sheets em R e Python, sobre Machine Learning: link Git-GitHub, Git-Gitlab, Cheatsheets do R como: Base R, Base R em portguês, Data-table em português, Data Import, Data e horários com Lubridate, trabalhe com mais facilidade com listas e funções com o purr, stringr, dplyr, R Reference Card, Usar Python no R com reticulate, ggplot, R Markdown com o R Markdown Cheatsheet ou o Reference Guide, Regular Expressions, R Programming por Arianne Colton e Sean Chen, Uma lista com estes e outros cheatsheets aqui do Rstudio aqui. "]]
