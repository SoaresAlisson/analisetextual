[["index.html", "Introdução à Análise Textual aplicada à Sociologia Objetivos deste manual Plano do livro", " Introdução à Análise Textual aplicada à Sociologia Alisson Soares Primeira versão em 22 de abril de 2021. Última atualização em 16 junho de 2022 Objetivos deste manual Sou Alisson Soares, sociólogo, e comecei no campo das humanidades digitais e ciências sociais computacionais. Achei por bem montar este manual como uma forma de compartilhar o que venho aprendendo. Algumas ferramentas, softwares serão de modo gráfico, mas muitas vezes iremos recorrer à programação. A linguagem aqui será o R, uma linguagem de programação gratuita, livre e de código aberto, bastante utilizada no mundo acadêmico. O objetivo deste manual é: Ser um livro em progresso constante. (Confira a data de atualização para saber se há material novo) Ser introdutório, mas também sempre indicar bons materiais para maior aprofundamento dos temas tratados. Sempre que possível, oferecer material em português, apesar de nem sempre isto ser possível. Oferecer uma introdução às ferramentas computacionais utilizadas em pesquisa das ciências sociais. Usar preferencialmente ferramentas gratuitas e de código aberto. Sempre que possível, apresentar exemplos de pesquisa e ciência social utilizando estas ferramentas. Ao final de cada capítulo sessão apresentamos dicas de links externos. Há também um capítulo dedicado a links de temas correlatos, que podem ajudar a vida dos humanistas digitais. Caso encontre algum erro ou imprecisão, caso tenha algum elogio ou crítica, meu contato é alissonmsoares@gmail.com e meu contato no Twitter. Agradecimento aos grupos “R Brasil” e “R Humanidades” no Telegram que ajudaram na construção deste manual, com dicas que foram aproveitadas aqui. Plano do livro Por se tratar de um livro em elaboração há ainda muitos pontos ainda a serem trabalhaos. Há vários temas a serem incorporados futuramente e já estão em elaboração, como por exemplo: obtenção de dados (datasets de pacotes, webscraping básico, APIs) diferentes tipos de visualizações, como dendrogramas, pyramid plot, etc. Correlação, tipos de distâncias/semelhança extração de palavras chave (keyword extraction) Introdução à análise de redes Inteligência Artificial: clusterização; aprendizado supervisionado e não supervisionado (como topic modeling) Análise de sentimentos (+ visualizações) "],["intro.html", "Introdução O termo “humanidade digitais” Fluxo de trabalho", " Introdução “the best digital theory-building of the past decade stems from social and computational origins. As such, it is increasingly apparent that digital sociologists need to develop a computational as well as a sociological imagination.” (Neil Selwyn) Social researchers of the future will need to write code, wrangle data, and think computationally as well as sociologically. (EVANS &amp; FOSTER) O termo “humanidade digitais” Segundo um estudo do LinkedIn em 15 países, prevendo 150 milhões de vagas nos próximos 5 anos, das áreas de trabalhos digitais em alta e 2021, duas se relacionam a Humanidades Digitais: Big data Analytics (segundo lugar) e “Text, image and voice processing” (6º lugar). Profissionalmente, pode ser interessante entrar nesta área. Existem diversos termos correlatos a “Humanidades digitais” (“Digital humanities”), como “E-humanities,”Cultural Analytics”, “ciências sociais computacionais”, “sociologia digital”, “história digital”, “etnografia digital”, “etnografia quantitativa”, “Análise cultural quantitativa” (quantitative cultural analysis) “cultural analytics”, “humanities data science”, “humanities data analysis” “humanities computing”, “computational social science”, “distant reading”, “text as data”, “text analytics” etc. Embora estes termos não sejam todos sinônimos entre si, há diversos pontos de contato. Em quase todos eles há a junção de algo de humanidades e algo de digital, demonstrando seu caráter transdisciplinar. A área possui contribuições de diversas áreas, como das ciências da informação, linguística, ciências da computação, história, sociologia, comunicação, etc. O termo é controverso mesmo entre os participantes, uma vez que é amplo demais. Uma definição ampla define o termo como intersecção entre humanidades e computação. Porém, uma busca no Google por um texto já configura uso de ferramenta digital. Outra definição mais restrita envolveria aprender a pensar como o computador e a partir disto extrair diversas novas possibilidades. Uma introdução aos diferentes significados do termo pode ser vista no artigo “Humanidades digitais” do grupo de pesquisas da USP. Em termos mais práticos, Humanidades digitais inclui a análise de: sons/música, imagens. textos. Nosso objetivo aqui Há todo um repertório de ferramentas comuns utilizados tanto por cientistas de dados como por humanistas digitais, com diferentes fins. Nosso foco será aprender diversas destas ferramentas de uso comum, mas focando em seu uso na ciência social computacional. Em ramos da ciência social como Sociologia, Ciência Política, Comunicação Social, etc. é bem comum o termo “Ciência Social Computacional” (Computational Social Science). Este artigo de Chris Bail, Mapping Computational Social Science de 2019 mostra como este campo está explodindo no número de publicações. Uma outra dica é o artigo Computational social science: Obstacles and opportunities de 2020 na Science, com nomes importantes do campo. Fluxo de trabalho O fluxo de trabalho “workflow” básico consiste em: &lt;!– A[1.Adquirir dados] – &gt;B[2.Pré tratar dados]; –&gt; "],["história-da-análise-textual.html", "1 História da Análise Textual 1.1 Linha do tempo da história da Análise Textual", " 1 História da Análise Textual “May we hope that when things come to such a crisis, human labor of the literary sort may be in part superseded by machinery? Machinery has done wonders, and when we think of what literature is becoming, it is certainly to be wished that we could read it by machinery, and by machinery digest it” (Andrew Stauffer In London’s Daily News. 15 de Setembro de 1869 apud Catherine DeRose.) A análise textual abarca campos do conhecimento bem variados, como psicologia, ciências da computação, ciências da informação, linguística, ciência política, sociologia, etc. Apresentamos aqui uma linha do tempo com alguns dos principais eventos relacionados à análise textual, bem como ao seu uso com computadores. De modo resumido, podemos pensar nos primeiros desenvolvimentos ao final do século XIX, a introdução do computador e mais recentemente, a introdução da inteligência artificial como pontos marcantes nesta cronologia. Fizemos aqui uma breve cronologia, que não pretende ser extensiva, com todos eventos importantes, mas apenas demarcar alguns pontos interessantes, para ajudar a dar alguma ideia àqueles que iniciam no campo das humanidades digitais e na sociologia digital. 1.1 Linha do tempo da história da Análise Textual Séc. XVII - Igreja Católica analisa a proporção de textos impressos de conteúdo não religioso 1887 Medenhall. Analisa o comprimento de palavras: MENDENHALL, T. C. . The characteristic curves of composition. Science.Vol ns-9, Issue 214S. 11 March 1887. baixar pdf 1888 Benjamin Bourdon (1860-1943, psicólogo e professor da Université de Rennes): Ao pesquisar sobre a expressão de emoções através de palavras, analisou o livro “Exodus” da Bíblia e calculou frequências, classificou e eliminou as stopwords. “In 1888, in a research on the expression of emotions through words, Benjamin Bourdon analysed the Exodus of the Bible and calculated the frequencies by rearranging and classifying them, eliminating the stop words” fonte, link para o livro online. 1888 Friedrich Kaeding (1855 - 1929), cria índices de frequência para estruturação de sistemas estenográficos (sistema de escrita por abreviações para que a escrita seja tão rápida como a fala). Wincenty Lutoslawski, criador do termo “estilometria”, lança sua análise de palavras raras na obra de Platão. LUTOSLAWSKI, W. Origin and Growth of Plato’s Logic. with an account of Plato’s style and of the chronology of his writings. New York: Longsmans, Green. 1897. p. 613. link. 1934 Harold Laswell (1902-1978, cientista político) produz a primeira contagem de palavras chave. O linguista norteamericano George Kingsley Zipf (1902–1950) publica “The psycho-biology of language” onde sugere o “principle of relative frequency”, que ficou conhecido posteriormente como “Zipf’s Law”. 1934 Vygostky produz a primeira análise quantitativa de narrativa 1949 Robert Busa (padre jesuíta) junto à IBM com o projeto Index Thomisticus, que levou 34 anos, envolveu cerca de 70 pessoas, sobre as obras de São Tomás de Aquino, que envolveu indexação e lematização de palavras e frequência de termos. Técnicas ali desenvolvidas foram usadas posteriormente nos manuscritos do Mar Morto, para tentar preencher partes faltantes do texto. “The IBM… considered this first enterprise of using a computer for linguistic and lexicographic goals as a pilot-project” fonte, indexando mais de 10 milhões de palavras. Busa, R. (1980). “The Annals of Humanities Computing: The Index Thomisticus”. Computers and the Humanities. 14 (2): 83–90. doi:10.1007/BF02403798. ISSN 0010-4817. ROCKWELL, Geoffrey; PASSAROTTI, Marco (2019-05-27). “The Index Thomisticus as a Big Data Project”. Umanistica Digitale (5). doi:10.6092/issn.2532-8816/8575. “The Index Thomisticus, itself, is divided into two parts - the indexes and the concordances. The index alphabetically notes each word along with a reference to its distribution and frequency. Besides a general index for the entire study, there is also one for each work. The concordances, on the other hand , list alphabetically all the words and cite every passage in which a word app ears.”. “Jesuit Father Uses Computer to Analyze Works of St.Thomas Aquinas”. Modern Data, 1973. pp.41-2 1950 Gottschalk usa Content Analysis para rastrear temas freudianos. GOTTSCHALK, Louis A. The Measurement of Psychological States Through the Content Analysis of Verbal Behavior. University of California Press. 1969. 317p. Gottschalk-Gleser Content Analysis Method of Measuring the Magnitude of Psychological Dimensions 1950 Alan Turing aplica Inteligência Artificial a textos. 1952 Bereleson publica o primeiro manual de análise de conteúdo. BERELSON, B. (1952). Content analysis in communication research. New York: Hafner. 1954 Primeira tradução automática de texto (Georgetown–IBM experiment) do russo para o inglês. HUTCHINS, John. The first public demonstration of machine translation: the Georgetown-IBM system, 7th January 1954. 2006. Press Release da IBM 1963 Mosteller e Wallace analisam a autoria dos Federalist Papers. MOSTELLER, F., WALLACE, D. L.. 1963. Inference in an authorship problem. Journal of the American Statistical Association 58:275–309. 1965 Tomashevsky formaliza análise quantitativa de narrativa. TOMASHEVSKY, B. (1965). Thematics. In L T. Lemon &amp; M. I. Reis (Eds. &amp; Trans.), Russian formalist criticism: Four essays (pp. 61-95). Lincoln: University of Nebraska Press. (Original de 1925) 1966 Stones e Bales usam computador para medir propriedades psicométricas de textos na RAND. 1980 Declínio do formalismo chomskyano; nascimento do Processamento de Linguagem Natural (PLN). 1980 Aplicação de Aprendizado de máquinas (Machine Learning) ao Processamento de Linguagem Natural 1981 Walter Weintraub e a contagem de parts-of-speech. WEINTRAUB, Walter. Verbal Behavior: adaptation and psychopathology. Springer:NY. 1981. SOBEL, Dava. Language patterns reveal problems in personality. NYT. Oct. 27,1981. 1985 Schrodt introduz codificação automática de eventos (Automated Event Coding). SCHRODT, Philip A. Automated Coding Of International Event Data Using Sparse Parsing Techniques. 2000. 1986 James W. Pennebaker desenvolve LIWC (Linguistic Inquiry and Word Count). 1989 Roberto Franzosi (perfil no Research Gate) (sociólogo) traz a análise quantitativa de narrativa (quantitative Narrative Analysis) para as Ciências Sociais. 1998 Primeiro desenvolvimento de Topic Models. 1998 John W Mohr conduz a primeira análise quantitativa de visões de mundo. 1999 Peter Bearman (sociólogo) et al. aplicam métodos de rede a narrativas “Narrative network”. 2001 David M. Blei et al desenvolvem a LDA (Latent Dirichlet Allocation). David M. Blei, Andrew Y. Ng, Michael I. Jordan. Latent Dirichlet Allocation. Journal of Machine Learning Research 3 (2003) 993-1022. Github da Blei Lab. 2003 MALLET (MAchine Learning for LanguagE Toolkit), um dos primeiros sistemas de topic models, é criado. “MALLET is a Java-based package for statistical natural language processing, document classification, clustering, topic modeling, information extraction, and other machine learning applications to text. MALLET includes sophisticated tools for document classification: efficient routines for converting text to”features”, a wide variety of algorithms (including Naïve Bayes, Maximum Entropy, and Decision Trees), and code for evaluating classifier performance using several commonly used metrics.” 2005 Quin et al analisam discursos políticos usando topic models. Kevin M Quinn, Burt L Monroe, Michael Colaresi,Michael H Crespin, and Dragomir R Radev. 2010. How to analyze political attention with minimal assumptions and costs. American Journal of Political Science54(1):209–228 2010 Gary King e Daniel Hopkins trazem Topic Models ao mainstream. Hopkins, Daniel, and Gary King. 2010. “A Method of Auto-mated Nonparametric Content Analysis for Social Science.”American Journal of Political Science54(1): 229–47. 2014 Margaret Roberts, et al. desenvolvem “Structural Topic Models”. 2014 - Primeiro Workshop sobre Argument mining ou “argumentation mining” Proceedings das edições anteriores aqui e aqui (na Sessão “Past Workshops” é possível acessar papers de edições anteriores - desde 2014 - do evento). Fonte: Versão ampliada, baseado parcialmente em: “SICSS 2018 - History of Quantitative Text Analysis” slides, video e BROWN, Taylor W. Workshop on automated text analysis no Summer Institute in Computational Social Science na Universidade de Oxford em 2019. Em inglês, sem legendas. Parte 1. Pretende-se posteriormente expandir esta seção, explicando em mais detalhes alguns dos exemplos acima "],["exemplos-de-pesquisas-em-humanidades-digitais.html", "2 Exemplos de Pesquisas em Humanidades Digitais 2.1 Bibliometria / cientometria / cienciometria 2.2 Exemplo: Google Trends como Proxy para epidemias 2.3 Exemplo: Como as fake news sobre a pandemia de Covid-19 se assemelham/divergem entre os países? 2.4 Exemplo: Mudança de significado de palavras 2.5 Exemplo: Análise de complexidade musical 2.6 Exemplo: Polarização 2.7 Exemplo de integração quali-quanti: Complementando dados qualitativos 2.8 Exemplo de integração quali-quanti: Depurando dados quantitativos 2.9 Exemplo: Processo Civilizador 2.10 Ex.: Determinantes sociais do florescimento da cultura Incel", " 2 Exemplos de Pesquisas em Humanidades Digitais Para entender os potenciais das humanidades digitais para pesquisa, nada melhor que observarmos exemplos de pesquisas. Aqui seguem alguns de exemplos, selecionados pelo potencial de integração entre humanidades e métodos digitais, mais que quanto ao possível mérito/demérito científico. 2.1 Bibliometria / cientometria / cienciometria O estudo das citações, das rede de citações em artigos científicos foi talvez um dos pioneiros no uso de algumas das técnicas aqui descritas, existindo já há décadas. Chamada de “bibliometria”, “cientometria” ou “cienciometria”, ela conta citações de determinados autores em artigos científicos e tenta avaliar o quão influentes estes são. 2.1.1 Exemplo: Filósofos da ciência na Sociologia Por exemplo, HEDSTRÖM et al (1998), buscando saber a influência dos principais filósofos da ciência (Hempel, Kuhn, Popper e Wittgenstein) na sociologia em diferentes países e regiões (países nórdicos, EUA, Grã Bretanha, Alemanha e França), analisou o número de artigos nas principais revistas sociológicas que os citaram. (Fonte: HEDSTRÖM et al. 1998. p. 343) Pelos dados ali apresentados, Popper seria o filósofo mais influente na Europa, principalmente nos países de língua alemã, ao passo que Kuhn seria mais dominante nos EUA. HEDSTRÖM, Peter; SWEDBERG; and UDÉHN, Lars. Popper’s Situational Analysis and Contemporary Sociology. Philosophy of the Social Sciences 1998; 28; 339-64] 2.1.2 Exemplo: A teoria dos sistemas sociais de Niklas Luhmann Stephen Roth analisou a chamada diferenciação funcional dos subsistemas da sociedade mundial, isto é, como os subssitemas como política, economia, religião, ciência, direito, meios de comunicação de massa, etc. se autonomizam em relação aos outros, entre os anos de 1800 e 2000, e para tal utilizou dados do Google Ngram viewer, que por sua vez se baseia no Google Books (mais detalhes sobre estas ferramentes na seção sobre frequência de palavras). Ele encontrou, por exemplo, declínio da presença relativa (isto é, proporcional a cada ano) da palavra “Deus” (god) nos livros em inglês ao longo do tempo. Fonte: Roth (2014, p.46). E se examinarmos os termos relacionados aos Meios de Comunicação de Massa, podemos ver a importância relativa do termo “imprensa” (press) aumentando ao longo do tempo. Fonte: Roth(2014, p.47). Roth, Steffen. 2014. “Fashionable Functions: A Google Ngram View of Trends in Functional Differentiation (1800-2000)” International Journal of Technology and Human Interaction, 34–58. Uma versão posterior, um pouco mais elaborada em: Steffen Roth, Carlton Clark, Nikolay Trofimov, Artur Mkrtichyan, Markus Heidingsfelder, Laura Appignanesi, Miguel Pérez-Valls, Jan Berkel, Jari Kaivo-oja. Futures of a distributed memory. A global brain wave measurement (1800–2000). Technological Forecasting &amp; Social Change 118 (2017) 307–323 Dicas Há o pacote no Python Get Ngrams e o R possui o pacote ngramr que pega os dados no site Google Ngram, os coloca no formato de dataframe do R, bem como plota o gráfico no R usando o pacote ggplot Veja a sessão dedicada ao pacote ngramr neste manual 2.1.3 Exemplo: Tendência de termos chave da Sociologia Além de contar citação nas referências, pode-se contar as palavras mais frequentes no corpo do texto e compará-las. Bernau (2018), por exemplo, coletou dados do JSTOR’s Data for Research e plotou um gráfico longitudinal (ao longo do tempo) de frequência de palavas com termos chaves da sociologia, como “classe”, “raça” e “gênero” da revista American Sociological Review. Ele também disponibilizou o script em R que desenvolveu para esta análise. BERNAU, John A. Text Analysis with JSTOR Archives. November 12, 2018. https://doi.org/10.1177/2378023118809264 2.1.4 Exemplo: Tendências das correntes na filosofia Inspirado neste trabalho, Brian Weatherson usou também o pacote jstor_dfr, baixou dados da filosofia, e os clusterizou através de topic modeling. Podemos ver as tendências gerais de vários ramos da filosofia ao longo do tempo: Tendencias na história da filosofia O resultado e mais informações estão em seu livro online: WEATHERSON, Brian . A History of Philosophy Journals. Volume 1: Evidence from Topic Modeling, 1876-2013) 2.2 Exemplo: Google Trends como Proxy para epidemias A busca no Google por certos sintomas de doenças, ou melhor, a variação na busca por certas doenças e sintomas correlatos pode indicar que variação real da doença. Isso acontece, por exemplo, com sintomas de gripe. Um pico no aumento das buscas pelos sintomas indica um prenúncio do aumento das infecções, a ser checado/validado posteriormente. No artigo Google Trends: A Web-Based Tool for Real-Time Surveillance of Disease Outbreaks., os autores explicam que a ferramenta lançada em 2018: &gt; “Google Flu Trends can detect regional outbreaks of influenza 7–10 days before conventional Centers for Disease Control and Prevention surveillance systems” Para funcionar, há certas pré-condições sociais. Vale para gripe, e a ferramenta também prevê aumentos da Covid-19. Na reportagem da Piaui “No carnaval, buscas por “sintomas covid” voltaram a subir; sete dias depois, número de novos casos bateu recorde” de 09 de março de 2021 compara as buscas no google com casos reais. Gráfico: buscas por “sintomas Covid no Google” versus casos reais Ver também “Sintomas Covid” en Google trends:.Un indicador alternativo para el seguimiento de la incidencia de casos. com exemplos da Espanha, México, Chile e Argentina. No entanto, a ferramenta que parecia promissora falhou em prever o pico de gripe de 2013, sobrestimando por 140%. A empresa achou melhor terminar o projeto, conforme um artigo da Wired de 2015. Mais detalhes podem lidos no artigo The Parable of Google Flu: Traps in Big Data Analysis. O Instituto de Ciências Cognitvas de Osnabrück leva a ideia adiante, com modelo mais complexo, utilizando dados de redes sociais como Twitter e através do Watson da IBM.(site do projeto). O R possui o pacote gtrendsR que pega dados do Google Trends para trabalhar 2.3 Exemplo: Como as fake news sobre a pandemia de Covid-19 se assemelham/divergem entre os países? O relatório de 23/11/2020 sobre isolamento científico, intitulado “scientific [self] isolation” do Laut, Centro de Análise da Liberdade e do Autoritarismo, cruzou as checagens de fake news de 129 países diferentes (há uma plataforma que traduz as reportagens de fact checking de todo mundo para o inglês) e investigou a distribuição de notícias falsas sobre os tratamentos da Covid19. Os pesquisadores procederam então uma distribuição num plano das discussões nos países conforme sua semelhança. Quanto mais próximos, mais semelhantes os debates ao redor do tema. Encontraram então que o Brasil é o país mais isolado em sua discussão envolvendo certos medicamentos, no canto superior direito. Gráfico feito com o Iramuteq 2.4 Exemplo: Mudança de significado de palavras Kulkarni et al (2015) mostraram como através de ferramentas computacionais é possível identificar a mudança de significado de termos, seja ao longo de um século (com dados do Google NGram), seja em dinâmicas mais rápidas, como no twitter. Um dos termos analisado foi o “gay”: Linguistic Change da palavra “gay” Várias outras palavras foram analisadas, como “tape” que significava “fita adesiva”, mas passou a significar também “fita cassete” nos anos 1970; ou “apple” e “windows” que ganharam novo significado com a indústria da computação. KULKARNI, V., Al-Rfou, R; PEROZZI, B e SKIENA, S. Statistically Significant Detection of Linguistic Change. WWW 2015, May 18–22, 2015. http://dx.doi.org/10.1145/2736277.2741627 . 2.5 Exemplo: Análise de complexidade musical A reportagem da Folha de São Paulo Música brasileira foi simplificada ao longo das décadas, diz pesquisa cita o trabalho do cientista de dados Leonardo Sales (blog do autor), que analisou os acordes e vocabulário das letras, com base em 44 mil cifras e 102 mil letras raspadas de sites como cifras.com.br e letras.com.br em uma série de postagens: parte 1, parte 2, parte 3 sobre as letras, parte 4. Os códigos para raspagem de dados destes sites estão disponíveis em Python. grafico Como quase toda notação, há vantagens e desvantagens. Uma desvantagem, que levantou críticas, é que a análise se baseia em cifra, muito utilizada em músicas mais populares, mas inadequada para estilos mais complexos, como jazz. 2.6 Exemplo: Polarização Uma boa parte de pesquisas em política com métodos digitais se dedicou a analisar o fenômeno da polarização política. Christopher A. Bail, Lisa P. Argyle, Taylor W. Brown, John P. Bumpus, Haohan Chen, M. B. Fallin Hunzaker, Jaemin Lee, Marcus Mann, Friedolin Merhout, and Alexander Volfovsky. 2018. Exposure to opposing views on social media can increase political polarization. Proceedings of the National Academy of Sciences 115, 37 (Sept. 2018), 9216–9221. https://doi.org/10.1073/pnas.1804840115 Publisher: National Academy of Sciences Section: Social Sciences. Parte do trabalho de Franzosi descrevemos na seção sobre dados estruturados. Roberto P. Franzosi. Sociology, narrative, and the quality versus quantity debate (Goethe versus Newton): Can computer-assisted story grammars help us understand the rise of Italian fascism (1919–1922)? . Theor Soc (2010) 39:593–629 DOI 10.1007/s11186-010-9131-3 2.7 Exemplo de integração quali-quanti: Complementando dados qualitativos Uma dica de integração quali-quanti (qualitativo e quantitativo) usando análise textual vem do PEW Research. Aqui explicam como a partir de uma análise de grupos focais feita em 2019 com grupos dos EUA e da Grã Bretanha sobre atitudes frente a globalização/nacionalismo, complementaram com pesquisa quantitativa de análise textual, analisando as diferenças entre os grupos de cada país. Usaram técnicas como frequência de palavras, correlação de palavras e Topic modelling. Através destas análises, encontraram tópicos que se mostraram relevantes a serem incorporados em surveys futuros. DEVLIN, Kat.“How quantitative methods can supplement a qualitative approach when working with focus groups”. medium. Dec 18, 2020. 2.8 Exemplo de integração quali-quanti: Depurando dados quantitativos Nem sempre uma tabela de dados estruturados tem tudo estruturado, do modo que a sua pergunta de pesquisa necessite. Dentro de um campo específico de uma tabela pode-se precisar de desmembrar ainda mais os dados. Eis aqui um exemplo. Os pesquisadores do Dadoscope queriam investigar se houve aumento na abertura de igrejas evangélicas durante os anos Lula e Dilma. Os pesquisadores baixaram dados da Receita Federal referente ao Cadastro Nacional de Pessoa Jurídica e filtraram por “94.91–0–00 — Atividades de organizações religiosas ou filosóficas” no campo “Classificação Nacional de Atividades Econômicas”. O problema é que isto agrega não só igrejas evangélicas, como também católicas e de outras religiões e até agremiações filosóficas e institutos de psicanálise. Aqui entra a integração: Para tentarmos realizar a classificação das 150 mil igrejas evangélicas de maneira semi-supervisionada nós usamos Snorkel, uma biblioteca escrita em Python… foi preciso treinar um algoritmo de classificação usando uma amostra dos dados. De forma sucinta, os dados são separados em amostras que são usadas para treino, teste e validação da classificação. Para classificar os mais de 150 mil nomes únicos presentes na amostra de treino, criamos funções que classificam de forma grosseira as igrejas (e.g., se a palavra “assembleia” estiver presente, classificar a igreja como “evangélica”). Depois de escrever dezenas dessas funções, comparamos sua acurácia com uma amostra de teste de 5% dos nomes únicos, manualmente classificados por dois pesquisadores. Feito isso, usamos uma rede neural que combina em camadas estas funções e voilá: 91.8% de acurácia. Sabemos que este resultado não é perfeito, mas ele torna o trabalho de classificação viável. O artigo completo “Exclusivo: Igrejas evangélicas pentecostais tiveram boom de crescimento nos governos Lula e Dilma” pode ser lido aqui: artigo na Forum e o mesmo artigo na Medium. Caderno de notas no dadoscope para entender melhor como o processo foi realizado. Sobre a ferramenta utilizada, a Snorkel, ver a página do Github, introdução (em inglês) ao Snorkel. Por fim, outra dica para pensar a integração de dados quantitativos e qualitativos é a palestra de Dr. Christof Schöch: The Convergence of Quantitative and Qualitative Approaches, ocorrida no 1st Summerschool of Digital Humanities: Distant Reading - Potentials and Applications, em inglês. 2.9 Exemplo: Processo Civilizador O processo civilizador é uma tese consagrada pelo sociólogo Norbert Elias, de que, ao longo dos muitos anos, séculos, haveria um longo e lento processo de diminuição da violência e da demonstração de brutalidade cotidiana, no trato do dia à dia, do aumento do sentimento de vergonha e de intimidade. Três pesquisadores tentaram ver este processo com base nos arquivos de um tribunal, analisando 11.485 julgamentos, entre 1760 e 1913. Klingenstein S, Hitchcock T, DeDeo S. The civilizing process in London’s Old Bailey. Proc Natl Acad Sci U S A. 2014 Jul 1;111(26):9419-24. doi: 10.1073/pnas.1405984111. Com base nisto conseguiram observar uma tendência de longo prazo de progressiva distinção entre semântica de atos violentos e não violentos. 2.10 Ex.: Determinantes sociais do florescimento da cultura Incel Os celibatários involuntários ou Incels, são jovens, geralmente do sexo masculino, que não tem acesso aos meios sexuais, por isso “involuntários”. De um database de nada menos que 4 bilhões de tweets (entre 2012-2018), pegaram os dados de geolocalização de 321 milhões. Filtraram 3649 tweets que usaram linguagem peculiar aos incels e 3.745 sobre incels. Com estas informações em mãos, os pesquisadores descobriram informações importantes sobre os condicionantes sociais em que floresce a cultura incel. Brooks RC, Russo-Batterham D, Blake KR. Incel Activity on Social Media Linked to Local Mating Ecology. Psychological Science. January 2022. doi:10.1177/09567976211036065 https://txtlab.org/2015/11/how-i-predicted-the-giller-prize/ "],["estrutura-de-dados-e-tipos-de-formatos.html", "3 Estrutura de dados e tipos de formatos 3.1 Dados estruturados 3.2 Dados não estruturados 3.3 Dados semi-estruturados 3.4 Observações finais", " 3 Estrutura de dados e tipos de formatos Objetivos do capítulo: Apresentar a distinção entre dados estruturados, dados não estruturados e dados semi estruturados Apresentar alguns formatos de arquivos frequentes na análise textual e humanidades digitais, como csv e tsv, Json, markdown, yaml, LaTex, BibTex, xml e html. Podemos pensar a organização de dados quanto à sua estrutura de três formas: dados estruturados, dados semi estruturados e dados não estruturados. 3.1 Dados estruturados Formatos de arquivos estruturados são csv,xml, json, xls, xlsx, etc. Muitos destes possuem formato de tabela, o que torna bastante fácil encontrar a informação buscada. 3.1.1 Os formatos csv (comma separeted values) e tsv. O formato csv (comma separeted values ou “valores separados por vírgula”) é um dos mais simples, consiste de arquivo de texto simples, com valores separados por um caractere (ou conjunto de caracteres) que separam os valores em cada linha, sendo geralmente vírgula ou ponto e vírgula ou tabulação (tecla tab). Qualquer caractere ou conjunto de caracteres pode ser usado como separador de campos. Na imensa maioria dos casos cada linha é separada pela quebra de linha. Por exemplo, a seguinte tabela: Estado sigla capital região Acre AC Rio Branco Norte Alagoas AL Maceió Nordeste Amapá AP Macapá Norte Amazonas AM Manaus Norte Bahia BA Salvador Nordeste Ceará CE Fortaleza Nordeste Em abrirmos o csv no bloco de notas (notepad): Estado;sigla;capital;região; Acre;AC;Rio Branco;Norte; Alagoas;AL;Maceió;Nordeste; Amapá;AP;Macapá;Norte; Amazonas;AM;Manaus;Norte; Bahia;BA;Salvador;Nordeste; Ceará;CE;Fortaleza;Nordeste; O separador de campo neste arquivo CSV é o ponto e vírgula ;. Ao pedirmos ao computador para localizar qual a designação da sigla “AP”, ele saberá buscar facilmente esta informação. No caso ali, a vírgula é o separador de campos, mas qualquer outro caractere pode ser usado como separador. O formato .tsv, por exemplo, é separado por tabulação - ou o símbolo \\t. Mas é possível encontrar arquivo csv, porém com separador tipo “ ou”;“. 3.1.2 O formato Json O Json (“JavaScript Object Notation”, isto é “Notação de Objetos JavaScript”), é organizado no esquema de pares nome/valor. Por exemplo, ao separarmos primeiro nome firstName de sobrenome lastName no Json: {&quot;employees&quot;:[ { &quot;firstName&quot;:&quot;João&quot;, &quot;lastName&quot;:&quot;da Silva&quot; }, { &quot;firstName&quot;:&quot;Ana&quot;, &quot;lastName&quot;:&quot;Maria&quot; }, { &quot;firstName&quot;:&quot;Joaquim&quot;, &quot;lastName&quot;:&quot;Xavier&quot; } ]} O arquivo json inicia e termina com colchetes [] Todo Json é delimitado por chaves {}, Os dados são representados no esquema nome/valor \"nome\": \"valor\". Estes são separados por vírgula. O Json tem sido muito usado nas ciência de dados como um modo leve e fácil de armazenamento de dados. É possível que ao requisitar dados em um site, ele venha em Json. DICA: Caso queira mais detalhes sobre o formato Json: Video introdutório sobre o formato Json do canal Código Fonte TV JSON // Dicionário do Programador. Video introdutório, porém mais prático, focado na estrutura do mesmo: JSON em 6 minutos do canal “Canal TI”. Para ver as regras de sintaxe do Json. 3.2 Dados não estruturados Os dados não estruturados são a forma como encontramos em livros impressos, artigos, jornais, revistas, etc. São a forma de texto que nós humanos lemos normalmente. Por exemplo: “Algum tempo hesitei se devia abrir estas memorias pelo principio ou pelo fim, isto é, se poria em primeiro logar o meu nascimento ou a minha morte. Supposto o uso vulgar seja começar pelo nascimento, duas considerações me levaram a adoptar differente methodo: a primeira é que eu não sou propriamente um autor defunto, mas um defunto autor, para quem a campa foi outro berço; a segunda é que o escripto ficaria assim mais galante e mais novo. Moysés, que tambem contou a sua morte, não a poz no introito, mas no cabo: differença radical entre este livro e o Pentateuco….” Este tipo de texto, não estruturado, é alvo do Processamento de linguagem natural (PLN)/ Natural Language Process (NLP). 3.3 Dados semi-estruturados Dados semi-estruturados são um meio termo entre os estruturados e os semi estruturados. Por vezes são chamados de “auto-descritivos”. Vejamos exemplos destes. 3.3.1 Exemplos de dados semi-estruturados 3.3.1.1 Markup Códigos especiais, ou linguagem “markup” é uma notação de documento que tem duas apresentações, uma simplificada como texto normal para humanos, e outra com os “markup” para que o computador entenda. 3.3.2 O formato Markdown Um exemplo bem simples de markup é o Markdown, usado na escrita rápida de textos. Exemplo de markdown 3.3.3 O formato YAML O YAML (“YAML Ain’t Markup Language”) é um padrão de serialização de dados que prima por ser “human friendly”, isto é, de fácil leittura também para humanos. Em arquivos markdown tem-se usado o yaml como cabeçalho, com informações para a renderização do pdf, como título, subtítulo, resumo, palavras chave, etc. Ao converter markdown para o formato final, o computador irá interpretar estas informações. Um exemplo de yaml no arquivo markdown: --- title: &quot;Título do meu pdf&quot; subtitle: subtitulo qualquer author: Fulano de Tal # comentário qualquer fontsize: 12pt urlcolor: blue geometry: margin=2.5cm abstract: &gt; meu resumo bla bla bla bla --- # Titulo Texto texto texto texto texto texto texto ## Subtitulo Texto texto texto texto texto texto texto O cabeçalho em yaml é delimitado no seu início e fim por três traços consecutivos ---. Repare que o símbolo tralha # dentro do yml é interepretado como comentário, já no markdown, indica capítulo. DICA: Um modo prático de trabalhar na elaboração de textos - principalmente acadêmicos - com markdown e yaml é renderizá-lo com o pandoc, que é um canivete suíço na transformação de formatos de texto. Com ele, pode-se criar pdfs, html, doc, docs, odt, etc. a partir de seu arquivo markdown. Pandoc funciona via linha de comando. 3.3.4 O Formato LaTex O LaTex é uma linguagem usada na confecção, principalmente de textos (livros, artigos) acadêmicos, bem como apresentações. O formato LaTex permite grande flexibilidade, e é muito usado para escrever fórmulas matemáticas e gerar as referências bibliográficas automaticamente. Por isso, o LaTex é muito usado no contexto acadêmico. O seu formato mínimo pode ser visto assim: \\documentclass{article} \\begin{document} Olá Mundo \\end{document} Exemplo simples de texto em LaTex e sua renderização Ou em um exemplo um pouco mais elaborado: Exemplo de LaTex com o software Gummi no Linux Perceba que antes de \\begin{document}, isto é, no cabeçalho do documento temos várias informações, entre elas o título do artigo na linha 5 em title{}, e em \\author{}, nas linhas de 6 a 8, temos os autores. Temos também delimitados os capítulos ou seções, no caso ali em section{}. DICA: Para renderizar textos .tex em pdfs deve-se usar um interpretador. O pandoc é uma opção. Embora seja possível usar apenas o interpretador/conversor e um bloco de notas, o mais comum em Tex e LaTex é usar algum programa focado. O TeXstudio é uma boa opção. Caso use Linux e queria a renderização à medida que edita o texto, olhe o Gummi. Caso queira fazer os documentos em LaTex sem ter de “programar”, dê uma olhada no LyX. Há também editores de LaTex online, como o overleaf, que possibilita trabalhar em equipe, observando as alterações feitas por cada pessoa 3.3.5 O formato BibTex Um formato “irmão” do LaTex e markdown é o BibTex, um formato estruturado, com dados bibliográficos usado como fonte para gerar automaticamente a bibliografia ao final do texto renderizado em formatos como Tex, LaTex e markdown. As referências nesse formato ficam salvos num grande arquivo .bib. Um exemplo de citação dentro do bib: @book{Coleman:IntroMathSociology, address = {New York}, pages = {570}, publisher = {The Free Press of Glencoe Collie, Macmillan Limited}, title = {Introduction to Mathematical Sociology}, year = {1964} } O @book indica o tipo, podendo ser também, por exemplo, @article para artigos, @inbook para parte de um livro, @phdthesis para tese de phd (há mais opções). Para uma lista completa, ver The 14 BibTeX entry types. O Coleman:IntroMathSociology é o ID, a identificação única, que é também usado na citação do LaTex (Por exemplo, usando \\cite{Coleman:IntroMathSociology} dentro do Tex) ou do Markdown (usando [@IntroMathSociology] dentro do texto) para que o compilador saiba qual texto está sendo citado no texto. Podemos usar o texto que quisermos ali, desde que sem espaço. Exemplo de citação usando bibtex no LaTex Este arquivo bib que contém as referências bibliográficas, como é texto puro, pode ser editado num editor de texto comum, como o notepad, Gedit, etc. Mas o mais indicado é usar um software gestor de bibliografia, como o JabRef ou o KBibTex. O KBibTex possui menos recursos que o JabRef mas dá plenamente conta do recado, sendo inclusive o gerenciador que utilizo. Além de ser usado para gerar pdfs com as referências, o formato também pode ser usado em pesquisas bibliométricas. 3.3.6 Os formatos xml e html HTML, ou “Hyper Text Markup Language” é a linguagem padrão das páginas web No caso, nome seria “FirstName” e seu valor seria “João”, nome seria “lastName” e seu valor “da Silva” E esses mesmos dados no formato xml: &lt;employees&gt; &lt;employee&gt; &lt;firstName&gt;João&lt;/firstName&gt; &lt;lastName&gt;da Silva&lt;/lastName&gt; &lt;/employee&gt; &lt;employee&gt; &lt;firstName&gt;Ana&lt;/firstName&gt; &lt;lastName&gt;Maria&lt;/lastName&gt; &lt;/employee&gt; &lt;employee&gt; &lt;firstName&gt;Joaquim&lt;/firstName&gt; &lt;lastName&gt;Xavier&lt;/lastName&gt; &lt;/employee&gt; &lt;/employees&gt; Algumas linguagens usadas em texto são chamadas de markup, onde o que é mostrado na tela, para humanos lerem, difere do que o computador “entende”. Exemplo é o xml acima, o html ou ainda linguagens como markdown e LaTex. O html tem por base o xml. O html possui basicamente a seguinte estrutura &lt;!doctype html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Titulo da pagina&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;Título do capítulo&lt;/h1&gt; &lt;p&gt;Texto texto texto&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; Se você copiar o conteúdo acima e salvar num arquivo com o nome, digamos teste.html e abrí-lo com seu navegador de internet (Firefox, Chrome, Opera, etc.), verá como funciona esta ideia de markup. No caso do html, os valores são delimitados por tags, como: &lt;ALGO&gt;conteúdo&lt;/ALGO&gt;, onde &lt;/ indica que estamos fechando a tag. Assim, em &lt;h1&gt;Título do capítulo&lt;/h1&gt;, o texto “Título do capítulo” está entre a tag “h1”. Os arquivos html, assim como o LaTex, possui duas partes principais. O cabeçalho (head) e o corpo (body). Figure 3.1: Tatuagem head/body. Autor desconhecido Entre &lt;head&gt; e seu fechamento, &lt;/head&gt; ficam os metadados, como título, data, etc. Entre &lt;body&gt; e &lt;/body&gt; fica o conteúdo da página que aparece dentro do navegador. Um outro exemplo: &lt;name&gt;Joaquim José da Silva Xavier&lt;/name&gt;, o Tiradentes (&lt;local&gt;Fazenda do Pombal&lt;/local&gt;, batizado em &lt;data&gt;12 de novembro de 1746&lt;/data&gt; — &lt;local&gt;Rio de Janeiro&lt;/local&gt;, &lt;data&gt;21 de abril de 1792&lt;/data&gt;), foi um &lt;profissao&gt;dentista&lt;/profissao&gt;, &lt;profissao&gt;tropeiro&lt;/profissao&gt;, &lt;profissao&gt;minerador&lt;/profissao&gt;, &lt;profissao&gt;comerciante&lt;/profissao&gt;, &lt;profissao&gt;militar&lt;/profissao&gt; e &lt;profissao&gt;ativista político&lt;/profissao&gt; &lt;gentilico&gt;brasileiro&lt;/gentilico&gt;, que atuou nas capitanias de &lt;local&gt;Minas Gerais&lt;/local&gt; e &lt;local&gt;Rio de Janeiro&lt;/local&gt;. Onde podemos ver tags como &lt;name&gt;, &lt;local&gt;, &lt;data&gt;, etc. ao redor de certas informações, o que torna possível ao computador encontrar estas informações. Para tutorial gratuito (em inglês) sobre html, ver W3 School. 3.3.7 Formatos mais raros Há ainda a possibilidade de uso de estruturação de texto não muito comuns e com fins bem específicos. Por exemplo, Franzosi (2010) ao fazer análise da narrativa de jornais italianos da época de ascensão do Fascismo, passou textos não estruturados como este: Republicans plunged in Bissone di S. Cristina around 10pm of this month at the pub Prati. A guy, who went by the name of “captain,” took out a list of names and did the roll call loudly. Para o seguinte formato: [Semantic triplet 1: [Participant: [Actor: republicans]] [[Process: [[Verb: plunge] [Circumstances: [Space: [City: Bissone di S. Cristina] [[Location: pub] [Name: Prati]]]] [[Time: [Date: 05/07/1921] [Hour: 10pm]]]]] [Semantic triplet 2: [Participant: [Actor: captain]] [Process: [[Verb: does roll call] [Circumstances: [Type of action: loudly] [Instrument: list]]] [Participant: [Actor: workers]] Para tal, Franzosi desenvolveu um software para análise de narrativas textuais, o PC-ACE (Program for Computer-Assisted Coding of Events) e pôde ter uma noção melhor da violência cotidiana na época, gerado tabelas como esta: Lista da ocorrências diárias de triplets de violência, obtidas no jornal Avanti! (FRANZOSI, p.607) Frequência da distribuição dos triplets de violência no jornal Avanti! (FRANZOSI, p.607) E ainda fez um “mapa de calor” (“heat map”) com a localização da violência fascista na Itália Mapa de calor de ações fascistas de violência e localização de suas sedes insittucionais (FRANZOSI, p.609) Referência: FRANZOSI, Roberto P.. Sociology, narrative, and the quality versus quantity debate (Goethe versus Newton): Can computer-assisted story grammars help us understand the rise of Italian fascism (1919–1922)?. Theor Soc (2010) 39:593–629. DOI 10.1007/s11186-010-9131-3 3.4 Observações finais Os dados semi-estruturados não tem, portanto, formato de tabela, mas contêm indicações de informações mais abstratas, através de tags ou outras marcações. Com base nestas informações que faremos análises de texto de modo computacional. Este processo de transformação de dados não estruturados em estruturados é chamado de “datificação”. Os formatos como Json, csv, tsv, xml são importantes pois ao solicitar dados em diversos sites, muitos APIs retornam dados nestes formatos. "],["noções-básicas-de-programação-em-r.html", "4 Noções básicas de programação em R 4.1 Sequências de scaping 4.2 Variável e atribuição 4.3 Funções no R 4.4 Condicionais: se/então, If/else 4.5 Operadores 4.6 Loops, repetições", " 4 Noções básicas de programação em R Objetivos deste capítulo: Apresentar uma breve introdução aos principais comandos usados em programação 4.1 Sequências de scaping Há alguns caracteres especiais que não podem ser usados diretamente numa string, pois possuem um significado especial. As aspas são um caso. Assim, por exemplo: variavel &lt;- 'moinho d'agua' retorna erro. Para poder utilizar as aspas, temos de dizer ao programa para “escapar”. Isto é feito de modo geral com o caractere \\. Assim, para funcionar, nosso exemplo fica: variavel &lt;- 'moinho d\\'agua'. No caso de aspas simples ' ou duplas \", é possível escapar estes caracteres colocando um dentro do outro. Por exemplo: variavel &lt;- \"moinho d'agua\" ou variavel &lt;- 'O conto \"Trio em Lá Menor\" de Machado de Assis foi publicado em 1896.'. Outros caracteres que levam escape são: 4.1.1 Comentando o código Nas linguagens de programação é possível acrescentar comentários para orientar os humanos e que não serão lidos pelo computador. Cada linguagem tem o seu próprio padrão. No R, assim como em muitas linguagens de programação, os comentários no código são feito com a tralha # ou hashtag, jogo da velha, etc. Tudo que vier depois deste símbolo é então ignorado. print(&quot;Olá mundo!&quot;) ## [1] &quot;Olá mundo!&quot; print(&quot;Olá mundo R!&quot;) # comentário que o computador ignora ## [1] &quot;Olá mundo R!&quot; Se retirarmos o símbolo # temos então mensagem de erro. print(&quot;Olá mundo! Olá R&quot;) comentários que dará erro ## Error: &lt;text&gt;:1:28: unexpected symbol ## 1: print(&quot;Olá mundo! Olá R&quot;) comentários ## ^ 4.2 Variável e atribuição Em programação, no processo de automatização de tarefas, o uso de variáveis é essencial. Ele permite que reutilizemos as coisas, através de nomes mais simplificados. Normamente, usa-se o símbolo de igual =para atribuir algo a uma variável, no seguinte modo: Noma_da_variavel = valores O R também usa este símbolo, mas também usa outro que lhe é prório, o &lt;- que lembra uma seta. nome1 = &quot;Fulano&quot; nome2 &lt;- &quot;Ciclano&quot; message(&quot;Um nome é &quot;, nome1, &quot;, o outro é &quot;, nome2) ## Um nome é Fulano, o outro é Ciclano Pode-se usar na direção contrária também &quot;Beltrano&quot; -&gt; nome3 nome3 ## [1] &quot;Beltrano&quot; Há algumas regras para criar variáveis. Não conter espaço no nome é uma delas. Ao invés disso, pode-se usar o underscore (Ex.: bla_ble) ou ponto (bla.ble). Há alguns caracteres especiais que não podem ser usados diretamente numa string, pois possuem um significado especial. As aspas são um caso. Assim, por exemplo: variavel &lt;- 'moinho d'agua' retorna erro. Para poder utilizar as aspas, temos de dizer ao programa para “escapar”. Isto é feito de modo geral com o caractere \\. Assim, para funcionar, nosso exemplo fica: variavel &lt;- 'moinho d\\'agua'. No caso de aspas simples ' ou duplas \", é possível escapar estes caracteres colocando um dentro do outro. Por exemplo: variavel &lt;- \"moinho d'agua\" ou variavel &lt;- 'O conto \"Trio em Lá Menor\" de Machado de Assis'. 4.3 Funções no R Identificamos as funções no R por elas sempre virem acompanhados dos parênteses, como function(). Criar nossas próprias funções é útil para evitar retrabalho. Possuem a seguinte sintaxe: # criando uma função no R nome_da_funcao &lt;- function(argumento_1, argumento_2, ...) { fazendo_algo_com(argumento_1) fazendo_algo_mais_com(argumento_2) } # rodando a função nome_da_funcao(argumento_1,sargumento_2) E com um exemplo: minhaFuncao &lt;- function(entrada){ entrada + 10 } minhaFuncao(2) ## [1] 12 minhaFuncao(6) ## [1] 16 minhaFuncao2 &lt;- function(entrada1, entrada2){ entrada1 + entrada2 } minhaFuncao2(2,4) ## [1] 6 minhaFuncao2(1,7) ## [1] 8 arg_1, arg_2 são os parâmetros de entrada na função, que serão processados. Podemos ver quais argumentos são requeridos por determinada função através do comando args() (que por sua vez, também é uma função). args(mean) ## function (x, ...) ## NULL args(median) ## function (x, na.rm = FALSE, ...) ## NULL Ao chamar uma função, passa-se argumentos para esta processar e nos retornar um determinado resultado. No R, repassamos argumentos 1) pela ordem apresentado em args(), ou 2) indicando qual valor para qual argumento. Nesta segunda opção, podemos enviar argumentos fora de ordem. Ao criar nossas próprias funções, os argumentos usados na entrada também podem conter valores padrão, caso a informação não lhe seja repassada à função. Em um exemplo simples de função: x &lt;- function(){ print(&#39;Olá Mundo&#39;) } x() ## [1] &quot;Olá Mundo&quot; Ou um outro exemplo, com a função tendo argumentos de entrada: # Criando função com valores padrão: minhafuncao &lt;- function(a = 2, b = 3) { resultado &lt;- a + b print(resultado) } # chamando a função sem nenhum argumento. minhafuncao() ## [1] 5 # chamando a função repassando argumentos. minhafuncao(10,4) ## [1] 14 # chamando a função repassando parte dos argumentos. minhafuncao(,4) ## [1] 6 Geralmente, mas não sempre, como fazem transformações, funções são nomeadas com verbos, como “filtrar”, “mutar”, “cortar”, , etc. Isso nos ajuda a compreender mais intuitivamente do que se trata a função. Quanto aos tipos de argumentos, esses podem ser 1) os dados a serem computados, ou 2) detalhes dos argumentos, especificando melhor um modo específico/diferente do padrão para computar estes dados. Por exemplo, ao observarmos os argumentos da função cor(), de correlation/correlação: args(cor) ## function (x, y = NULL, use = &quot;everything&quot;, method = c(&quot;pearson&quot;, ## &quot;kendall&quot;, &quot;spearman&quot;)) ## NULL x e y: indicam os dados que entrarão na função. De modo geral - mas nem sempre! - os dados aparecem como primeiro argumento das funções. Este é o padrão sobretudo nas funções mais recentes do R, mas vamos encontrar algumas funções onde isso não é verdade, como na função grep. method = c(\"pearson\", \"kendall\", \"spearman\") mostram os argumentos possíveis em “method”, no caso desta função, especificando o tipo de correlação que queremos. Para especificarmos então qual destas usar, digamos “spearman”, o argumento ficaria method = \"spearman\". 4.4 Condicionais: se/então, If/else Se uma condição, então faça algo. Se outra condição, então faça outra coisa. else: Se nenhuma das condições anteriores for satisfeita, então faça o seguinte. Em sua versão mais simples if (condição) { fazer_algo() } Se certa condição for satisfeita, for verdadeira, então algo especificado será feito. Se certa condição não for satisfeita, for falsa, então nada será feito. Perceba que nossa função apenas retorna algo se o argumento de entrada for maior que 5. Caso contrário - como foi com o valor 3 - ela nada faz. if(3 &gt; 5){ print(&quot;X é maior que 5&quot;) } if(14 &gt; 5){ print(&quot;X é maior que 5&quot;) } ## [1] &quot;X é maior que 5&quot; 4.4.1 Else Uma versão mais elaborada da condicional envolve a condição else if (condição) { fazer_algo() } else { fazer_outra_coisa() } Se certa condição for satisfeita, for verdadeira, então algo especificado - a função fazer_algo() - será realizada. Se certa condição não for satisfeita, for falsa, então outra coisa será feita. Ex.: minhafuncao &lt;- function(arg1){ if(arg1 &gt; 5){ message(arg1, &quot; é maior que 5&quot;) } else { message(arg1, &#39; não é maior que 5&#39;) } } minhafuncao(3) ## 3 não é maior que 5 minhafuncao(13) ## 13 é maior que 5 4.4.2 else if Podemos colocar mais condições intermediárias, quantas quisermos, com else if comes &lt;- function(arg1){ if (arg1 == &quot;laranja&quot;){ print(&quot;Prefiro tamarindo&quot;) } else if (arg1 == &quot;rucula&quot;){ print(&quot;bleaaaargh&quot;) } else if (arg1 == &quot;sanduiche&quot;){ print(&quot;Com suco de tamarindo!&quot;) } else { print(&quot;Só sei com laranjas&quot;) } } # chamando a função acima comes(&quot;laranja&quot;) ## [1] &quot;Prefiro tamarindo&quot; comes(&quot;rucula&quot;) ## [1] &quot;bleaaaargh&quot; comes(&quot;sanduiche&quot;) ## [1] &quot;Com suco de tamarindo!&quot; comes(&quot;abacaxi&quot;) ## [1] &quot;Só sei com laranjas&quot; comes(&quot;morango&quot;) ## [1] &quot;Só sei com laranjas&quot; Combinando função e If/Else verifica &lt;- function(arg_entrada1, arg_entrada2){ if (arg_entrada1 &gt; arg_entrada2) { message(arg_entrada1, &#39; é maior que &#39;, arg_entrada2) } else { message(arg_entrada1, &#39; não é maior que &#39;, arg_entrada2) } } Chamando a função verifica(1,5) ## 1 não é maior que 5 verifica(7,2) ## 7 é maior que 2 Há ainda uma versão compacta de condicionais com a função ifelse(), e que segue a mesma lógica: ifelse(condição, se TRUE faça X, Se FALSE faça Y) a &lt;- c(5, 6, 3, 9, 7) ifelse(a &gt; 5 ,&quot;maior&quot;,&quot;menor&quot;) ## [1] &quot;menor&quot; &quot;maior&quot; &quot;menor&quot; &quot;maior&quot; &quot;maior&quot; 4.5 Operadores Usamos operadores para realizar transformações ou comparações entre valores ou variáveis (que por sua vez, contém valores). 4.5.1 Operadores de atribuição (assignment) Os operadores de atribuição são aqueles que vimos na sessão “variável”, que são =, &lt;-, &lt;&lt;-, -&gt; e -&gt;&gt;. 4.5.2 Operadores Aritiméticos Operador Descrição + adição - subtração * multiplicação / divisão ^ ou ** exponencial x %% y modulus: ao dividir x por y, %% retorna o resto 5+7 ## [1] 12 2-1 ## [1] 1 4*2 # 4 vezes 2 ## [1] 8 15/3 # 15 dividido por 3 ## [1] 5 2^3 # dois elevado a 3 ## [1] 8 3**2 # 3 elevado a 2 ## [1] 9 Como no operador %% nos dá a sobra da divisão (relembrando, os outros elementos da divisão são dividendo, divisor, quociente ou produto), a operação x %% y == 0 nos diz se há ou não resto, e portando, diz se x é divisível por y. 10 %% 3 ## [1] 1 10 %% 5 ## [1] 0 # De 1 a 10, quais são múltiplos de 2? (retorna lógicos) 1:10 %% 2 == 0 ## [1] FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE # múltiplos de 2 até 10 (1:10)[1:10 %% 2 == 0] ## [1] 2 4 6 8 10 # múltiplos de 3 até 15 (1:15)[1:15 %% 3 == 0] ## [1] 3 6 9 12 15 Há ainda o operador %/%, utilizado em divisão integral. 4.5.3 Operadores relacionais Operador Descrição &lt; menor que &lt;= menor ou igual a &gt; maior que &gt;= maior ou igual que == exatamente igual a != diferente de 5&gt;6 ## [1] FALSE 7&lt;=6 ## [1] FALSE variavel &lt;- &quot;teste&quot; variavel == &quot;abacate&quot; ## [1] FALSE variavel != &quot;TESTE&quot; ## [1] TRUE 4.5.4 Operadores booleanos E OU NÃO AND OR NOT Símbolos &amp; | ! No R também temos &amp;&amp; || ! Operador Descrição != não igual a !x não x x | y x ou y x &amp; y x E y isTRUE(x) teste se X é TRUE https://stat.ethz.ch/R-manual/R-devel/library/base/html/Logic.html txt &lt;- c(&quot;bla bla&quot;, &quot;bla ble&quot;, &quot;bla bli&quot;, &quot;bla non&quot;) grep(&#39;bla (bla|ble)&#39;, txt, value =T) ## [1] &quot;bla bla&quot; &quot;bla ble&quot; y &lt;- c(&quot;bla&quot;, &quot;ble&quot;, &quot;bli&quot;) y==&quot;bla&quot; ## [1] TRUE FALSE FALSE y!=&quot;bla&quot; ## [1] FALSE TRUE TRUE Algumas buscas na internet disponibilizam alguns operadores. No portal da Cãmara dos deputados, pode-se buscar discursos dos parlamentares no banco de discursos utilizando “and” e “or” (link aqui). Na busca do Google, é possível excluir termos da busca (no caso, o operador “NOT”), utilizando o sinal de menos antes do termo. 4.5.5 Outros operadores Operador Descrição : Cria uma sequência de números em uma sequência %in% Se um elemento pertence/está contido em um vetor de elementos %*% Multiplicação de matrizes # geranto automaticamente uma sequencia de 5 a 10 x &lt;- 5:10 x ## [1] 5 6 7 8 9 10 # gerando uma sequencia de letras minúsculas letters[1:5] ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; # gerando uma sequencia de letras miúsculas LETTERS[5:10] ## [1] &quot;E&quot; &quot;F&quot; &quot;G&quot; &quot;H&quot; &quot;I&quot; &quot;J&quot; # checando se os numeros estão contidos no vetor x 2 %in% x ## [1] FALSE 7 %in% x ## [1] TRUE # Num outro exemplo, vamos usar o operador para checar intesecção entre dois vetores v1 &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, 1, 2, 67, 53, 73) v2 &lt;- c(&quot;x&quot;, &quot;y&quot;, &quot;z&quot;, &quot;2&quot;, &quot;h&quot;, &quot;j&quot;, &quot;c&quot;) # obtendo um vetor com booleanos (TRUE e FALSE) de todos os elementos v1 %in% v2 ## [1] FALSE FALSE TRUE FALSE TRUE FALSE FALSE FALSE # observando apenas os elementos que tem intersecção entre os vetores v1[v1 %in% v2] ## [1] &quot;c&quot; &quot;2&quot; Num exemplo prático do operador %in%, o comando colors() lista as cores que possuem nome no R. Com o operador %in%, podemos checar se um nome de cor específico ou um vetor de nomes está presente no vetor de cores que o comando colors() retorna. &quot;darkred&quot; %in% colors() ## [1] TRUE # Um vetor c(&quot;darkred&quot;, &quot;lightred&quot;) %in% colors() ## [1] TRUE FALSE 4.6 Loops, repetições 4.6.1 For Loops Quando necessita de fazer uma mesma operação, repetida, em vários itens de uma lista, usamos os loops. Com loops iteramos, aplicamos uma operação a cada item de uma série de itens. for (item in variavel_com_varios_itens){ faça algo com cada item n da variável } Por exemplo: x &lt;- c(1, 3, 5) # o loop abaixo pegará os elementos 1,3 e 5 # e adicionará 1 a cada um deles. for (i in x){ print(i+1) } ## [1] 2 ## [1] 4 ## [1] 6 Também podemos aplicar loops a strings: x &lt;- c(&#39;bla&#39;, &#39;ble&#39;) for (i in x){ print(paste(&#39;Alguém disse: &#39;, i)) } ## [1] &quot;Alguém disse: bla&quot; ## [1] &quot;Alguém disse: ble&quot; nomes &lt;- c(&quot;Fulano&quot;, &quot;Beltrano&quot;, &quot;Maria&quot;, &quot;Karen&quot;) for(i in nomes) { # o comando nchar() conta quantos caracteres há no item print(paste(&quot;O nome&quot;, i, &quot;contém&quot;, nchar(i), &quot;caracteres.&quot;)) } ## [1] &quot;O nome Fulano contém 6 caracteres.&quot; ## [1] &quot;O nome Beltrano contém 8 caracteres.&quot; ## [1] &quot;O nome Maria contém 5 caracteres.&quot; ## [1] &quot;O nome Karen contém 5 caracteres.&quot; 4.6.2 While loop O for loops é usado quando temos uma lista que sabemos quantos elementos temos. Para o caso de não sabermos ao certo quantos e quais elementos temos, há ainda um outro tipo de loop em programação, o while loop, que diz enquanto uma condição for verdadeira, ela continuará rodando, e irá parar assim que for falsa. # criando a variável &quot;i&quot; de valor 1 i &lt;- 1 # Enquanto a condição i for menor que 7, continue while (i &lt; 7) { # imprime a mensagem message(i, &quot; é menor que 7&quot;) # adiciona 1 ao valor de i i &lt;- i + 1 # volta ao início } ## 1 é menor que 7 ## 2 é menor que 7 ## 3 é menor que 7 ## 4 é menor que 7 ## 5 é menor que 7 ## 6 é menor que 7 No exemplo acima, criamos a variável “i” que tem valor 1, e dentro do loop há a operação de acrescentar +1 a cada “rodada” do loop, que começa com o valor “1”. A cada rodada, o loop avalia se a condição i &lt; 7 é verdadeira. Se sim, imprime o valor atual da variável i e adiciona +1 ao valor de i. O loop continua rodando para o próximo item. Assim que chega em 7, o loop pára, já que “7 &lt; 7” é falso. 4.6.3 apply (lapply, sapply, tapply e mapply) Além de for loop e while loop, no R temos a família apply (lapply, sapply, tapply e mapply), que funciona como um for loop normal, mas que pode simplificar um pouco a repetição, além de ser mais rápido que o for loop. Entretanto, o apply funciona aplicando uma função já pré definida anteriormente. Assim, no nosso exemplo com o vetor nomes &lt;- c(&quot;Fulano&quot;, &quot;Beltrano&quot;, &quot;Maria&quot;, &quot;Karen&quot;) Em que aplicamos o for loop for(i in nomes) { # o comando nchar() conta quantos caracteres há no item print(nchar(i)) } ## [1] 6 ## [1] 8 ## [1] 5 ## [1] 5 Podemos fazer algo parecido com o apply sapply(nomes, nchar) ## Fulano Beltrano Maria Karen ## 6 8 5 5 Além de forloops e da família apply, o R conta também com: família de funções map_ do pacote purrr do tidyverse pacote foreach, que possibilita forloops com processamento paralelo, isto é, mais rápido. As funções do purrr também podem ser paralelizadas para ficarem mais rápidas através do pacote furrr. "],["introdução-ao-r.html", "5 Introdução ao R 5.1 Obtendo Ajuda no R 5.2 R em modo gráfico: RKWard e RCommander 5.3 Tipos de Dados no R (data types) 5.4 Estrutura de dados no R (Data Structures) 5.5 Instalando pacotes no R 5.6 A suíte de pacotes tidyverse 5.7 Manipulando data e hora 5.8 Links e Dicas", " 5 Introdução ao R Objetivos do capítulo: Apresentar como obter ajuda no R Apresentar os tipos de dados e as estruturas de dados no R Apresentar a suíte de pacotes do tidyverse Supondo que já tenha o R e o RStudio instalados (há diversos tutoriais de instalação no Youtube), vamos aos primeiros passos. É possível também rodar o R de modo online, de modo gratuito, sem instalar nada em seu computador, com o One Compiler e o Snippets, tendo mais de 19 mil pacotes instalados. O lado negativo é que não é um modo prático e tão rápido como rodar em seu computador, mas pode servir para pequenos testes. Snippets: um site para rodar R online Outro modo de rodar o R online é criar uma conta no RStudio Cloud, que possui também modalidade de conta gratuita, com algumas limitações: 15 horas de uso mensais, 1 Gb de Ram (pouco, mas funciona). Vantagem do Rstudio Cloud: a instalação de pacotes ocorre sem maiores problemas. Desvantagem: be mais lento que o RStudio Desktop. RStudio Cloud: rodando o RStudio nas nuvens através de navegador de internet HISTÓRIA A linguagem R foi criada no ano de 1993 por Ross Ihaka e Robert Gentleman do Departamento de Estatística da Universidade de Auckland, da Nova Zelândia, se baseando na linguagem S (por sua vez, criada em 1976). O anúncio oficial de lançamento do R ocorreu em 23 de abril de 1997. IHAKA, Ross; GENTLEMAN, Robert. “R: A Language for Data Analysis and Graphics”. Journal of Computational and Graphical Statistics. 5 (3): 299. set.1996 doi:10.2307/1390807. ISSN 1061-8600. JSTOR 1390807. O Podcast History of Computing fez um episódio sobre a história do R (em inglês) link: player fm. R é uma linguagem aberta e gratuita, mutiplataforma (roda em Linux, Windows e Mac), publicada sob a licença General Public License (GNU). O R é criado por estatísticos e é bem popular no mundo acadêmico. O R vem com diversos pacotes e funções nativamente. Só para ter uma ideia desta quantidade: Pacote Número de funções base 1244 datasets 104 graphics 87 grDevices 112 methods 203 utils 215 stats 449 Dificilmente vamos dominar estes pacotes e mais as funções dos pacotes específicos que iremos instalar 5.1 Obtendo Ajuda no R A primeira coisa a se aprender é como conseguir ajuda, como conseguir a informação que precisamos. A busca por ajuda é uma constante, mesmo entre os mais experiente. Obtendo ajuda no RStudio Comando Descrição Exemplo de uso help.start() para abrir o sistema de ajuda em HTML no seu navegador help.start() help(\"função\") ou ?funcao() Acesso à documentação de funções, data sets e outros. No Rstudio, a ajuda é aberta na aba “Help”. Caso rode direto no console, sem o RStudio, uma vez dentro da página de ajuda, digite / para realizar busca e q para sair ?getwd ou help(getwd) help(função, package=\"NOME_PACOTE\") para obter ajudar de um pacote que não foi cerregado help(rlm, package=\"MASS\") Ajuda para a função rlm() do pacote MASS help(package = 'NOME_PACOTE') Mostra um índice de páginas de ajuda para o pacote help(package = 'dplyr' example() Mostra exemplos de uso da função example(grep) help.search() ou ??busca Para busca mais vaga. Caso não se lembre do nome exato da função, ou caso busque uma função que faça determinada tarefa ??regression mostra diversas funções de diversos pacotes que contém a palavra “regression” RSiteSearch() Busca no seu navegador (browser) um padrão/assunto em sites especializados em R RSiteSearch(\"text analysis\") args() Mostra os argumentos que uma função pode receber. args(grep) apropos() Busca pelo nome de uma função. Útil quando não lembramos exatamente o nome de uma função apropos(\"grep\") demo() Lista todas as demonstrações de todos os seus pacotes demo() demo(package=\"package-name\") lista as demonstrações de um pacote particular demo(package=\"stats\") lsf.str(\"package:nome_do_pacote\") Mostra as funções do pacote (que precisa já ter sido carregado) lsf.str(\"package:dplyr\") ls(\"package:nome_do_pacote\") Mostra os objetos do pacote (que precisa já ter sido carregado) ls(\"package:dplyr\") search() Lista os pacotes já carregados search() Fonte: Versão expandida de Getting Help with R. Uma boa dica de mecanismo de busca (como o Google, Bing, DuckDuckGo, etc.) e que é especializado em R é o https://rseek.org/. Uma dica adicional é acrescentá-lo como mais uma opção de buscador em seu navegador de internet (ver figura mais abaixo). Buscador rseek: buscando apenas assuntos relacionados ao R Adicionando o Rseek como uma opção de buscador no Firefox # Buscando por funções que contenham &quot;grep&quot; apropos(&quot;grep&quot;) ## [1] &quot;agrep&quot; &quot;agrepl&quot; &quot;grep&quot; &quot;grepl&quot; &quot;grepRaw&quot; # Mostrando os argumentos que uma função pode receber args(gsub) ## function (pattern, replacement, x, ignore.case = FALSE, perl = FALSE, ## fixed = FALSE, useBytes = FALSE) ## NULL Outra dica é buscar a documentação do pacote no site do cran ou buscar por problemas específicos no o site Stack Overflow. 5.2 R em modo gráfico: RKWard e RCommander Caso deseje rodar análises estatísticas tradicionais, - não para análise textual - há pelo menos dois modos de rodar o R em modo gráfico (GUI), usando cliques de mouse. Um é através do RKWard que funciona como um SPSS/PSPP, porém, tendo o R como linguagem base. A segunda alternativa é o Jamovi, também livre e aberto. Uma terceira alternativa é o Rcmdr ou R Commander. Ao carregar este pacote dentro do R, uma janela aparece onde escolhemos o tipo de operação, e ele mostra o código para tal. O Rcmdr ensina como importar dados (SPSS, Excell, Stata, ruls, etc.), como realizar diversas operações estatísticas, como plotar gráficos. Ao rodar o Rcmdr pela primeira vez, pacotes adicionais serão instalados, como na imagem abaixo. Instalação do R Commander. Instalando pacotes adicionais Após a instalação de pacotes adicionais, O R Commander abre uma janela com os comandos. R Commander em ação Caso feche a janela do RCommander, rodar library(Rcmdr) novamente não abrirá a janela, uma vez que o pacote já está carregado. A solução é rodar então Commander() para ter a janela novamente. Caso queira fechar este pacote, basta digitar no Console detach(\"package:Rcmdr\", unload=TRUE) ou apenas feche o R. Para trabalhar com análise textual com estes acima: Há um plugin do RCmd para análise textual, o RcmdrPlugin.temis (porém, última atualização em 2018). No caso do RKward, o pacote koRpus oferece plugin GUI, com funcionalidades para a língua portuguesa. Dicas: R Commander LAURETTO, Marcelo. Introdução ao R Commander. (Pequeno tutorial do RCommander, em PDF e em português.) The R Commander: A Basic-Statistics GUI for R FOX, John. Getting Started With the R Commander. (Artigo do criador do RCommander) Video tutorial do R Commander em ação e em português: Aprendendo Estatística com o R Commander do Departamento de Estatística UFLA 5.2.1 NVIM-R Por fim, para os usuários mais avançados quem não tem medo do terminal e gosta de usar o editor Vim ou o Neovim, uma dica é usar o plugin Nvim-R que traz diversas facilidades para usar o Nvim em conjunto com o R. Trata-se de uma alternativa leve e rápida ao Rstudio. (O autor do pacote é um professor brasileiro do departamento de Sociologia da UFC). 5.3 Tipos de Dados no R (data types) “Tudo que existe no R é um objeto” (John Chambers, apud WICKHAM) Tipos de dados se referem à forma mais simples de objetos, ou tipos de dados atômicos do R (R atomic data types) Os seis tipos de dados básicos são: Tipo de dado no R Exemplo de uso 1 character ou “string” (texto) “a”, “bla”, “Fulano de tal” 2 numeric (real ou decimal) -2, 43, 3,333333 3 integer (integral) 3L (O “L” é o modo que o R entende que é um integral) 4 logical (lógico) TRUE ou FALSE 5 complex (complexo) 1-4i (números complexos com partes reais e imaginárias) 6 raw (bytes, para arquivos com dados binários) O raw é de uso raro. &gt; charToRaw('olá') resulta em: 6f 6c c3 a1 Integrais ou “integer” são um tipo de dado sem frações, ou números inteiros, podendo ser positivos ou negativos. Tipos reais, ou float são números que possuem fração. Estes nomes tem a ver com o modo com o computador salva esta informação. O float possui tamanho de 4 bytes, e se o número possuir mais de 7 dígitos, o valor é arredondado para 7. Já o “double” possui 8 bytes, o “dobro” do float, podendo chegar a 15 dígitos de acurácia. o que lhe confere maior acurácia, mas também ocupa mais espaço e requer mais processamento computacional. Se quiser saber melhor sobre float e double, veja este artigo “Float vs Double”). Algumas linguagens possuem até o tipo “long double” ou big decimal, que possuem acurácia maior que o double. Portanto, “double” tem recebe este nome pelo modo como é salvo no computador. Uma vez criados os dados, para examinar as características dos objetos há algumas funções no R Função no R para examinar tipos de dados Descrição Exemplo class() Que tipo de objeto é? class(3), class(\"bla\") length() Qual o tamanho do vetore? Quantos itens possui? | ` length(c(“bla”,“ble”))` attributes() Possui metadados? # Criando um texto (character). Tem de ser delimitado entre aspas minha_variavel_texto &lt;- &quot;bla&quot; minha_variavel_texto ## [1] &quot;bla&quot; class(minha_variavel_texto) ## [1] &quot;character&quot; # Criando uma variável numérica. Não pode ser delimitado por aspas. minha_variavel_numerica &lt;- 2 minha_variavel_numerica ## [1] 2 class(minha_variavel_numerica) ## [1] &quot;numeric&quot; # Criando um integral: minha_variavel_integral &lt;- 5L minha_variavel_integral ## [1] 5 class(minha_variavel_integral) ## [1] &quot;integer&quot; # Criando um variável lógica. Também sem aspas. ## 5 é maior que 3? minha_variavel_logica &lt;- 5 &gt; 3 minha_variavel_logica ## [1] TRUE class(minha_variavel_logica) ## [1] &quot;logical&quot; ## 5 é menor que 3? minha_variavel_logica2 &lt;- 5 &lt; 3 minha_variavel_logica2 ## [1] FALSE minha_variavel_logica3 &lt;- TRUE # É possível atribuir minha_variavel_logica3 ## [1] TRUE minha_variavel_logica4 &lt;- F # Também é possível usar abreviaturas &quot;T&quot; e &quot;F&quot; minha_variavel_logica4 ## [1] FALSE # Criando um número complexo: numero_complexo &lt;- 3 + 2i numero_complexo ## [1] 3+2i class(numero_complexo) ## [1] &quot;complex&quot; # Convertendo para raw ## convertendo character para raw meuraw &lt;- charToRaw(&#39;teste&#39;) meuraw ## [1] 74 65 73 74 65 class(meuraw) ## [1] &quot;raw&quot; Na criação de tipos de dados, pode-se usar de coerção (coercion) através das funções as. Função Coerção para as.numeric Numeric as.integer Integer as.double Double as.character Character as.logical Boolean as.raw Raw EM CONSTRUÇÃO 5.4 Estrutura de dados no R (Data Structures) EM CONSTRUÇÃO Vimos os tipos de dados. Estes podem ser organizados de distintos modos, e a depender desta organização, temos diferentes estruturas de dados. O R possui diversas estruturas de dados, como: vetores numéricos e atômicos lista (list) fatores (factors) matriz (matrix) data frame 5.4.1 Vetor (vector) Como dito no manual, “Pode-se conceber vetores como células contíguas contendo dados” (Vectors can be thought of as contiguous cells containing data”), ou uma série de valores do mesmo tipo de dado. Vetor é a estrutura de dados mais comum no R, os tipos básicos de vetores atômicos são aqueles seis mencionados anteriormente. Criamos vetores com a função c() de “concatenate” ou “combine”. No caso de strings (character), deve-se colocar os valores dentro de aspas: meu_vetor &lt;- c(&quot;bla&quot;, &quot;ble&quot;, &quot;bli&quot;, &quot;blo&quot;, &quot;blu&quot;) # criando o vetor com strings meu_vetor # imprimindo na tela o vetor que acabamos de criar ## [1] &quot;bla&quot; &quot;ble&quot; &quot;bli&quot; &quot;blo&quot; &quot;blu&quot; class(meu_vetor) ## [1] &quot;character&quot; length(meu_vetor) # observando quantos itens possui nosso vetor ## [1] 5 meu_vetor[3] # acessando o item do 3 de nosso vetor ## [1] &quot;bli&quot; # Criando um vetor de valores booleanos # Não se usa aspas nesse caso vetor_logico &lt;- c(TRUE, FALSE, FALSE) # criando um vetor de valores booleanos class(vetor_logico) ## [1] &quot;logical&quot; # Criando um vetor com diferentes tipo de dados vetor_pan &lt;- c(&quot;bla&quot;, 3, TRUE, 5L) vetor_pan ## [1] &quot;bla&quot; &quot;3&quot; &quot;TRUE&quot; &quot;5&quot; # criando uma sequência numérica de 2 a 8. Desta vez não iremos salvar em uma variável c(2:8) ## [1] 2 3 4 5 6 7 8 Tanto números simples (ex.: 2.5) como strings (ex.: “Olá mundo R!”) tem comprimento (length) 1. Já, por exemplo, c('Olá', 'mundo', 'R') possui comprimento 3. length(&#39;Olá mundo R&#39;) ## [1] 1 length(c(&#39;Olá&#39;, &#39;mundo&#39;, &#39;R&#39;)) ## [1] 3 Vetores podem ser de dois tipos: 1) atômicos, ou 2) listas. Mas o mais frequênte é encontrarmos “vetor” como sinônimo de “vetor atômico”, e listas serem consideradas como algo à parte. Para examinarmos os vetores, podemos usar as funções: class(), para saber o tipo, length(), para saber quantos elementos possui e str(), bastante útil ao lidarmo com dataframes, o comando mostra a estrutura básica: quantos elementos, quantas colunas (se houver), uma amostra dos elementos. Há ainda o comando typeof(), mas recomendamos usar o class() para evitar confusão. É ainda possível também atribuir nomes aos elementos de um vetor usando a função names() valores &lt;- c(12, 34, 13) # Criando um vetor nomes &lt;- c(&quot;banana&quot;, &quot;uva&quot;, &quot;abacate&quot;) # Criando vetor com os nomes names(valores) &lt;- nomes # atribuindo o vetor &quot;nomes&quot; como titulo do vetor &quot;valores&quot; valores # imprimindo o vetor com nomes e valores ## banana uva abacate ## 12 34 13 names(valores) # imprimindo apenas os nomes ## [1] &quot;banana&quot; &quot;uva&quot; &quot;abacate&quot; class(valores) ## [1] &quot;numeric&quot; 5.4.2 Fator (Factor) Considere a seguinte tabela fictícia: Nome Altura(cm) Região do país Fulano 175 NE Ciclano 134 SE Beltrano 166 S João 187 S Maria 173 NE José 159 CO Joaquim 161 SE Se em uma coluna certos valores se repetem, podemos considerá-los como “fatores”. No caso, a coluna “Nome” possui valores únicos, a coluta “Altura” também, mas a coluna “Região” possui uma quantidade limitada de valores que se repetirão: N,S,NO,SE,CO. Podemos então considerar os valores desta coluna como “fatores”. Fatores podem ser tanto strings, como no caso acima, como integrais, como no caso da idade de estudantes de uma mesma turma. Um exemplo prático: # Criando um vetor de caracteres com siglas de aeroportos brasileiros aeroportos &lt;- c(&quot;BSB&quot;, &quot;CON&quot;, &quot;BSB&quot;, &quot;VIC&quot;, &quot;GUA&quot;, &quot;FOR&quot;, &quot;MAO&quot;,&quot;GUA&quot;, &quot;CON&quot;, &quot;CON&quot;, &quot;REC&quot;, &quot;UDI&quot;, &quot;VIC&quot;, &quot;GUA&quot;) table(aeroportos) ## aeroportos ## BSB CON FOR GUA MAO REC UDI VIC ## 2 3 1 3 1 1 1 2 # Se tento criar um barplot com os valores, dá erro: barplot(table(aeroportos)) # Temos de transformar os dados em um fator, que vamos chamar de &quot;aeroportos.factor&quot; aeroportos.fator &lt;- factor(aeroportos) # Perceba que &quot;Levels&quot; mostra os valores sem repetição aeroportos.fator ## [1] BSB CON BSB VIC GUA FOR MAO GUA CON CON REC UDI VIC GUA ## Levels: BSB CON FOR GUA MAO REC UDI VIC # Mostrando a frequência de cada termo com a função &#39;summary()&#39; summary(aeroportos.fator) ## BSB CON FOR GUA MAO REC UDI VIC ## 2 3 1 3 1 1 1 2 # Agora é possível gerar o gráfico de barras (barplot) barplot(summary(aeroportos.fator)) Fatores são comuns em tabelas, mas são raros em análise textual. Assim, ao trabalhar com análise textual, convém mudar as opções globais de strings, para não serem consideradas fatores: options(stringsAsFactors = FALSE) # ou podemos transformar informações em fatores através do comando aeroportos.fator &lt;- as.factor(aeroportos) aeroportos.fator ## [1] BSB CON BSB VIC GUA FOR MAO GUA CON CON REC UDI VIC GUA ## Levels: BSB CON FOR GUA MAO REC UDI VIC Fator é um tipo de dado usado para campos com valores pré-definidos, valores finitos, como em dados categóricos. Assim, por exemplo, raça, ou status matrimonial contém um número finito de valores (solteira(o), amasiada(o), casada(o), divorciada(o), viúva(o)). No caso dos aeroportos, podemos ver os valores únicos com o comando levels() levels(aeroportos.fator) ## [1] &quot;BSB&quot; &quot;CON&quot; &quot;FOR&quot; &quot;GUA&quot; &quot;MAO&quot; &quot;REC&quot; &quot;UDI&quot; &quot;VIC&quot; 5.4.3 Matriz (Matrix) Vejamos as matrizes. Podemos pensar em matrizes como tabelas contendo dados do mesmo tipo. Vamos gerar uma matriz para termos contato com uma. # 1:12=Os elementos da nossa matriz, 1 a 12 # 4=linhas no eixo vertical 3=linhas no eixo horizontal minha.matriz &lt;- matrix(1:12, 4, 3) minha.matriz ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 ## [3,] 3 7 11 ## [4,] 4 8 12 # Acessando itens, linha 2, coluna 3 minha.matriz[2, 3] ## [1] 10 # Acessando uma linha inteira minha.matriz[2, ] ## [1] 2 6 10 Matrizes também são bastante usadas na análise textual, como nas Document-Term-Matrix e nas Term Document Matrix, que são um passo intermediário de diversas análises textuais, como preparação para Topic Modeling, por exemplo. Veremos em mais detalhes posteriormente, mas apenas para um primeiro contato, o DTM Dadas as frases “O rato roeu a roupa do rei”, “O rei riu do rato”, “A roupa do rato é de rei”, consideraremos cada linha como um documento (documento, neste caso, é cada uma das frases). Ao criar uma matriz tipo DTM os documentos são as linhas e as colunas são as palavras e os números são a frequência de cada termo em cada frase: (#tab:DTM_kable)Um exemplo de DTM rato rei riu roeu roupa 1 1 0 1 1 1 1 1 0 0 1 1 0 0 1 Se esta é a primeira vez com estrutura de dados, matriz, o que vimos de matriz pode já ser o suficiente por agora e você poderá pular para a próxima estrutura de dados, a lista. Mais para frente, quando a análise e o pacote necessitarem, você será reencaminhado para esta seção, de como fazer um Document-Term_Matrix (DTM) e um Term-Document-Matrix (TDM). Os Document-Term-Matrix e o Term-Document-Matrix são usados em análise textual e PLN como passo intermediário para outras análises. Matriz é bem útil ao transformar texto em um formato que o computador consegue entender e processar. Vejamos alguns modos de gerar matrizes com diferentes pacotes. 5.4.4 Gerando um DTM com o pacote TM library(tm) frases &lt;- c(&quot;O rato roeu a roupa do rei&quot;, &quot;O rei riu do rato&quot;, &quot;A roupa do rato é de rei&quot;) myCorpus &lt;- data.frame(frases) corpus &lt;- Corpus(VectorSource(myCorpus$frases)) x &lt;- inspect(DocumentTermMatrix(corpus)) ## &lt;&lt;DocumentTermMatrix (documents: 3, terms: 5)&gt;&gt; ## Non-/sparse entries: 10/5 ## Sparsity : 33% ## Maximal term length: 5 ## Weighting : term frequency (tf) ## Sample : ## Terms ## Docs rato rei riu roeu roupa ## 1 1 1 0 1 1 ## 2 1 1 1 0 0 ## 3 1 1 0 0 1 5.4.5 Gerando DTM com o pacote Tidytext No pacote Tidytext, para gerar o DTM, 1) precisamos tokenizar usamos a função cast_dtm(coluna_documentos, coluna_termos, contagem_palavras). Para a contagem de palavras requerida, podemos usar a função count() previamente. frases &lt;- c(&quot;O rato roeu a roupa do rei&quot;, &quot;O rei riu do rato&quot;, &quot;A roupa do rato é de rei&quot;) frases_df &lt;- dplyr::tibble(id_doc = 1:3, frases = frases) # tokenizando o tibble frasesR &lt;- tidytext::unnest_tokens(frases_df, palavras, frases) # Criando o DTM frasesR %&gt;% # contando termos com &#39;count&#39;, que gera nova coluna &#39;n&#39; dplyr::count(palavras, id_doc) %&gt;% tidytext::cast_dtm(id_doc, palavras, n) ## &lt;&lt;DocumentTermMatrix (documents: 3, terms: 10)&gt;&gt; ## Non-/sparse entries: 19/11 ## Sparsity : 37% ## Maximal term length: 5 ## Weighting : term frequency (tf) O que nos retorna: - A indicação de que se trata de um Document Term Matrix - A frequência de termos e de documentos -“Non-/sparse entries” refere-se aos não zeros - “sparsity” refere-se à proporção de zeros na matriz: - Caso não exista nenhum zero em nossa matriz, o sparsity será 0%. Por exemplo, quando temos um DTM com apenas um documento. - Tivemos um valor alto de sparsity, mas por que temos poucas frases e bem parecidas - É comum termos valores altos de sparsity como 100%, principalmente quando temos muitos documentos e matrizes bem maiores que esta. Se quisermos visualizar a matriz, precisamos ampliar nosso código com as.matrix() frasesR %&gt;% # contando termos com &#39;count&#39;, que gera nova coluna &#39;n&#39; dplyr::count(palavras, id_doc) %&gt;% tidytext::cast_dtm(id_doc, palavras, n) %&gt;% as.matrix() ## Terms ## Docs a de do é o rato rei riu roeu roupa ## 1 1 0 1 0 1 1 1 0 1 1 ## 3 1 1 1 1 0 1 1 0 0 1 ## 2 0 0 1 0 1 1 1 1 0 0 5.4.6 Listas (list) Listas (ou “vetores genéricos”) são conjuntos de vetores mais gerais no R. Podemos colocar tipos diferentes de objetos em uma lista (por exemplo, uma matriz , vetores, dataframes, etc.) Para criar uma lista, usamos o comando list() minha_lista = list(c(T,T,F, F, T, T, F), c(&quot;Joaquim&quot;, &quot;José&quot;, &quot;Silva&quot;), c(3,6,3,67,22) ) minha_lista ## [[1]] ## [1] TRUE TRUE FALSE FALSE TRUE TRUE FALSE ## ## [[2]] ## [1] &quot;Joaquim&quot; &quot;José&quot; &quot;Silva&quot; ## ## [[3]] ## [1] 3 6 3 67 22 # Acessando o vetor 3 de nossa lista. No caso de listas, usamos o duplo colchete minha_lista[[3]] ## [1] 3 6 3 67 22 # Acessando o vetor 2 de nossa lista, item 3 minha_lista[[2]][3] ## [1] &quot;Silva&quot; É possível também “quebrar” um texto em vetores, através do comando strsplt() texto = &quot;Bla bla bla. Ble ble ble ble. Bli bli. Blo. Blu blu blu&quot; class(texto) ## [1] &quot;character&quot; strsplit(texto, &quot;\\\\.&quot;) ## [[1]] ## [1] &quot;Bla bla bla&quot; &quot; Ble ble ble ble&quot; &quot; Bli bli&quot; &quot; Blo&quot; ## [5] &quot; Blu blu blu&quot; No exemplo acima, o primeiro argumento em strplit() é a variável, o segundo argumento é o critério a ser usado para quebrar o texto. Alguns caracteres são considerados especiais e precisam ser indicados que queremos seu significado literal. É o caso de ponto final. Para conseguirmos usá-lo como ponto final, ao invés de usarmos \".\", usamos \\\\. . Note que ficaram espaços no início dos elementos dos vetores. Podemos melhorar nosso código acrescentando um espaço em branco após o ponto final: strsplit(texto, &quot;\\\\. &quot;) ## [[1]] ## [1] &quot;Bla bla bla&quot; &quot;Ble ble ble ble&quot; &quot;Bli bli&quot; &quot;Blo&quot; ## [5] &quot;Blu blu blu&quot; Se examinarmos o tipo de arquivo que temos, veremos que se trata de uma lista: texto2 &lt;- strsplit(texto, &quot;\\\\. &quot;) class(texto2) ## [1] &quot;list&quot; Se quisermos que este seja um vetor simples, e não lista: class(texto2) ## [1] &quot;list&quot; # usar o comando unlist() unlist(texto2) ## [1] &quot;Bla bla bla&quot; &quot;Ble ble ble ble&quot; &quot;Bli bli&quot; &quot;Blo&quot; ## [5] &quot;Blu blu blu&quot; class(unlist(texto2)) ## [1] &quot;character&quot; # Ou ainda, acessando o item 1 da lista strsplit(texto, &quot;\\\\. &quot;)[[1]] ## [1] &quot;Bla bla bla&quot; &quot;Ble ble ble ble&quot; &quot;Bli bli&quot; &quot;Blo&quot; ## [5] &quot;Blu blu blu&quot; É possível fazer uma lista nomeada: minha.lista &lt;- list(nome1 = c(&quot;Joaquim&quot;, &quot;José&quot;, &quot;Silva&quot;), letras = letters[1:6], minha_matriz = matrix(1:8, nrow = 2) ) # vendo a lista criada minha.lista ## $nome1 ## [1] &quot;Joaquim&quot; &quot;José&quot; &quot;Silva&quot; ## ## $letras ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; ## ## $minha_matriz ## [,1] [,2] [,3] [,4] ## [1,] 1 3 5 7 ## [2,] 2 4 6 8 # Acessando os itens da lista: minha.lista[[2]][3] ## [1] &quot;c&quot; # ou ainda, no caso de lista nomeada minha.lista$letras[3] ## [1] &quot;c&quot; 5.4.7 Data Frames Uma vantagem de se trabalhar com data frame é que as colunas tem nome e não apenas índice, e isto facilita nosso trabalho na hora de procurar as informações que queremos, pois buscar a coluna pelo seu número pode nos causar confusão. As principais maneiras de manipulação de dataframes no R podem ser feitas de três modos: 1) do modo tradicional, com pacote base, que veremos à seguir 2) Ela foi facilitada com os tibbles e os pacotes do tidyverse (veremos mais abaixo), principalmente o dplyr. 3) Pode ser feita de forma mais rápida com o pacote data.table (mais rápida que o Python inclusive), especial para quantidade de dados maiores. Cada um destes pacotes possuem comandos e lógicas um pouco diferentes. Primeiro, vejamos o modo mais tradicional de tratar dataframes, sem a necessidade de carregar pacotes adicionais. # Criando 3 vetores com valores aleatórios idade &lt;- as.integer(c(12,23,35)) genero &lt;- as.character(c(&quot;fem&quot;, &quot;mas&quot;, &quot;fem&quot;)) raça &lt;- as.character(c(&quot;pret&quot;, &quot;branc&quot;, &quot;pard&quot;)) # Jogando estes vetores no nosso dataframe, que vamos nomear de &quot;df&quot; df &lt;- data.frame(idade, genero, raça) df # vendo nosso dataframe no console ## idade genero raça ## 1 12 fem pret ## 2 23 mas branc ## 3 35 fem pard # ver o dataframe em uma nova janela pop-up View(df) class(df) # Qual a classe de nosso objeto, caso tenhamos esquecido? ## [1] &quot;data.frame&quot; names(df) # retornando apenas o nome das colunas ## [1] &quot;idade&quot; &quot;genero&quot; &quot;raça&quot; # Ver o número de linhas e colunas através do comando # dim() de &quot;dimensions&quot; dim(df) ## [1] 3 3 # mostra a estrutura do dataframe, como # quantidade de linhas (obs.) e colunas (variables) # nomes das colunas, tipo de dado nesta coluna, primeiros valores str(df) ## &#39;data.frame&#39;: 3 obs. of 3 variables: ## $ idade : int 12 23 35 ## $ genero: chr &quot;fem&quot; &quot;mas&quot; &quot;fem&quot; ## $ raça : chr &quot;pret&quot; &quot;branc&quot; &quot;pard&quot; df$genero # Filtrando o dataframe &quot;df&quot; pela coluna &quot;genero&quot; ## [1] &quot;fem&quot; &quot;mas&quot; &quot;fem&quot; df[2] # Filtrando o dataframe pela coluna 2: df[numero] ## genero ## 1 fem ## 2 mas ## 3 fem df[,2] # Filtrando o dataframe pela coluna 2: ## [1] &quot;fem&quot; &quot;mas&quot; &quot;fem&quot; df[,&quot;genero&quot;] # Filtrando o dataframe pelo nome da coluna 2: ## [1] &quot;fem&quot; &quot;mas&quot; &quot;fem&quot; # Filtrando o dataframe pelo número da linha: df[número_da_Linha,] . # Repare na necessidade da vírgula. df[1,] # imprimindo a linha 1 ## idade genero raça ## 1 12 fem pret df[3,] # imprimindo a linha 3 ## idade genero raça ## 3 35 fem pard df[1:2,] # retornando as linhas de 1 a 2 ## idade genero raça ## 1 12 fem pret ## 2 23 mas branc # Para o caso de excluir as primeiras linhas # exclui as linhas de 1 a 2 df[-(1:2),] ## idade genero raça ## 3 35 fem pard df[,-3] # retirando a coluna pelo seu índice ## idade genero ## 1 12 fem ## 2 23 mas ## 3 35 fem # media da coluna &quot;idade&quot; mean(df$idade) ## [1] 23.33333 summary(df) # Obtendo uma visão estatística ampla da tabela ## idade genero raça ## Min. :12.00 Length:3 Length:3 ## 1st Qu.:17.50 Class :character Class :character ## Median :23.00 Mode :character Mode :character ## Mean :23.33 ## 3rd Qu.:29.00 ## Max. :35.00 summary(df$genero) # sumário da coluna &quot;gênero&quot; do dataset &quot;df&quot; ## Length Class Mode ## 3 character character summary(df$idade) # sumário da coluna &quot;idade&quot; do dataset &quot;df&quot; ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 12.00 17.50 23.00 23.33 29.00 35.00 # testando se valores da coluna &quot;genero&quot; são &quot;fem&quot; df$genero==&quot;fem&quot; ## [1] TRUE FALSE TRUE # Filtrando apenas as linhas onde a coluna &quot;genero&quot; for &quot;fem&quot;. # Explicando de outro modo: quando &quot;genero&quot; igual a &quot;fem&quot; for verdadeiro, imprima df[df$genero==&quot;fem&quot;,] ## idade genero raça ## 1 12 fem pret ## 3 35 fem pard # Filtrando apenas as linhas onde a escolaridade for maior que 12. df[df$idade &gt; 12,] ## idade genero raça ## 2 23 mas branc ## 3 35 fem pard Além de criar um dataframe a partir de vetores, outro modo de criá-los é com a função read.table(), muito usada para carregar tabelas em texto puro (como as tabelas em csv). meuDataFrame &lt;- read.table( # Se header (cabeçalho) = True, então a primeira linha será considerada cabeçalho header=TRUE, text=&#39; Letra Valor A 9 B 14 C 11 D 12&#39;) meuDataFrame ## Letra Valor ## 1 A 9 ## 2 B 14 ## 3 C 11 ## 4 D 12 # Vendo a estrutura do dataframe criado str(meuDataFrame) ## &#39;data.frame&#39;: 4 obs. of 2 variables: ## $ Letra: chr &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; ## $ Valor: int 9 14 11 12 No caso acima, o delimitador - que separa os campos/coluna - é o caractere de espaço. Caso queira utilizar outro caractere - digamos o : - usa-se o argumento sep = \":\" em read.table(): meuDataFrame &lt;- read.table(header=TRUE, sep = &quot;:&quot;, text=&#39; Indice:Letra:Valor 1:A ala:9 2:B bla:14 3:C blable:11 4:D Ble:12&#39;) meuDataFrame ## Indice Letra Valor ## 1 1 A ala 9 ## 2 2 B bla 14 ## 3 3 C blable 11 ## 4 4 D Ble 12 5.4.8 Busca difusa (fuzzy search) Vimos como filtrar linhas utilizando operadores como ==, que busca valor exato. Caso queiramos buscar utilizando valores aproximados - por exemplo, buscar números que comecem com 2, ou variações de uma mesma palavra - usamos a fuzzy search . Procedemos da seguinte forma: Utilizando grepl buscamos por um padrão: grepl(\"padrão\", x , ignore.case = TRUE): x refere-se à base de dados onde vamos realizar a busca Podemos buscar tanto em maiúsculas como minúsculas com ignore.case = TRUE o comando retorna um vetor de booleanos: TRUE se o elemento contém o padrão e FALSE onde o elemento não atende ao critério A partir disso - um vetor de valores TRUE e FALSE - vamos filtrar o dataframe com os valores TRUE: dataframe[vetor_valores_booleanos,] # grepl busca por padrões e retorna booleanos true e false grepl(&quot;ble&quot;, meuDataFrame, ignore.case = TRUE) ## [1] FALSE TRUE FALSE # A sequência acima buscou se nas COLUNAS há o valor &quot;ble&quot;. # temos 3 colunas e 4 linhas no nosso DF # Vamos buscar somente em uma coluna específica com: grepl(&quot;ble&quot;, meuDataFrame$Letra, ignore.case = TRUE) ## [1] FALSE FALSE TRUE TRUE # Para vermos apenas os itens com o valor &quot;ble&quot; (que portanto, são TRUE) meuDataFrame$Letra[grepl(&quot;ble&quot;, meuDataFrame$Letra, ignore.case = TRUE)] ## [1] &quot;C blable&quot; &quot;D Ble&quot; # Mostrando as linhas do df com o valor buscado # (atenção para a vírgula ao final): df[grepl,] meuDataFrame[grepl(&quot;ble&quot;, meuDataFrame$Letra, ignore.case = TRUE),] ## Indice Letra Valor ## 3 3 C blable 11 ## 4 4 D Ble 12 A busca difusa pode ficar mais poderosa com o uso de expressões regulares, que veremos mais à frente. 5.4.8.1 Manipulando/Editando o data frame # adicionando (append) nova linha ao data.frame # Modo1: através do &#39;nrow&#39; df[nrow(df)+1,]=c(23, &quot;mas&quot;, &quot;branc&quot;) df ## idade genero raça ## 1 12 fem pret ## 2 23 mas branc ## 3 35 fem pard ## 4 23 mas branc # Modo 2: rbind() df &lt;- rbind(df, c(20,&quot;fem&quot;, &quot;pret&quot;)) df ## idade genero raça ## 1 12 fem pret ## 2 23 mas branc ## 3 35 fem pard ## 4 23 mas branc ## 5 20 fem pret # mudando um valor, em uma célula # data.frame[número_da_linha, número_da_coluna] = new_value df[4,1] = 14 df ## idade genero raça ## 1 12 fem pret ## 2 23 mas branc ## 3 35 fem pard ## 4 14 mas branc ## 5 20 fem pret # mudando um valor usando o nome da coluna df[4,&quot;raça&quot;] = &quot;pret&quot; df ## idade genero raça ## 1 12 fem pret ## 2 23 mas branc ## 3 35 fem pard ## 4 14 mas pret ## 5 20 fem pret # mudando valores com condicionais # onde coluna raça for &quot;branc&quot;, mudar na coluna &quot;raça&quot; para &quot;branco&quot; # se o segundo parâmetro não for inserido, todas células ganharão valor &quot;branco&quot; df[df$raça==&quot;branc&quot;, &quot;raça&quot;] = &quot;branco&quot; df ## idade genero raça ## 1 12 fem pret ## 2 23 mas branco ## 3 35 fem pard ## 4 14 mas pret ## 5 20 fem pret # deletando linhas # com base no numero da linha df[-c(1),] # deleta a linha 1 ## idade genero raça ## 2 23 mas branco ## 3 35 fem pard ## 4 14 mas pret ## 5 20 fem pret df &lt;- df[-c(1),] # para tornar a mudança permanente # com base no valor df = df[!df$a==&quot;bla&quot;,] 5.4.9 Considerações finais sobre estrutura de dados Estes não são os únicos tipos de estrutura de dados no R. Alguns pacotes podem criar o seu próprio tipo e não convém tentar cobrir todos. Apesar de considerar tipos diferentes, podemos considerar os diferentes tipos como subcategorias de outras. Data frames são um tipo restrito de listas. Listas, por sua vez, são um tipo de vetor. Veremos a seguir a suíte de pacotes do tidyverse que possui formato próprio de dados. 5.5 Instalando pacotes no R No R não carregamos tudo de uma vez, mas carregamos os pacotes ou bibliotecas que vamos usar através do comando library(Nome_do_pacote), e assim salvamos mais memória para o processamento no computador. Podemos ainda chamar apenas uma função específica de um pacote específico através do comando pacote::função() sem carregar todo o pacote. Neste caso a função é utilizada de modo ad hoc. Em alguns tutoriais sobre o R podemos ver o uso conjunto tanto do library(pacote) como do pacote::função() e o motivo é didático, para tornar claro a qual pacote pertence a função. Um modo de instalar pacotes é via linha de comando 5.5.1 Modo 1: instalando via linha de comando install.packages(&quot;Nome_do_pacote&quot;) Para checar os pacotes instalados: rownames(installed.packages()) 5.5.2 Modo 2: Instalando pacotes no modo gráfico No RStudio, outro modo é ir na aba packages / install RStudio: instalando pacotes Após isso, digite o nome do pacote na caixa, deixando marcado o “install dependencies” e depois em “install”. É muito comum os pacotes precisarem de outros pacotes, e com esta opção marcada, o R instala tudo. 5.6 A suíte de pacotes tidyverse O tidyverse é uma suíte de pacotes bastante úteis e de uso mais fácil que os pacotes tradicionais do R. Dicas O artigo do Hadley Wickham, o criador da ideia, explicando a “filosofia” dos dados “tidy”. &gt; Hadley Wickham. Tidy Data. Journal of Statistical Software Para instalar a suíte de pacotes tidyverse install.packages(tidyverse) O tidyverse conta com diversos pacotes e estão sempre aumentando. library(tidyverse) # vendo os pacotes disponíveis no tidyverse: tidyverse_packages() ## [1] &quot;broom&quot; &quot;cli&quot; &quot;crayon&quot; &quot;dbplyr&quot; ## [5] &quot;dplyr&quot; &quot;dtplyr&quot; &quot;forcats&quot; &quot;googledrive&quot; ## [9] &quot;googlesheets4&quot; &quot;ggplot2&quot; &quot;haven&quot; &quot;hms&quot; ## [13] &quot;httr&quot; &quot;jsonlite&quot; &quot;lubridate&quot; &quot;magrittr&quot; ## [17] &quot;modelr&quot; &quot;pillar&quot; &quot;purrr&quot; &quot;readr&quot; ## [21] &quot;readxl&quot; &quot;reprex&quot; &quot;rlang&quot; &quot;rstudioapi&quot; ## [25] &quot;rvest&quot; &quot;stringr&quot; &quot;tibble&quot; &quot;tidyr&quot; ## [29] &quot;xml2&quot; &quot;tidyverse&quot; O ecossistema tidyverse Tidyverse ecosystem picture (author: Silvia Canelón, PhD. Original link) Para nós, destes pacotes, os mais interessantes são: Pacote tidyverse descrição readr Lê dados retangulares (de tabelas) como csv, tsv e fwf rvest usado para minerar dados na web de modo fácil tibble Para trabalharmos com tibble, um tipo de data frame ggplot2 Famoso pacote de geração de gráficos dplyr Para manipulação facilitada de dados stringr Usado na manipução de strings lubridate Para lidar com data e hora de modo fácil purrr Usado em programação funcional, torna a construção de loops mais limpa e fácil magrittr Para usar pipes (%&gt;%). Parte dele é carregado ao carregar outros pacotes da suíte forcats Usado para trabalharmos com fatores, ao lidar com dados categóricos haven Para ler e salvar arquivos dos softwares SPSS, SAS e Stata readxl Para ler arquivos do Excell 5.6.1 Pipes Quando aplicamos uma série de funções em série, o modo tradicional é de fazê-lo é colocando funções dentro de outras funções, as “nested functions”, por exemplo: função3(função2(função1))) Representação metafórica de funções dentro de outras funções Repare que a ordem das funções é invertida, o que pode tornar bem trabalhosa a tarefa de descobrir a ordem das funções. Mas podemos fazer o mesmo de um modo mais fácil e bem mais inteligível, usando pipes do pacote magritrr do tidyverse. Para carregá-lo, basta carregar qualquer um dos pacotes do tidyverse, como stringr ou dplyr. Com ele, ao invés das funções dentro de funções do exemplo anterior, é possível ordenar os comandos intercalados com pipes de um modo mais intuitivo função1() %&gt;% função2() %&gt;% função3() Caso não haja parâmetros dentro dos parênteses, pode-se retirar os parênteses. função1 %&gt;% função2 %&gt;% função3 Caso, a função exija mais de um parâmetro, deve-se indicar o local onde o nome do dataframe/vetor, etc com um ponto final. função1() %&gt;% função2(., parametroQualquer = ValorDoParametroQualquer) ou se a função for diferente (ao olhar os argumentos da função, o x diz onde o ponto de ir) função1() %&gt;% função3(parametroQualquer, OutroParametroQualquer, .) Há ainda outros pipes, mas neste caso, o pacote magrittr deve ser carregado explicitamente. como o %$%. data.frame %$% coluna que equivale a data.frame$coluna. Em maio de 2021 foi introduzido no R versão 4.1.0 um novo pipe, nativo, vindo por padrão no R, o |&gt;, que tem suas peculiaridades. Talvez o mais prudente é aguardar um pouco mais para usá-lo. Até lá, indica-se usar o pipe do tidyverse. 5.6.2 Tibbles O “Tibble” é um modo de chamar objetos da classe “tbl_df”, um tipo de dataframe, mas que possui algumas vantagens. O tibble, ao ser chamado, mostra apenas as 10 primeiras linhas, ao invés do dataframe inteiro, o que é bem útil quando temos tabelas que não sejam minúsculas. O Tibble, ao contrário do dataframe, também mostra os tipos de dados das variáveis, e tenta se adequar ao tamanho da tela. Além disso, um tibble não força caracteres para fatores, que é o modo como lidamos com strings ao fazermos a análise textual. O video “Qual a diferença entre dataframe e tibble?” do canal de Samuel Macedo explica as diferenças entre estes dois tipos. Vamos criar um tibble com os dados que usamos no exemplo anterior do dataframe. # criando vetores com valores idade &lt;- as.integer(c(12,23,35, 22, 73)) genero &lt;- as.character(c(&quot;fem&quot;, &quot;mas&quot;, &quot;fem&quot;, &quot;fem&quot;, &quot;mas&quot;)) raca &lt;- as.character(c(&quot;preto&quot;, &quot;branco&quot;, &quot;pardo&quot;, &quot;pardo&quot;, &quot;branco&quot;)) # cria-se tibble com a função &#39;tibble()&#39; a partir dos vetores anteriores MeuTibble &lt;- dplyr::tibble(idade, # Não esqueça de colocar vírgula ao final destas linhas genero, # a ultima liha não leva vírgula raca) MeuTibble ## # A tibble: 5 × 3 ## idade genero raca ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 12 fem preto ## 2 23 mas branco ## 3 35 fem pardo ## 4 22 fem pardo ## 5 73 mas branco # mostrando apenas uma colunar} MeuTibble$genero ## [1] &quot;fem&quot; &quot;mas&quot; &quot;fem&quot; &quot;fem&quot; &quot;mas&quot; Também é possível alterar o nome da coluna/variável ao criar o tibble, usando o formato nome_da_coluna_no_tibble = nome_da_variável, MeuTibble2 &lt;- dplyr::tibble(idade_amostra = idade, genero_amostra = genero, raca_amostra = raca) MeuTibble2 ## # A tibble: 5 × 3 ## idade_amostra genero_amostra raca_amostra ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 12 fem preto ## 2 23 mas branco ## 3 35 fem pardo ## 4 22 fem pardo ## 5 73 mas branco 5.6.3 Dplyr: Verbos (ou comandos) O pacote Dplyr possui vários “verbos” ou comandos, para manipulação de dados: Manipulando linhas: filter() filtra linhas com base nos valores das colunas. slice() escolhe linhas com base na localização. arrange() reordena as linhas com base na ordem de uma ou mais colunas. Pode ser combinada com desc() para inverter a ordem. Por exemplo, em um dataframe, podemos reordená-la com bbase na ordem alfabética dos nomes, ou das datas ou de alguma outra coluna. Colunas: select() seleciona/filtra variáveis/colunas. Para inverter a seleção, usar select(!Variavel) rename() muda o nome das colunas. mutate() muda os valores de colunas e pode também criar novas colunas (útil para criar nova coluna com base em uma já existente). relocate() muda a ordem das colunas. Grupo de linhas: summarise() colapsa um grupo de linhas em uma linha única, sendo combinada com outros comandos group_by() junta linhas de mesmo valor (bom para usar com fatores). Deve ser usado em conjunto com outros comandos. (Esta é uma função genérica, isto é, provê implementação de outros pacotes) Fonte: Baseado parcialmente em Introduction to dplyr. Vejamos exemplos destes comandos. library(dplyr) # carregando o pacote caso não tenha sido carregado 5.6.3.0.1 dplyr::filter() Usamos o verbo filter para filtrar linhas com certos valores específicos # Filtrando um valor específico de uma coluna MeuTibble %&gt;% filter(genero == &quot;fem&quot;) ## # A tibble: 3 × 3 ## idade genero raca ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 12 fem preto ## 2 35 fem pardo ## 3 22 fem pardo # Filtrando por valores de duas colunas MeuTibble %&gt;% filter(genero == &quot;fem&quot; &amp; raca == &quot;pardo&quot;) ## # A tibble: 2 × 3 ## idade genero raca ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 35 fem pardo ## 2 22 fem pardo # Filtrando com termo aproximado # para tal, combinamos filter com grepl MeuTibble %&gt;% filter(grepl(&quot;p.*&quot;, raca)) ## # A tibble: 3 × 3 ## idade genero raca ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 12 fem preto ## 2 35 fem pardo ## 3 22 fem pardo 5.6.3.0.2 dplyr::arrange() # ordenando por valores da coluna &quot;idade&quot; arrange(MeuTibble, idade) ## # A tibble: 5 × 3 ## idade genero raca ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 12 fem preto ## 2 22 fem pardo ## 3 23 mas branco ## 4 35 fem pardo ## 5 73 mas branco # Invertendo a ordenção: Opção 1 MeuTibble %&gt;% arrange(desc(idade)) ## # A tibble: 5 × 3 ## idade genero raca ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 73 mas branco ## 2 35 fem pardo ## 3 23 mas branco ## 4 22 fem pardo ## 5 12 fem preto # Invertendo a ordenção: Opção 2 MeuTibble %&gt;% arrange(-idade) ## # A tibble: 5 × 3 ## idade genero raca ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 73 mas branco ## 2 35 fem pardo ## 3 23 mas branco ## 4 22 fem pardo ## 5 12 fem preto 5.6.3.0.3 dplyr::select() Restringindo as colunas/variáveis. MeuTibble %&gt;% select(genero) ## # A tibble: 5 × 1 ## genero ## &lt;chr&gt; ## 1 fem ## 2 mas ## 3 fem ## 4 fem ## 5 mas # todas colunas, exceto a genero MeuTibble %&gt;% select(!genero) ## # A tibble: 5 × 2 ## idade raca ## &lt;int&gt; &lt;chr&gt; ## 1 12 preto ## 2 23 branco ## 3 35 pardo ## 4 22 pardo ## 5 73 branco # Selecionando a coluna pelo índice, e não pelo nome select(MeuTibble,1) ## # A tibble: 5 × 1 ## idade ## &lt;int&gt; ## 1 12 ## 2 23 ## 3 35 ## 4 22 ## 5 73 5.6.3.0.4 dplyr::rename() É posível renomear variáveis/colunas com o comando rename, que em o seguinte formato: rename(novo_nome = nome_antigos) MeuTibble %&gt;% dplyr::rename(genero_bin = genero) ## # A tibble: 5 × 3 ## idade genero_bin raca ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 12 fem preto ## 2 23 mas branco ## 3 35 fem pardo ## 4 22 fem pardo ## 5 73 mas branco 5.6.3.0.5 dplyr::mutate() mutate(NomeVariavel = função()) modifica e cria novas colunas/variáveis. Pode ser usado em conjunto com outros cálculos # criando variável expectativa de vida expectativa_vida = 70 MeuTibble %&gt;% select(genero, idade) %&gt;% # vamos ver quantos anos faltam para cada idade atingir a # expectativa de vida da região mutate(AnosParaExpecVida = expectativa_vida - idade) ## # A tibble: 5 × 3 ## genero idade AnosParaExpecVida ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 fem 12 58 ## 2 mas 23 47 ## 3 fem 35 35 ## 4 fem 22 48 ## 5 mas 73 -3 5.6.3.0.6 dplyr::summarise() A função summarise() ou summarize() (com “s” ou “z”, ambas funcionam), cria um novo data frame. Pode ser usado com funções como mean(), median(), ou para contar dados categóricos, dentre outras funções. Sua sintaxe é summarize(nome_nova_variavel = funcao) # verificando a média de idade MeuTibble %&gt;% summarise(mean(idade)) ## # A tibble: 1 × 1 ## `mean(idade)` ## &lt;dbl&gt; ## 1 33 5.6.3.0.7 dplyr::group_by() O comando summarise se torna mais útil em conjunto com o comando group_by(). # Media de idade por genero MeuTibble %&gt;% group_by(genero) %&gt;% summarise(media = mean(idade)) ## # A tibble: 2 × 2 ## genero media ## &lt;chr&gt; &lt;dbl&gt; ## 1 fem 23 ## 2 mas 48 Podemos usar o summarize também para contar dados categóricos, junto à função n() e ao group_by(). Por exemplo, vamos contar os itens na coluna “genero” MeuTibble %&gt;% group_by(genero) %&gt;% summarise(novo_nome_da_variável = n()) ## # A tibble: 2 × 2 ## genero novo_nome_da_variável ## &lt;chr&gt; &lt;int&gt; ## 1 fem 3 ## 2 mas 2 Um modo ainda mais simples de contar dados categóricos, é usando a função count(). A contagem aparece em uma nova coluna de nome “n” MeuTibble %&gt;% count(genero) ## # A tibble: 2 × 2 ## genero n ## &lt;chr&gt; &lt;int&gt; ## 1 fem 3 ## 2 mas 2 Caso prefira usar o índice da coluna ao invés de seu nome, use group_by_at(numero_da_coluna). Pode-se ainda acrescentar novas linhas ou juntar tibbles através dos comandos: comando descrição bind_rows() junta dois tibbles, permitindo linhas duplicadas union(df1,df2) Junta tibbles df1 e df2, mas acrescenta sem repetição. Pode-se retirar linhas com anti_join(). Este comando pode ser útil para retirar stopwords anti_join(stop_words) Dicas: Tibble Uma boa introdução aos tibbles (em inglês) temos no capítulo 10.Tibbles do livro “R for Data Science” de Wichham e Grolemund. 5.7 Manipulando data e hora Data e hora é algo simples, mas que pode dar dor de cabeça se não usar os pacotes já prontos. O R usa o seguinte formato \"ano-mês-dia hora:minuto:segundo\" ou, por exemplo, 2021-03-09 13:05:03. O mês vem antes de dia e ano vem antes de todos já que assim é possível organizar as datas facilmente usando a ordem alfabética/numérica. P.s: Cuidado! Para evitar confusão, não use data como nome para um objeto, já que data tem significado especial no R. Existe um padrão para datas usado em várias linguagens. Por exemplo, %d equivale a dia, %m equivale a mês em formato numérico, %b em formato por extenso. Para consultar a lista com este formato, consulte a ajuda ?strptime. Mas vale lembrar, esta conversão ficou mais fácil com o pacote lubridate, que veremos mais abaixo. # Dizendo ao R que nossa string &quot;12/05/1993 13:00:05&quot; é uma data minha_data = &quot;12/05/1993 13:00:05&quot; minha_data2 = strptime(minha_data, format = &quot;%d/%m/%Y %H:%M:%S&quot;) minha_data2 ## [1] &quot;1993-05-12 13:00:05 -03&quot; 5.7.1 Gerando uma sequencia de datas no R Em algum momento você pode precisar de uma sequência de datas, por exemplo, ao fazer a raspagem de dados de algum site que usa o formato de data (como é o caso de agendas de autoridades governamentais). Isto é bem fácil com o R. Vamos gerar datas entre “29 de novembro de 2020 (”2020-11-29”) e 02 de janeiro de 2021 (“2021-01-02”). seq(from=as.Date(&quot;2020-11-29&quot;), to=as.Date(&quot;2021-01-02&quot;), by=&quot;day&quot;) ## [1] &quot;2020-11-29&quot; &quot;2020-11-30&quot; &quot;2020-12-01&quot; &quot;2020-12-02&quot; &quot;2020-12-03&quot; ## [6] &quot;2020-12-04&quot; &quot;2020-12-05&quot; &quot;2020-12-06&quot; &quot;2020-12-07&quot; &quot;2020-12-08&quot; ## [11] &quot;2020-12-09&quot; &quot;2020-12-10&quot; &quot;2020-12-11&quot; &quot;2020-12-12&quot; &quot;2020-12-13&quot; ## [16] &quot;2020-12-14&quot; &quot;2020-12-15&quot; &quot;2020-12-16&quot; &quot;2020-12-17&quot; &quot;2020-12-18&quot; ## [21] &quot;2020-12-19&quot; &quot;2020-12-20&quot; &quot;2020-12-21&quot; &quot;2020-12-22&quot; &quot;2020-12-23&quot; ## [26] &quot;2020-12-24&quot; &quot;2020-12-25&quot; &quot;2020-12-26&quot; &quot;2020-12-27&quot; &quot;2020-12-28&quot; ## [31] &quot;2020-12-29&quot; &quot;2020-12-30&quot; &quot;2020-12-31&quot; &quot;2021-01-01&quot; &quot;2021-01-02&quot; O parâmetro by aceita ainda week e month. O R também calcula anos bissextos. Fazendo um teste, gerando datas entre 28 de fevereiro e 1 de março de diferentes anos: # criando uma função para testes FevMarc &lt;- function(ano) { # nome_da_funcao &lt;- function(input){ minhaDataInicial=paste0(ano, &quot;-02-28&quot;) MinhaDataFinal=paste0(ano, &quot;-03-01&quot;) seq(from=as.Date(minhaDataInicial), to=as.Date(MinhaDataFinal), by=&quot;day&quot;) } for ( ano in c(2015:2021) ) { x= FevMarc(ano) print(x) } ## [1] &quot;2015-02-28&quot; &quot;2015-03-01&quot; ## [1] &quot;2016-02-28&quot; &quot;2016-02-29&quot; &quot;2016-03-01&quot; ## [1] &quot;2017-02-28&quot; &quot;2017-03-01&quot; ## [1] &quot;2018-02-28&quot; &quot;2018-03-01&quot; ## [1] &quot;2019-02-28&quot; &quot;2019-03-01&quot; ## [1] &quot;2020-02-28&quot; &quot;2020-02-29&quot; &quot;2020-03-01&quot; ## [1] &quot;2021-02-28&quot; &quot;2021-03-01&quot; 5.7.2 Lubridate: facilitando manipulação de datas Com o Lubridate é mais fácil manipular data e hora, ou mesmo alterar fuso horário. No caso abaixo, não precisamos especificar o caractere separador, apenasa ordem. No caso abaixo, dia/mês/ano minha_data &lt;- &quot;12/05/2020&quot; lubridate::dmy(minha_data) ## [1] &quot;2020-05-12&quot; 5.7.3 Converter data em nome por extenso do mês Dado um vetor de meses, podemos gerar o mês não como número, mas com seu nome. vetor_meses = c(1,4,7,3,2,12,6) library(lubridate) # carregando o pacote lubridate ## ## Attaching package: &#39;lubridate&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## date, intersect, setdiff, union # o locale pode ser que funcione com o valor &#39;pt_BR&#39; Meses &lt;- month(as.numeric(vetor_meses), label = TRUE, # produz os meses locale = &quot;pt_BR.utf8&quot;) # lingua Meses ## [1] jan abr jul mar fev dez jun ## 12 Levels: jan &lt; fev &lt; mar &lt; abr &lt; mai &lt; jun &lt; jul &lt; ago &lt; set &lt; ... &lt; dez # De modo resumido lubridate::month(as.numeric(vetor_meses), label = TRUE, locale = &quot;pt_BR.utf8&quot;) ## [1] jan abr jul mar fev dez jun ## 12 Levels: jan &lt; fev &lt; mar &lt; abr &lt; mai &lt; jun &lt; jul &lt; ago &lt; set &lt; ... &lt; dez Fazendo o caminho inverso, supondo que tenhamos um vetor com nome dos meses e queiramos colocá-los em formato data dataPorExtenso &lt;- c(&#39;12 agosto 2020&#39;, &#39;17 dezembro 2021&#39;) as.Date(dataPorExtenso, format = &quot;%d %B %Y&quot;) ## [1] &quot;2020-08-12&quot; &quot;2021-12-17&quot; 5.8 Links e Dicas Dicas - An Introduction to R, manual mantido pelo R Core Team Tutorial do Lubridate em português do Curso-R Manipulação de Dados. Podemos trabalhar com outros formatos de datas Documentation Lubridate Video Youtube Como trabalhar com Data e Hora na linguagem R "],["normalização-de-texto-e-expressões-regulares.html", "6 Normalização de texto e Expressões Regulares 6.1 Expressões regulares (RegEx) 6.2 RegEx no R 6.3 Pacote stringr 6.4 Dicas/Sugestões: Regex no R", " 6 Normalização de texto e Expressões Regulares Objetivos deste capítulo: Apresentar as expressões regulares (regex) Apresentar comandos regex no R A normalização de texto consiste em converter texto para formatos mais padronizados, e expressões regulares, ou “regex” são uma ferramenta importante neste processo. A normalização de texto, a limpeza dos dados, sua reestruturação no formato necessário pode tomar a maior parte do tempo em um projeto. Vamos partir de um problema inicial. Quem quiser saber sobre uso de remédios como azitromicina, ivermectina e cloroquina em pacientes com “síndrome respiratória aguda grave”, podemos baixar a tabela csv do opendatasus e olhar a coluna “OUT_ANTIV”. Lá vemos que não há padronização, os dados estão bagunçados (ou “messy data”). Por exemplo, “azitromicina” foi escrita também como “azitronicina”,“az” “azt”, “azitro”, isto é, além de abreviações diferentes, há erros de ortografia. Em casos assim, expressões regulares podem ajudar a normalizar este campo, isto é, deixando tudo num mesmo padrão. Esta parte de limpeza, padronização, também chamada de cleaning e data wrangling (algo como “manipulação de dados”) consome boa parte do tempo “É dito, frequentemente, que 80% da análise de dados é gasto no processo de limpeza e preparação dos dados. A preparação dos dados é não só o primeiro passo, como deve ser repetido tantas vezes no curso da análise na medida que novos problemas aparecem ou novos dados são coletados.” It is often said that 80% of data analysis is spent on the process of cleaning and preparing the data (Dasu and Johnson 2003). Data preparation is not just a first step, but must be repeated many times over the course of analysis as new problems come to light or new datais collected. fonte: WICKHAM, Hadley. Tidy Data. Journal of Statistical Software. August 2014, Volume 59, Issue 10 Apesar desse número “80%” parecer ser ficcioso, ou que varie conforme as condições e destreza do pesquisador (veja aqui), não deixa de ser uma verdade que grande parte do tempo é gasto neste processo e processo de arrumar e rearrumar os dados. 6.1 Expressões regulares (RegEx) Certa vez Jamie Zawinski fez uma piada sobre regex que se tornou bastante conhecida ao se mencioar regex: Algumas pessoas, ao se defrontarem com um problema, pensam “Acho que vou usar expressões regulares”. Agora elas tem dois problemas. Talvez um pouco exagerada a piada, expressões regulares podem ser um pouco difíceis no início, mas depois que se aprende, fica difícil viver sem elas. Em editores de texto como Microsoft Word, OpenOffice, LibreOffice, GoogleDocs etc. é possível buscar por trechos de texto idênticos, é possível tornar a busca sensível a termos em maiúsculo e minúsculo. A busca utilizando regex permite isto e muito mais. Busca e subtituição com regex no Libre Office Calc Busca e subtituição com regex no Rstudio em Edit/Replace Find Regex trata-se de uma ferramenta coringa na hora de limpar ou transformar texto, estando presente nas mais diversas linguagens de programação, com sintaxe mais ou menos comum a todas elas. As expressões regulares (regular expressions), ou RegEx, RegExp, etc., mas mais conhecidas como regex, trata-se de busca e substituição avançada usando não só o texto exato, mas usando padrões de texto. Perguntas das expressões regulares: O quê? Números? Letras? Letras minúsculas ou maiúsculas? Palavras? Símbolos ou caracteres específicos? Quebras de linha? Tabulação? Quantas vezes? Um vez? Uma ou nenhuma? Uma ou várias vezes? Uma quantidade específica? Em regex chama-se isso de quantificadores. Onde? Antes ou de depois do quê? No início ou no fim da sentença? Em regex isso chama-se âncora. É possível delimitar a busca por letras específicas, por escopo de letras e/ou números, pode-se especificar a quantidade de caracteres, tirar ou acrescentar quebra de linha, acrescentar/retirar algo no início ou fim da linha. As regras do Regex são padronizadas em diversas linguagens de programação, apesar de algumas peculiaridades em cada uma, há uma gramática comum. Assim, aprendendo em uma linguagem, você aprendeu em outras.s Supondo que queira ter uma ferramenta simples de consulta do Qualis de revistas da área de sociologia. Baixando o relatório qualis capes de sociologia neste link é possível filtrar com base no nome ou na avaliação. Com regex podemos transformar uma tabela em csv Estado;sigla;capital;região; Acre;AC;Rio Branco;Norte; Alagoas;AL;Maceió;Nordeste; Amapá;AP;Macapá;Norte; Amazonas;AM;Manaus;Norte; Bahia;BA;Salvador;Nordeste; Ceará;CE;Fortaleza;Nordeste; para o formato de tabela em markdwon |Estado | sigla | capital| região| |------|----|-----|-----| |Acre |AC |Rio Branco |Norte| |Alagoas |AL |Maceió |Nordeste| |Amapá |AP |Macapá |Norte| |Amazonas |AM |Manaus |Norte| |Bahia |BA |Salvador |Nordeste| Com regex é possível fazer transformações, por exemplo, como pegar datas como “20/03/2020” ,“13/01/1990” ,“04/06/2001”, que estão no formado dd/mm/aaaa (dia/mês/ano) e mudá-las para outra disposição, como aaaa-mm-dd (ano/mês/dia), passando para “2020-03-20”, “1990-01-13”, “2001-06-04”. Há uma sessão nesse manual dedicada a manipulação de datas, com pacotes específicos. Vejamos os parâmetros que usamos com regex 6.1.1 Parâmetros das Regex 6.1.1.1 Metacaracteres especiais Alguns caracteres tem significado especial \\n usado para quebra de linha, ou “newline”. \\t caractere de tabulação. \\\\ para usar a própria barra \\r retorno de carro ou “carriage return”, que move o cursor para a próxima linha, mas sem aparecer em seu início. De uso no Windows. \\v tab vertical \\f form feed Destes, os mais comuns são o \\n, \\t, \\\\ e por vezes o \\r. 6.1.1.2 Âncoras símbolo descrição exemplo \\^a Início da string ^a busca “aaa aaa” a$ Fim da string a$ busca “aaaa aaa” \\b limite de palavra \\b[A-Za-z]\\b \\B NÃO limite de palavra \\\\&lt; Início de uma palavra \\\\&gt; Fim de uma palavra Exemplo de substituição com regex no Google Docs: acrescentou-se “Disse:” no início das células demarcadas 6.1.1.3 Operadores Descrição Exemplo . Pega um caractere único, qualquer um. Dado aba, abc, asa, a busca por ab. retorna aba, abc. A busca de a.. retorna aba, asa [..] Lista de caracteres, podendo usar a barra - para definir um escopo. Dado abcdefgh a busca por [a-d], retorna abcd [^..] Inversão do caso anterior. Pega todos os elementos, exceto os especificados ali Dado abcdefgh a busca por [^a-d], retorna efgh. | Operador booleano do tipo “ou”. Dado \"bananada, bananeira, bandolim\", a busca utilizando \"eira|dol\", retorna \"bananeira, bandolim\" (...) Agrupamento. É bastante usado com “backreference” Dado \"banana, bananada, bananeira\", a busca utilizando \"banan(ada|eira)\", retorna \"bananada, bananeira\" Podemos usar os colchetes [] para pegarmos um escopo. Se usamos [a-f], então nossa expressão pega qualquer coisa que contenha a,b,c,d,e, ou f, em minúscula. Assim, se quisermos pegar todas as letras minúsculas, usamos [a-z], se quisermos pegar todas as maiúsculas e minúsculas, usamos [A-Za-z]. Mas atenção, desta forma não detectamos caracteres acentuados, como “à”, “ç”, “ã”, etc. Para isso, acrescentamos à-ÿ que pega o escopo de caracteres acentuados. 6.1.1.4 Quantificadores Os quantificadores especificam quantas vezes o padrão anterior é repetido símbolo explicação exemplo * busca o item anterior zero ou mais vezes + busca o item anterior uma ou mais vezes ? o item anterior é opcional, e pode aparecer no máximo uma vez. O padrão bananas? encontra “banana” “bananas” “bananal” “bananada” {n} busca o item anterior exatamente n vezes {n,} busca o item anterior n vezes ou mais {n,m} busca o item anterior com no mínimo n vezes e com no máximo m vezes. (Faz sentido usálo com delimitadores como \\b) no_vec &lt;- c(\"no\", \"nono\", \"nonono\", \"nononono\", \"nonato\") grep(\"(no){2,3}\", no_vec, value=TRUE) [1] \"nono\" \"nonono\" \"nononono\" Os quantificadores podem ser combinados. Por exemplo, a partir do texto: texto &lt;- &quot;O SR. PRESIDENTE (Omar Aziz. PSD - AM. Fala da Presidência.) – Os Srs. Senadores que as aprovam permaneçam como se encontram. (Pausa.)&quot; Possui dois textos entre parênteses. Assim se usarmos: stringr::str_extract(texto, &#39;\\\\(.*\\\\)&#39;) ## [1] &quot;(Omar Aziz. PSD - AM. Fala da Presidência.) – Os Srs. Senadores que as aprovam permaneçam como se encontram. (Pausa.)&quot; Pegamos o texto desde o primeiro parêntese até o último. Isto acontece por o símbolo de asterisco * ser “greedy” (voraz, guloso, segundo a terminologia de regex). Precisamos torná-lo “lazy” (preguiçoso) se quisermos pegar apenas o primeiro caso. O fazemos com a sequência de quantificadores *? stringr::str_extract(texto, &#39;\\\\(.*?\\\\)&#39;) ## [1] &quot;(Omar Aziz. PSD - AM. Fala da Presidência.)&quot; Exemplos: banana_vec &lt;- c(&quot;banana&quot;, &quot;bananas&quot;, &quot;bananal&quot;, &quot;bananada&quot;, &quot;bananeira&quot;, &quot;bandolim&quot;) grep(&quot;ban.*&quot;, banana_vec, value=T) ## [1] &quot;banana&quot; &quot;bananas&quot; &quot;bananal&quot; &quot;bananada&quot; &quot;bananeira&quot; &quot;bandolim&quot; grep(&quot;banan.*&quot;, banana_vec, value=T) ## [1] &quot;banana&quot; &quot;bananas&quot; &quot;bananal&quot; &quot;bananada&quot; &quot;bananeira&quot; grep(&quot;banana.*&quot;, banana_vec, value=T) ## [1] &quot;banana&quot; &quot;bananas&quot; &quot;bananal&quot; &quot;bananada&quot; grep(&quot;banana.?&quot;, banana_vec, value=TRUE) ## [1] &quot;banana&quot; &quot;bananas&quot; &quot;bananal&quot; &quot;bananada&quot; grep(&quot;banana.?$&quot;, banana_vec, value=TRUE) ## [1] &quot;banana&quot; &quot;bananas&quot; &quot;bananal&quot; # A sequência &quot;an&quot; pode aparecer uma ou mais vezes repetida, como &quot;ban&quot;, &quot;banan&quot;, &quot;bananan&quot;, &quot;banananan&quot;, etc. grep(&quot;b(an)+&quot;, banana_vec, value=T) ## [1] &quot;banana&quot; &quot;bananas&quot; &quot;bananal&quot; &quot;bananada&quot; &quot;bananeira&quot; &quot;bandolim&quot; grep(&quot;b[an]{2}&quot;, banana_vec, value=T) ## [1] &quot;banana&quot; &quot;bananas&quot; &quot;bananal&quot; &quot;bananada&quot; &quot;bananeira&quot; &quot;bandolim&quot; grep(&quot;b(an){2}&quot;, banana_vec, value=T) ## [1] &quot;banana&quot; &quot;bananas&quot; &quot;bananal&quot; &quot;bananada&quot; &quot;bananeira&quot; # se quiser delimitar a busca, restringindo a encontrar apenas o termo exato (exact match), deve-se usar o delimitador de palavra &quot;\\b&quot;. E para usá-lo aqui, deve-se acrescentar mais uma barra para o escape, ficando &quot;\\\\b&quot;. grep(&quot;\\\\bbanana\\\\b&quot;, banana_vec, value=T) ## [1] &quot;banana&quot; grep(&quot;\\\\bbanana.\\\\b&quot;, banana_vec, value=T) ## [1] &quot;bananas&quot; &quot;bananal&quot; A História do comando GREP: O comando grep exite graças à uma demanda da análise textual. Na década de 1960, um problema intrigava diversos pesquisadores. Quem são os autores de cada texto dos textos dos Federalistas? Os Federalist Papers foram 85 textos publicados de modo anônimo sob pseudônimo de “Publius”, podendo ser de autoria de James Madinson, Alexander Hamilton, John Jay em 1787 e 1788. Mas quem escreveu quais textos? Lee McMahon comentou deste problema com seus colegas. Ele queria buscar por certas palavras através de vários textos. Ken Thompson - personagem importante na história da programação - escutou aquilo e no dia seguinte voltou com o programa que veio a ser chamado grep: “g” de “Global”, procurar em vários documentos “re” de regular expressions “p” de print. Se achar o padrão de regex, então imprima na tela. Poderia ainda ser apagar, ou substituir também. O grep se tornou parte obrigatória do arcabouço computacional na programação em geral, principalmente nos sistemas baseados em Unix (Mac e Linux). Esta história foi contada em Where GREP Came From - Computerphile. Outros detalhes mais técnicos podem ser vistos aqui A identificação de autoria pelo padrão de escrita veio a ser chamado de “digitais linguísticas” (linguistic fingerprint). Posteriormente, um grupo de pesquisadores da Universidade de Aston usaram de técnica semelhante (porém mais avançada) para identificar a identidade real de Satoshi Nakamoto, criador do Bitcoin e da tecnologia de blockchain. 6.1.1.5 Escapando (escaping) Vimos que os símbolos [, ], (, ), {, }, os operadores *, +, ? ., |, - possuem um significado especial nas expressões regulares. E para usar estes mesmos símbolos literalmente? Para isto, usamos o “escape”, usando uma barra '\\' antes destes símbolos. Assim, se quisermos indicar o ponto final, usamos \\.. No caso do R, usamos barra dupla. O exemplo anterior ficaria \\\\.. 6.1.1.6 Classes POSIX POSIX Descrição Equivalente [:digit:] ou \\\\d Dígitos: 0 1 2 3 4 5 6 7 8 9. [0-9] [:lower:] letras em minúsculo (Lower-case) [a-z] [:upper:] Caracteres maiúsculos [A-Z] [:alpha:] Caracteres alfabéticos: [:lower:] and [:upper:]. [a-zA-Z] [:alnum:] ou \\w Caracteres alfanuméricos: [:alpha:] e [:digit:]. [a-zA-Z0-9] ou [A-z0-9] \\W Não palavra [^A-z0-9] [:blank:] Caracteres vazios, como espaço e tab. [:cntrl:] Caracteres de controle, como \\n, \\r, [\\x00-\\x1F\\x7F] [:graph:] Caracteres gráficos : [:alnum:] e [:punct:]. [:print:] Caracteres impimíveis: [:alnum:], [:punct:] and space. [:space:] Caracteres de espaço: tab \\t, nova linha \\n, tab vertical, form feed, carriage return \\r, espaço e outros similares [:xdigit:] Dígitos hexadecimais: 0 1 2 3 4 5 6 7 8 9 A B C D E F a b c d e f. \\s espaço em branco [ ] \\S (maiúsculo) tudo, exceto espaço em branco [^ ] [:punct:] Caracteres de pontuação: ‘! ” # $ % &amp; ' ( ) * + , - . / : ; &lt; = &gt; ? @ [  ] ^ _ ` { } ~ . |’ Mas no R, para usar esta classe posix, deve-se reforçar as colchetes. Assim, para usarmos [:punct:] no R, devemos fazê-lo assim: [[:punct:]] 6.1.1.7 Backreference Podemos reaproveitar trechos através de backreference. Ao fazer substituições nos textos, um modo de delimitar ou reorganizar os elementos é feita com o uso de backreference. Indicamos o que queremos reutilizar com os parênteses, e reutilizamos através de numeração como ‘\\1’, ‘\\2’. nome &lt;- &quot;Fulano Silva&quot; gsub(&#39;(.*) (.*)&#39;, &#39;\\\\2,\\\\1.&#39;, nome) ## [1] &quot;Silva,Fulano.&quot; Onde \\\\0 indica toda a string, \\\\1 indica o primeiro caso, \\\\2 indica o segundo e assim por diante. 6.1.2 Dicas/Sugestões SICSS 2019 – Basic text analysis with grep. Video do YouTube do Summer Institute in Computational Social Science. https://www.rexegg.com/regex-quickstart.html 6.1.3 Indicação de leitura sobre Regex JURAFSKY, Dan.; MARTIN, James H.Regular Expressions, Text Normalization, Edit Distance in __Speech and language processing: An introduction to speech recognition, computational linguistics and natural language processing. Upper Saddle River, NJ: Prentice Hall, 2020. Cap.2 p.2-28 FRIEDL, Jeffrey E. F. Mastering Regular Expressions, 3rd Edition. 2006. O’Reilly Media, Inc. ISBN: 9780596528126. Há opção free trial do livro. cheatsheet de expressões regulares (em inglês) Softwares Para lidar com dados bagunçados (“messy data”), uma sugestão é usar o OpenRefine. Antes “Google Refine”, mas após a Google abandonar o projeto, virou “Open Refine”, livre e de código aberto, ajuda a padronizar dados de tabelas, planilhas, limpando, corrigindo, filtrando (com regex), clusterizando ou transformando em outro formato. No site há tutoriais. Ele abre no seu navegador de internet, mas roda direto do seu computador. Usam esta ferramenta do OpenRefine, por exemplo, o Basômetro do Estadão Dados, que mede o apoio que o governo federal tem no legislativo e o ProPublica de jornalismo investigativo independente, No próprio site do OpenRefine há tutoriais e no ProPublica, ambos em inglês). 6.2 RegEx no R O uso de regex vem por padrão no R, no pacote base, mas há também uma expansão de regex com o pacote stringr que veremos mais à frente. 6.2.1 Grep Como vimos, o grep serve para filtrar linhas que contenham algo que buscamos. Primeiro, vamos criar duas variáveis com texto. nosso_texto &lt;- &quot;Estamos aprendendo expressão regular no R. Bla bla bla no no no.&quot; poema &lt;- c(&quot;E agora, José?&quot;, &quot;A festa acabou,&quot; , &quot;a luz apagou,&quot;, &quot;o povo sumiu,&quot;, &quot;a noite esfriou&quot;, &quot;e agora, José?&quot;) Usaremos estas frases como exemplos a seguir: 6.2.1.1 grepl() Com o grepl(\"Termo\", Nome_variável) verificamos se uma expressão está presente em nosso texto e ele nos retorna booleanos TRUE(verdadeiro, termo encontrado) ou FALSE (falso, termo não encontrado). grepl(&quot;Estamos&quot;, nosso_texto) ## [1] TRUE grepl(&quot;paralelepípedo&quot;, nosso_texto) ## [1] FALSE grepl(&quot;festa&quot;, poema) ## [1] FALSE TRUE FALSE FALSE FALSE FALSE 6.2.1.2 grep(“padrão”, variável) Tendo várias linhas em um vetor, como por exemplo: grep(&quot;festa&quot;, poema) ## [1] 2 grep(&quot;José&quot;, poema) ## [1] 1 6 grep(&quot;josé&quot;, poema) ## integer(0) No primeiro e segundo exemplo, buscamos por “festa” e “José” e o R nos retornou o local do termo buscado no vetor. No terceiro exemplo, o termo “josé” - todo em minúsculo - não existia, e por isso o R retornou integer(0). Se quisermos ver as respectivas linhas, fazemos assim: poema[grep(&quot;José&quot;, poema)] ## [1] &quot;E agora, José?&quot; &quot;e agora, José?&quot; poema[grep(&quot;festa&quot;, poema)] ## [1] &quot;A festa acabou,&quot; Ou ainda, usando value = TRUE) para mostrar o texto encontrado, e não apenas o índice. grep(&quot;José&quot;, poema, value = TRUE) ## [1] &quot;E agora, José?&quot; &quot;e agora, José?&quot; grep(&quot;festa&quot;, poema, value = TRUE) ## [1] &quot;A festa acabou,&quot; O grep em sua versão completa no R, pode ter ainda as seguintes designações: &gt; grep(padrão, nome_variável, ignore.case = FALSE, perl = FALSE, value = FALSE, fixed = FALSE, useBytes = FALSE, invert = FALSE) Campo do grep Descrição padrão padrão regex a ser buscado nome_variável Nome da variável. Se o grep vier após um pipe (%&gt;%), é substituído por ponto . ignore.case = FALSE se ignore.case = TRUE , então a busca não distinguirá maiúscula de minúscula perl = FALSE usa o padrão Perl de regex value = FALSE Imprime os índices. Se TRUE, mostra os elementos. `| |fixed = FALSE| considera a busca como texto puro, sem nenhuma expressão regex| |useBytes = FALSE| seTRUEa busca é feita byte-a-byte ao invés de caractere a caractere| |invert = FALSE| Seinvert = TRUE`, então serão exibidas todas as linhas, exceto as que contenham o padrão buscado. https://stat.ethz.ch/R-manual/R-devel/library/base/html/grep.html 6.2.2 gsub() Se quisermos substituir trechos de nossa frase, usamos gsub(\"o que será retirado\", \"O que queremos colocar no lugar\", nome_variável) gsub(&quot;Bla&quot;, &quot;paralelepípedo&quot;, nosso_texto) ## [1] &quot;Estamos aprendendo expressão regular no R. paralelepípedo bla bla no no no.&quot; gsub(&quot;bla&quot;, &quot;paralelepípedo&quot;, nosso_texto) ## [1] &quot;Estamos aprendendo expressão regular no R. Bla paralelepípedo paralelepípedo no no no.&quot; Também podemos usar gsub para limpar nosso texto, colocando \"\" no campo intermediário da nossa função gsub. Por exemplo, se quisermos retirar os “no”: gsub(&quot;no&quot;, &quot;&quot;, nosso_texto) ## [1] &quot;Estamos aprendendo expressão regular R. Bla bla bla .&quot; Ou se quisermos retirar os “bla”: gsub(&quot;bla&quot;, &quot;&quot;, nosso_texto) ## [1] &quot;Estamos aprendendo expressão regular no R. Bla no no no.&quot; gsub(&quot;[bB]la&quot;, &quot;&quot;, nosso_texto) ## [1] &quot;Estamos aprendendo expressão regular no R. no no no.&quot; gsub(&quot;?&quot;, &quot;&quot;, poema) ## [1] &quot;E agora, José?&quot; &quot;A festa acabou,&quot; &quot;a luz apagou,&quot; &quot;o povo sumiu,&quot; ## [5] &quot;a noite esfriou&quot; &quot;e agora, José?&quot; gsub(&quot;,&quot;, &quot;&quot;, poema) ## [1] &quot;E agora José?&quot; &quot;A festa acabou&quot; &quot;a luz apagou&quot; &quot;o povo sumiu&quot; ## [5] &quot;a noite esfriou&quot; &quot;e agora José?&quot; A retirada da vírgula ocorreu normalmente. Já a interrogação continuou, uma vez que é um caractere usado com significado específico no regex. Devemos então indicar que se trata de uma interrogação comum. gsub(&quot;\\\\?&quot;, &quot;&quot;, poema) ## [1] &quot;E agora, José&quot; &quot;A festa acabou,&quot; &quot;a luz apagou,&quot; &quot;o povo sumiu,&quot; ## [5] &quot;a noite esfriou&quot; &quot;e agora, José&quot; E retirando também a vírgula: gsub(&quot;[\\\\?,]&quot;, &quot;&quot;, poema) ## [1] &quot;E agora José&quot; &quot;A festa acabou&quot; &quot;a luz apagou&quot; &quot;o povo sumiu&quot; ## [5] &quot;a noite esfriou&quot; &quot;e agora José&quot; 6.2.3 Exercício: Qual o Qualis de certas revistas? Quais revistas possuem certo Qualis? Vamos montar um script de busca dos Qualis de revistas de sociologia. Vasculhando a internet, encontrei uma lista com os Qualis das revistas de sociologia em pdf. Precisamos extrair a informação do pdf, precisamos dos pacotes pdftools e tidyverse. O comando pdf_text() do pacote pdf tools carrega tanto arquivos locais, em seu computador, como direto da internet. library(pdftools) library(tidyverse) # O endereço de nosso pdf PDF.url = &quot;https://www2.ufjf.br/ppgcso/wp-content/uploads/sites/133/2015/04/1-Qualis-Sociologia-Atualizado.pdf&quot; # Carregando o conteúdo do pdf na variável &quot;meupdf&quot; meupdf &lt;- pdf_text(PDF.url) # Vamos checar se deu tudo certo, imprimindo apenas as primeiras linhas head(meupdf) ## [1] &quot; Consulta por Classificação / Área Avaliação\\nISSN TÍTULO ESTRATO ÁREA DE AVALIAÇÃO STATUS\\n0335-5322 Actes de la Recherche en Sciences Sociales A1 SOCIOLOGIA Atualizado\\n\\n0002-7294 American Anthropologist A1 SOCIOLOGIA Atualizado\\n\\n1042-0533 American Journal of Human Biology A1 SOCIOLOGIA Atualizado\\n\\n0003-2573 Análise Social A1 SOCIOLOGIA Atualizado\\n\\n1573-3416 An International Journal of Politics, Culture and Society (Dordrecht. Online) A1 SOCIOLOGIA Atualizado\\n\\n0004-0002 Archives of Sexual Behavior A1 SOCIOLOGIA Atualizado\\n\\n0261-3050 Bulletin of Latin American Research A1 SOCIOLOGIA Atualizado\\n\\n1983-8239 Caderno CRH (Online) A1 SOCIOLOGIA Atualizado\\n\\n0103-4979 Caderno CRH (UFBA. Impresso) A1 SOCIOLOGIA Atualizado\\n\\n0104-8333 Cadernos Pagu (UNICAMP. Impresso) A1 SOCIOLOGIA Atualizado\\n\\n0010-4159 Comparative Politics A1 SOCIOLOGIA Atualizado\\n\\n1351-0487 Constellations (Oxford. Print) A1 SOCIOLOGIA Atualizado\\n\\n0094-3061 Contemporary Sociology (Washington) A1 SOCIOLOGIA Atualizado\\n\\n1354-067X Culture &amp; Psychology A1 SOCIOLOGIA Atualizado\\n\\n0011-3204 Current Anthropology A1 SOCIOLOGIA Atualizado\\n\\n0011-3921 Current Sociology (Print) A1 SOCIOLOGIA Atualizado\\n\\n0011-5258 Dados (Rio de Janeiro. Impresso) A1 SOCIOLOGIA Atualizado\\n\\n1351-0347 Democratization (London) A1 SOCIOLOGIA Atualizado\\n\\n0101-7330 Educação &amp; Sociedade (Impresso) A1 SOCIOLOGIA Atualizado\\n\\n1044-3983 Epidemiology (Cambridge, Mass., Print) A1 SOCIOLOGIA Atualizado\\n\\n0185-4186 Estudios Sociológicos A1 SOCIOLOGIA Atualizado\\n\\n0141-9870 Ethnic and Racial Studies (Print) A1 SOCIOLOGIA Atualizado\\n\\n0873-6561 Etnográfica (Lisboa A1 SOCIOLOGIA Atualizado\\n\\n\\n\\n 1 Quarta-feira 04 Fevereiro 2015 18:33:10\\n&quot; ## [2] &quot;ISSN TÍTULO ESTRATO ÁREA DE AVALIAÇÃO STATUS\\n0014-2182 Etudes Rurales A1 SOCIOLOGIA Atualizado\\n\\n1368-4310 European Journal of Social Theory A1 SOCIOLOGIA Atualizado\\n\\n0104-5970 História, Ciências, Saúde-Manguinhos (Impresso) A1 SOCIOLOGIA Atualizado\\n\\n1678-4758 História, Ciências, Saúde-Manguinhos (Online) A1 SOCIOLOGIA Atualizado\\n\\n1806-9983 Horizontes Antropológicos (Online) A1 SOCIOLOGIA Atualizado\\n\\n0104-7183 Horizontes Antropológicos (UFRGS. Impresso) A1 SOCIOLOGIA Atualizado\\n\\n0539-0184 Information sur les Sciences Sociales (Paris) A1 SOCIOLOGIA Atualizado\\n\\n0891-4486 International Journal of Politics, Culture and Society A1 SOCIOLOGIA Atualizado\\n\\n2182-4096 International Journal on Working Conditions A1 SOCIOLOGIA Atualizado\\n\\n0192-5121 International Political Science Review A1 SOCIOLOGIA Atualizado\\n\\n0269-2171 International Review of Applied Economics A1 SOCIOLOGIA Atualizado\\n\\n0020-8701 International Social Science Journal (Print) A1 SOCIOLOGIA Atualizado\\n\\n0268-5809 International Sociology A1 SOCIOLOGIA Atualizado\\n\\n1468-795X Journal of Classical Sociology A1 SOCIOLOGIA Atualizado\\n\\n1467-6443 Journal of Historical Sociology (Online) A1 SOCIOLOGIA Atualizado\\n\\n0022-216X Journal of Latin American Studies (Print) A1 SOCIOLOGIA Atualizado\\n\\n0022-4537 Journal of Social Issues (Print) A1 SOCIOLOGIA Atualizado\\n\\n0094-582X Latin American Perspectives A1 SOCIOLOGIA Atualizado\\n\\n0023-8791 Latin American Research Review A1 SOCIOLOGIA Atualizado\\n\\n0102-6445 Lua Nova (Impresso) A1 SOCIOLOGIA Atualizado\\n\\n1678-4944 Mana (Rio de Janeiro. Online) A1 SOCIOLOGIA Atualizado\\n\\n0104-9313 Mana (UFRJ. Impresso) A1 SOCIOLOGIA Atualizado\\n\\n0101-3300 Novos Estudos CEBRAP (Impresso) A1 SOCIOLOGIA Atualizado\\n\\n0102-4469 Perspectiva Teológica (Belo Horizonte) A1 SOCIOLOGIA Atualizado\\n\\n\\n\\n 2 Quarta-feira 04 Fevereiro 2015 18:33:10\\n&quot; ## [3] &quot;ISSN TÍTULO ESTRATO ÁREA DE AVALIAÇÃO STATUS\\n0048-3931 Philosophy of the Social Sciences A1 SOCIOLOGIA Atualizado\\n\\n0191-4537 Philosophy &amp; Social Criticism A1 SOCIOLOGIA Atualizado\\n\\n0276-5624 Research in Social Stratification and Mobility A1 SOCIOLOGIA Atualizado\\n\\n0048-7333 Research Policy A1 SOCIOLOGIA Atualizado\\n\\n0102-6909 Revista Brasileira de Ciências Sociais (Impresso) A1 SOCIOLOGIA Atualizado\\n\\n0254-1106 Revista Crítica de Ciências Sociais A1 SOCIOLOGIA Atualizado\\n\\n0034-7701 Revista de Antropologia (USP. Impresso) A1 SOCIOLOGIA Atualizado\\n\\n0188-2503 Revista Mexicana de SociologÍa A1 SOCIOLOGIA Atualizado\\n\\n0390-6701 Revue Internationale de Sociologie A1 SOCIOLOGIA Atualizado\\n\\n0037-7686 Social Compass (Imprimé) A1 SOCIOLOGIA Atualizado\\n\\n0037-7732 Social Forces A1 SOCIOLOGIA Atualizado\\n\\n0303-8300 Social Indicators Research A1 SOCIOLOGIA Atualizado\\n\\n0277-9536 Social Science &amp; Medicine (1982) A1 SOCIOLOGIA Atualizado\\n\\n0102-6992 Sociedade e Estado (UnB. Impresso) A1 SOCIOLOGIA Atualizado\\n\\n0038-0199 Sociologia Ruralis (Print) A1 SOCIOLOGIA Atualizado\\n\\n1517-4522 Sociologias (UFRGS. Impresso) A1 SOCIOLOGIA Atualizado\\n\\n0735-2751 Sociological Theory A1 SOCIOLOGIA Atualizado\\n\\n0038-0296 Sociologie du Travail A1 SOCIOLOGIA Atualizado\\n\\n1069-4404 Sociology of Religion A1 SOCIOLOGIA Atualizado\\n\\n0340-918X Soziologie (Opladen) A1 SOCIOLOGIA Atualizado\\n\\n0103-2070 Tempo Social (USP. Impresso) A1 SOCIOLOGIA Atualizado\\n\\n0002-7162 The Annals of the American Academy of Political and Social Science A1 SOCIOLOGIA Atualizado\\n\\n0263-2764 Theory, Culture &amp; Society A1 SOCIOLOGIA Atualizado\\n\\n0725-5136 Thesis Eleven (Print) A1 SOCIOLOGIA Atualizado\\n\\n\\n\\n 3 Quarta-feira 04 Fevereiro 2015 18:33:10\\n&quot; ## [4] &quot;ISSN TÍTULO ESTRATO ÁREA DE AVALIAÇÃO STATUS\\n0730-8884 Work and Occupations A1 SOCIOLOGIA Atualizado\\n\\n0305-750X World Development A1 SOCIOLOGIA Atualizado\\n\\n0954-0121 Aids Care (Print) A2 SOCIOLOGIA Atualizado\\n\\n0889-2229 AIDS Research and Human Retroviruses A2 SOCIOLOGIA Atualizado\\n\\n1414-753X Ambiente e Sociedade (Campinas) A2 SOCIOLOGIA Atualizado\\n\\n1809-4422 Ambiente &amp; Sociedade (Online) A2 SOCIOLOGIA Atualizado\\n\\n0044-7447 Ambio (Oslo) A2 SOCIOLOGIA Atualizado\\n\\n1130-2887 América Latina Hoy A2 SOCIOLOGIA Atualizado\\n\\n0161-7761 Anthropology &amp; Education Quarterly A2 SOCIOLOGIA Atualizado\\n\\n0210-1963 Arbor (Madrid) A2 SOCIOLOGIA Atualizado\\n\\n0863-1808 Berliner Journal fur Soziologie A2 SOCIOLOGIA Atualizado\\n\\n0102-311X Cadernos de Saúde Pública (ENSP. Impresso) A2 SOCIOLOGIA Atualizado\\n\\n1298-6046 Cahiers du Genre (Paris) A2 SOCIOLOGIA Atualizado\\n\\n0008-0276 Cahiers Internationaux de Sociologie A2 SOCIOLOGIA Atualizado\\n\\n0826-3663 Canadian Journal of Latin American and Caribbean Studies A2 SOCIOLOGIA Atualizado\\n\\n1188-3774 Canadian Journal of Urban Research A2 SOCIOLOGIA Atualizado\\n\\n0309-8168 Capital &amp; Class A2 SOCIOLOGIA Atualizado\\n\\n0341-8162 Catena (Cremlingen) A2 SOCIOLOGIA Atualizado\\n\\n1519-6089 Civitas: Revista de Ciências Sociais (Impresso) A2 SOCIOLOGIA Atualizado\\n\\n0010-4086 Comparative Education Review A2 SOCIOLOGIA Atualizado\\n\\n0360-1315 Computers and Education A2 SOCIOLOGIA Atualizado\\n\\n0102-8529 Contexto Internacional (PUCRJ. Impresso) A2 SOCIOLOGIA Atualizado\\n\\n1478-0046 Cultural and Social History (Online) A2 SOCIOLOGIA Atualizado\\n\\n0419-1633 Diogène (Ed. Française) A2 SOCIOLOGIA Atualizado\\n\\n\\n\\n 4 Quarta-feira 04 Fevereiro 2015 18:33:10\\n&quot; ## [5] &quot;ISSN TÍTULO ESTRATO ÁREA DE AVALIAÇÃO STATUS\\n0392-1921 Diogenes (English ed.) A2 SOCIOLOGIA Atualizado\\n\\n1362-024X Durkheimian Studies A2 SOCIOLOGIA Atualizado\\n\\n0921-8009 Ecological Economics (Amsterdam) A2 SOCIOLOGIA Atualizado\\n\\n0143-831X Economic and Industrial Democracy A2 SOCIOLOGIA Atualizado\\n\\n0142-5455 Employee Relations A2 SOCIOLOGIA Atualizado\\n\\n0104-4036 Ensaio (Fundação Cesgranrio. Impresso) A2 SOCIOLOGIA Atualizado\\n\\n0014-1844 Ethnos (Stockholm) A2 SOCIOLOGIA Atualizado\\n\\n0015-704X Fordham Law Review A2 SOCIOLOGIA Atualizado\\n\\n0016-3287 Futures (London) A2 SOCIOLOGIA Atualizado\\n\\n0197-3975 Habitat International A2 SOCIOLOGIA Atualizado\\n\\n1468-2737 Hispanic Research Journal A2 SOCIOLOGIA Atualizado\\n\\n0018-7615 Humboldt (Spanische Ausg.) A2 SOCIOLOGIA Atualizado\\n\\n0265-5012 IDS Bulletin (Brighton. 1984) A2 SOCIOLOGIA Atualizado\\n\\n0306-4379 Information Systems (Oxford) A2 SOCIOLOGIA Atualizado\\n\\n1469-8412 Innovation: The European Journal of Social Science Research A2 SOCIOLOGIA Atualizado\\n\\n0034-9690 Interamerican Journal of Psychology A2 SOCIOLOGIA Atualizado\\n\\n1861-1303 International Journal of Action Research A2 SOCIOLOGIA Atualizado\\n\\n0020-8523 International Review of Administrative Sciences A2 SOCIOLOGIA Atualizado\\n\\n0037-9174 Journal de la Société des Américanistes A2 SOCIOLOGIA Atualizado\\n\\n1471-0358 Journal of Agrarian Change (Print) A2 SOCIOLOGIA Atualizado\\n\\n0168-7034 Journal of Consumer Policy A2 SOCIOLOGIA Atualizado\\n\\n1353-7903 Journal of Contemporary Religion A2 SOCIOLOGIA Atualizado\\n\\n1533-7928 Journal of Machine Learning Research (Online) A2 SOCIOLOGIA Atualizado\\n\\n0306-6150 Journal of Peasant Studies A2 SOCIOLOGIA Atualizado\\n\\n\\n\\n 5 Quarta-feira 04 Fevereiro 2015 18:33:10\\n&quot; ## [6] &quot;ISSN TÍTULO ESTRATO ÁREA DE AVALIAÇÃO STATUS\\n0047-2697 Journal of Political &amp; Military Sociology A2 SOCIOLOGIA Atualizado\\n\\n1053-1858 Journal of Public Administration Research and Theory A2 SOCIOLOGIA Atualizado\\n\\n0022-4200 Journal of Religion in Africa (Print) A2 SOCIOLOGIA Atualizado\\n\\n0022-4529 Journal of Social History A2 SOCIOLOGIA Atualizado\\n\\n1744-2222 Latin American and Caribbean Ethnic Studies (Print) A2 SOCIOLOGIA Atualizado\\n\\n1531-426X Latin American Politics and Society A2 SOCIOLOGIA Atualizado\\n\\n0439-4216 L&#39;Homme (Paris. 1961) A2 SOCIOLOGIA Atualizado\\n\\n0024-7413 Luso-Brazilian Review A2 SOCIOLOGIA Atualizado\\n\\n1548-9957 Luso-Brazilian Review (Online) A2 SOCIOLOGIA Atualizado\\n\\n1240-1307 Natures Sciences Sociétés A2 SOCIOLOGIA Atualizado\\n\\n0028-6060 New Left Review A2 SOCIOLOGIA Atualizado\\n\\n1461-4448 New Media &amp; Society (Print) A2 SOCIOLOGIA Atualizado\\n\\n0251-3552 Nueva Sociedad A2 SOCIOLOGIA Atualizado\\n\\n0104-6276 Opinião Pública (UNICAMP. Impresso) A2 SOCIOLOGIA Atualizado\\n\\n0146-1672 Personality &amp; Social Psychology Bulletin A2 SOCIOLOGIA Atualizado\\n\\n1203-9438 Politique et Sociétés (Montréal) A2 SOCIOLOGIA Atualizado\\n\\n0032-471X Population Review (Print) A2 SOCIOLOGIA Atualizado\\n\\n1544-8444 Population, Space and Place A2 SOCIOLOGIA Atualizado\\n\\n0102-7972 Psicologia: Reflexão e Crítica (UFRGS. Impresso) A2 SOCIOLOGIA Atualizado\\n\\n0963-6625 Public Understanding of Science (Print) A2 SOCIOLOGIA Atualizado\\n\\n0100-8587 Religião &amp; Sociedade (Impresso) A2 SOCIOLOGIA Atualizado\\n\\n0968-8080 Reproductive Health Matters (Print) A2 SOCIOLOGIA Atualizado\\n\\n0102-3098 Revista Brasileira de Estudos de População (Impresso) A2 SOCIOLOGIA Atualizado\\n\\n1806-9347 Revista Brasileira de História (Online) A2 SOCIOLOGIA Atualizado\\n\\n\\n\\n 6 Quarta-feira 04 Fevereiro 2015 18:33:10\\n&quot; Vamos filtrar a tabela; toda linha que tiver “Soziol” irá aparecer na nossa busca. (escolhi este nome por gerar poucos resultados e não encher nossa tela) grep(&quot;Soziolo&quot;, meupdf, value=T) ## [1] &quot;ISSN TÍTULO ESTRATO ÁREA DE AVALIAÇÃO STATUS\\n0048-3931 Philosophy of the Social Sciences A1 SOCIOLOGIA Atualizado\\n\\n0191-4537 Philosophy &amp; Social Criticism A1 SOCIOLOGIA Atualizado\\n\\n0276-5624 Research in Social Stratification and Mobility A1 SOCIOLOGIA Atualizado\\n\\n0048-7333 Research Policy A1 SOCIOLOGIA Atualizado\\n\\n0102-6909 Revista Brasileira de Ciências Sociais (Impresso) A1 SOCIOLOGIA Atualizado\\n\\n0254-1106 Revista Crítica de Ciências Sociais A1 SOCIOLOGIA Atualizado\\n\\n0034-7701 Revista de Antropologia (USP. Impresso) A1 SOCIOLOGIA Atualizado\\n\\n0188-2503 Revista Mexicana de SociologÍa A1 SOCIOLOGIA Atualizado\\n\\n0390-6701 Revue Internationale de Sociologie A1 SOCIOLOGIA Atualizado\\n\\n0037-7686 Social Compass (Imprimé) A1 SOCIOLOGIA Atualizado\\n\\n0037-7732 Social Forces A1 SOCIOLOGIA Atualizado\\n\\n0303-8300 Social Indicators Research A1 SOCIOLOGIA Atualizado\\n\\n0277-9536 Social Science &amp; Medicine (1982) A1 SOCIOLOGIA Atualizado\\n\\n0102-6992 Sociedade e Estado (UnB. Impresso) A1 SOCIOLOGIA Atualizado\\n\\n0038-0199 Sociologia Ruralis (Print) A1 SOCIOLOGIA Atualizado\\n\\n1517-4522 Sociologias (UFRGS. Impresso) A1 SOCIOLOGIA Atualizado\\n\\n0735-2751 Sociological Theory A1 SOCIOLOGIA Atualizado\\n\\n0038-0296 Sociologie du Travail A1 SOCIOLOGIA Atualizado\\n\\n1069-4404 Sociology of Religion A1 SOCIOLOGIA Atualizado\\n\\n0340-918X Soziologie (Opladen) A1 SOCIOLOGIA Atualizado\\n\\n0103-2070 Tempo Social (USP. Impresso) A1 SOCIOLOGIA Atualizado\\n\\n0002-7162 The Annals of the American Academy of Political and Social Science A1 SOCIOLOGIA Atualizado\\n\\n0263-2764 Theory, Culture &amp; Society A1 SOCIOLOGIA Atualizado\\n\\n0725-5136 Thesis Eleven (Print) A1 SOCIOLOGIA Atualizado\\n\\n\\n\\n 3 Quarta-feira 04 Fevereiro 2015 18:33:10\\n&quot; ## [2] &quot;ISSN TÍTULO ESTRATO ÁREA DE AVALIAÇÃO STATUS\\n0730-8884 Work and Occupations A1 SOCIOLOGIA Atualizado\\n\\n0305-750X World Development A1 SOCIOLOGIA Atualizado\\n\\n0954-0121 Aids Care (Print) A2 SOCIOLOGIA Atualizado\\n\\n0889-2229 AIDS Research and Human Retroviruses A2 SOCIOLOGIA Atualizado\\n\\n1414-753X Ambiente e Sociedade (Campinas) A2 SOCIOLOGIA Atualizado\\n\\n1809-4422 Ambiente &amp; Sociedade (Online) A2 SOCIOLOGIA Atualizado\\n\\n0044-7447 Ambio (Oslo) A2 SOCIOLOGIA Atualizado\\n\\n1130-2887 América Latina Hoy A2 SOCIOLOGIA Atualizado\\n\\n0161-7761 Anthropology &amp; Education Quarterly A2 SOCIOLOGIA Atualizado\\n\\n0210-1963 Arbor (Madrid) A2 SOCIOLOGIA Atualizado\\n\\n0863-1808 Berliner Journal fur Soziologie A2 SOCIOLOGIA Atualizado\\n\\n0102-311X Cadernos de Saúde Pública (ENSP. Impresso) A2 SOCIOLOGIA Atualizado\\n\\n1298-6046 Cahiers du Genre (Paris) A2 SOCIOLOGIA Atualizado\\n\\n0008-0276 Cahiers Internationaux de Sociologie A2 SOCIOLOGIA Atualizado\\n\\n0826-3663 Canadian Journal of Latin American and Caribbean Studies A2 SOCIOLOGIA Atualizado\\n\\n1188-3774 Canadian Journal of Urban Research A2 SOCIOLOGIA Atualizado\\n\\n0309-8168 Capital &amp; Class A2 SOCIOLOGIA Atualizado\\n\\n0341-8162 Catena (Cremlingen) A2 SOCIOLOGIA Atualizado\\n\\n1519-6089 Civitas: Revista de Ciências Sociais (Impresso) A2 SOCIOLOGIA Atualizado\\n\\n0010-4086 Comparative Education Review A2 SOCIOLOGIA Atualizado\\n\\n0360-1315 Computers and Education A2 SOCIOLOGIA Atualizado\\n\\n0102-8529 Contexto Internacional (PUCRJ. Impresso) A2 SOCIOLOGIA Atualizado\\n\\n1478-0046 Cultural and Social History (Online) A2 SOCIOLOGIA Atualizado\\n\\n0419-1633 Diogène (Ed. Française) A2 SOCIOLOGIA Atualizado\\n\\n\\n\\n 4 Quarta-feira 04 Fevereiro 2015 18:33:10\\n&quot; ## [3] &quot;ISSN TÍTULO ESTRATO ÁREA DE AVALIAÇÃO STATUS\\n0034-8910 Revista de Saúde Pública (Impresso) A2 SOCIOLOGIA Atualizado\\n\\n1518-8787 Revista de Saúde Pública (Online) A2 SOCIOLOGIA Atualizado\\n\\n1678-9873 Revista de Sociologia e Política (Online) A2 SOCIOLOGIA Atualizado\\n\\n0104-4478 Revista de Sociologia e Política (UFPR. Impresso) A2 SOCIOLOGIA Atualizado\\n\\n0104-026X Revista Estudos Feministas (UFSC. Impresso) A2 SOCIOLOGIA Atualizado\\n\\n0378-5548 Revista Internacional del Trabajo (Impresa) A2 SOCIOLOGIA Atualizado\\n\\n0034-9712 Revista Internacional de Sociología A2 SOCIOLOGIA Atualizado\\n\\n1293-8882 Revue Tiers Monde A2 SOCIOLOGIA Atualizado\\n\\n1363-4607 Sexualities (London) A2 SOCIOLOGIA Atualizado\\n\\n0097-9740 Signs (Chicago, Ill.) A2 SOCIOLOGIA Atualizado\\n\\n1350-4630 Social Identities (Print) A2 SOCIOLOGIA Atualizado\\n\\n0765-3697 Sociétés (Paris) A2 SOCIOLOGIA Atualizado\\n\\n0210-8364 Sociología del Trabajo A2 SOCIOLOGIA Atualizado\\n\\n2152-8586 South African Review of Sociology A2 SOCIOLOGIA Atualizado\\n\\n1654-0204 Stockholm Review of Latin American Studies A2 SOCIOLOGIA Atualizado\\n\\n1477-7487 Surveillance &amp; Society (Online) A2 SOCIOLOGIA Atualizado\\n\\n0730-479X The Tocqueville Review A2 SOCIOLOGIA Atualizado\\n\\n0143-6597 Third World Quarterly (Print) A2 SOCIOLOGIA Atualizado\\n\\n1745-641X Work Organisation, Labour &amp; Globalisation (Print) A2 SOCIOLOGIA Atualizado\\n\\n1935-6226 World Political Science Review A2 SOCIOLOGIA Atualizado\\n\\n0340-1804 Zeitschrift fur Soziologie A2 SOCIOLOGIA Atualizado\\n\\n0994-4524 Actuel Marx B1 SOCIOLOGIA Atualizado\\n\\n0095-3997 Administration &amp; Society B1 SOCIOLOGIA Atualizado\\n\\n1057-6290 Advances in Medical Sociology B1 SOCIOLOGIA Atualizado\\n\\n\\n\\n 7 Quarta-feira 04 Fevereiro 2015 18:33:10\\n&quot; Filtrando a tabela, toda linha que tiver “A1” irá aparecer, não importa em que coluna esteja. grep(&quot;A1&quot;, meupdf, value=T) ## [1] &quot; Consulta por Classificação / Área Avaliação\\nISSN TÍTULO ESTRATO ÁREA DE AVALIAÇÃO STATUS\\n0335-5322 Actes de la Recherche en Sciences Sociales A1 SOCIOLOGIA Atualizado\\n\\n0002-7294 American Anthropologist A1 SOCIOLOGIA Atualizado\\n\\n1042-0533 American Journal of Human Biology A1 SOCIOLOGIA Atualizado\\n\\n0003-2573 Análise Social A1 SOCIOLOGIA Atualizado\\n\\n1573-3416 An International Journal of Politics, Culture and Society (Dordrecht. Online) A1 SOCIOLOGIA Atualizado\\n\\n0004-0002 Archives of Sexual Behavior A1 SOCIOLOGIA Atualizado\\n\\n0261-3050 Bulletin of Latin American Research A1 SOCIOLOGIA Atualizado\\n\\n1983-8239 Caderno CRH (Online) A1 SOCIOLOGIA Atualizado\\n\\n0103-4979 Caderno CRH (UFBA. Impresso) A1 SOCIOLOGIA Atualizado\\n\\n0104-8333 Cadernos Pagu (UNICAMP. Impresso) A1 SOCIOLOGIA Atualizado\\n\\n0010-4159 Comparative Politics A1 SOCIOLOGIA Atualizado\\n\\n1351-0487 Constellations (Oxford. Print) A1 SOCIOLOGIA Atualizado\\n\\n0094-3061 Contemporary Sociology (Washington) A1 SOCIOLOGIA Atualizado\\n\\n1354-067X Culture &amp; Psychology A1 SOCIOLOGIA Atualizado\\n\\n0011-3204 Current Anthropology A1 SOCIOLOGIA Atualizado\\n\\n0011-3921 Current Sociology (Print) A1 SOCIOLOGIA Atualizado\\n\\n0011-5258 Dados (Rio de Janeiro. Impresso) A1 SOCIOLOGIA Atualizado\\n\\n1351-0347 Democratization (London) A1 SOCIOLOGIA Atualizado\\n\\n0101-7330 Educação &amp; Sociedade (Impresso) A1 SOCIOLOGIA Atualizado\\n\\n1044-3983 Epidemiology (Cambridge, Mass., Print) A1 SOCIOLOGIA Atualizado\\n\\n0185-4186 Estudios Sociológicos A1 SOCIOLOGIA Atualizado\\n\\n0141-9870 Ethnic and Racial Studies (Print) A1 SOCIOLOGIA Atualizado\\n\\n0873-6561 Etnográfica (Lisboa A1 SOCIOLOGIA Atualizado\\n\\n\\n\\n 1 Quarta-feira 04 Fevereiro 2015 18:33:10\\n&quot; ## [2] &quot;ISSN TÍTULO ESTRATO ÁREA DE AVALIAÇÃO STATUS\\n0014-2182 Etudes Rurales A1 SOCIOLOGIA Atualizado\\n\\n1368-4310 European Journal of Social Theory A1 SOCIOLOGIA Atualizado\\n\\n0104-5970 História, Ciências, Saúde-Manguinhos (Impresso) A1 SOCIOLOGIA Atualizado\\n\\n1678-4758 História, Ciências, Saúde-Manguinhos (Online) A1 SOCIOLOGIA Atualizado\\n\\n1806-9983 Horizontes Antropológicos (Online) A1 SOCIOLOGIA Atualizado\\n\\n0104-7183 Horizontes Antropológicos (UFRGS. Impresso) A1 SOCIOLOGIA Atualizado\\n\\n0539-0184 Information sur les Sciences Sociales (Paris) A1 SOCIOLOGIA Atualizado\\n\\n0891-4486 International Journal of Politics, Culture and Society A1 SOCIOLOGIA Atualizado\\n\\n2182-4096 International Journal on Working Conditions A1 SOCIOLOGIA Atualizado\\n\\n0192-5121 International Political Science Review A1 SOCIOLOGIA Atualizado\\n\\n0269-2171 International Review of Applied Economics A1 SOCIOLOGIA Atualizado\\n\\n0020-8701 International Social Science Journal (Print) A1 SOCIOLOGIA Atualizado\\n\\n0268-5809 International Sociology A1 SOCIOLOGIA Atualizado\\n\\n1468-795X Journal of Classical Sociology A1 SOCIOLOGIA Atualizado\\n\\n1467-6443 Journal of Historical Sociology (Online) A1 SOCIOLOGIA Atualizado\\n\\n0022-216X Journal of Latin American Studies (Print) A1 SOCIOLOGIA Atualizado\\n\\n0022-4537 Journal of Social Issues (Print) A1 SOCIOLOGIA Atualizado\\n\\n0094-582X Latin American Perspectives A1 SOCIOLOGIA Atualizado\\n\\n0023-8791 Latin American Research Review A1 SOCIOLOGIA Atualizado\\n\\n0102-6445 Lua Nova (Impresso) A1 SOCIOLOGIA Atualizado\\n\\n1678-4944 Mana (Rio de Janeiro. Online) A1 SOCIOLOGIA Atualizado\\n\\n0104-9313 Mana (UFRJ. Impresso) A1 SOCIOLOGIA Atualizado\\n\\n0101-3300 Novos Estudos CEBRAP (Impresso) A1 SOCIOLOGIA Atualizado\\n\\n0102-4469 Perspectiva Teológica (Belo Horizonte) A1 SOCIOLOGIA Atualizado\\n\\n\\n\\n 2 Quarta-feira 04 Fevereiro 2015 18:33:10\\n&quot; ## [3] &quot;ISSN TÍTULO ESTRATO ÁREA DE AVALIAÇÃO STATUS\\n0048-3931 Philosophy of the Social Sciences A1 SOCIOLOGIA Atualizado\\n\\n0191-4537 Philosophy &amp; Social Criticism A1 SOCIOLOGIA Atualizado\\n\\n0276-5624 Research in Social Stratification and Mobility A1 SOCIOLOGIA Atualizado\\n\\n0048-7333 Research Policy A1 SOCIOLOGIA Atualizado\\n\\n0102-6909 Revista Brasileira de Ciências Sociais (Impresso) A1 SOCIOLOGIA Atualizado\\n\\n0254-1106 Revista Crítica de Ciências Sociais A1 SOCIOLOGIA Atualizado\\n\\n0034-7701 Revista de Antropologia (USP. Impresso) A1 SOCIOLOGIA Atualizado\\n\\n0188-2503 Revista Mexicana de SociologÍa A1 SOCIOLOGIA Atualizado\\n\\n0390-6701 Revue Internationale de Sociologie A1 SOCIOLOGIA Atualizado\\n\\n0037-7686 Social Compass (Imprimé) A1 SOCIOLOGIA Atualizado\\n\\n0037-7732 Social Forces A1 SOCIOLOGIA Atualizado\\n\\n0303-8300 Social Indicators Research A1 SOCIOLOGIA Atualizado\\n\\n0277-9536 Social Science &amp; Medicine (1982) A1 SOCIOLOGIA Atualizado\\n\\n0102-6992 Sociedade e Estado (UnB. Impresso) A1 SOCIOLOGIA Atualizado\\n\\n0038-0199 Sociologia Ruralis (Print) A1 SOCIOLOGIA Atualizado\\n\\n1517-4522 Sociologias (UFRGS. Impresso) A1 SOCIOLOGIA Atualizado\\n\\n0735-2751 Sociological Theory A1 SOCIOLOGIA Atualizado\\n\\n0038-0296 Sociologie du Travail A1 SOCIOLOGIA Atualizado\\n\\n1069-4404 Sociology of Religion A1 SOCIOLOGIA Atualizado\\n\\n0340-918X Soziologie (Opladen) A1 SOCIOLOGIA Atualizado\\n\\n0103-2070 Tempo Social (USP. Impresso) A1 SOCIOLOGIA Atualizado\\n\\n0002-7162 The Annals of the American Academy of Political and Social Science A1 SOCIOLOGIA Atualizado\\n\\n0263-2764 Theory, Culture &amp; Society A1 SOCIOLOGIA Atualizado\\n\\n0725-5136 Thesis Eleven (Print) A1 SOCIOLOGIA Atualizado\\n\\n\\n\\n 3 Quarta-feira 04 Fevereiro 2015 18:33:10\\n&quot; ## [4] &quot;ISSN TÍTULO ESTRATO ÁREA DE AVALIAÇÃO STATUS\\n0730-8884 Work and Occupations A1 SOCIOLOGIA Atualizado\\n\\n0305-750X World Development A1 SOCIOLOGIA Atualizado\\n\\n0954-0121 Aids Care (Print) A2 SOCIOLOGIA Atualizado\\n\\n0889-2229 AIDS Research and Human Retroviruses A2 SOCIOLOGIA Atualizado\\n\\n1414-753X Ambiente e Sociedade (Campinas) A2 SOCIOLOGIA Atualizado\\n\\n1809-4422 Ambiente &amp; Sociedade (Online) A2 SOCIOLOGIA Atualizado\\n\\n0044-7447 Ambio (Oslo) A2 SOCIOLOGIA Atualizado\\n\\n1130-2887 América Latina Hoy A2 SOCIOLOGIA Atualizado\\n\\n0161-7761 Anthropology &amp; Education Quarterly A2 SOCIOLOGIA Atualizado\\n\\n0210-1963 Arbor (Madrid) A2 SOCIOLOGIA Atualizado\\n\\n0863-1808 Berliner Journal fur Soziologie A2 SOCIOLOGIA Atualizado\\n\\n0102-311X Cadernos de Saúde Pública (ENSP. Impresso) A2 SOCIOLOGIA Atualizado\\n\\n1298-6046 Cahiers du Genre (Paris) A2 SOCIOLOGIA Atualizado\\n\\n0008-0276 Cahiers Internationaux de Sociologie A2 SOCIOLOGIA Atualizado\\n\\n0826-3663 Canadian Journal of Latin American and Caribbean Studies A2 SOCIOLOGIA Atualizado\\n\\n1188-3774 Canadian Journal of Urban Research A2 SOCIOLOGIA Atualizado\\n\\n0309-8168 Capital &amp; Class A2 SOCIOLOGIA Atualizado\\n\\n0341-8162 Catena (Cremlingen) A2 SOCIOLOGIA Atualizado\\n\\n1519-6089 Civitas: Revista de Ciências Sociais (Impresso) A2 SOCIOLOGIA Atualizado\\n\\n0010-4086 Comparative Education Review A2 SOCIOLOGIA Atualizado\\n\\n0360-1315 Computers and Education A2 SOCIOLOGIA Atualizado\\n\\n0102-8529 Contexto Internacional (PUCRJ. Impresso) A2 SOCIOLOGIA Atualizado\\n\\n1478-0046 Cultural and Social History (Online) A2 SOCIOLOGIA Atualizado\\n\\n0419-1633 Diogène (Ed. Française) A2 SOCIOLOGIA Atualizado\\n\\n\\n\\n 4 Quarta-feira 04 Fevereiro 2015 18:33:10\\n&quot; A busca está funcionando! No entanto, vale lembrar, não conseguimos aqui delimitar nossa busca a colunas específicas. Poderíamos fazer com que buscássemos apenas na coluna com classificação. Há outros modos de importarmos estes dados e fazermos esta busca no R. Este foi um modo simples para treinarmos o regex no R. 6.2.4 Transformando strings em vetores com strsplit() É possível quebrar uma string em vários elementos de um vetor usando um padrão regex # nosso texto inicial rato_vetor &lt;- &quot;O rato roeu. Roeu a roupa. A roupa que era do rei. Qual rei? O rei de Roma!&quot; # Criando um vetor tendo como critério de quebra o ponto final, deve-se usar o símbolo de escape &quot;\\\\.&quot; strsplit(rato_vetor, &quot;\\\\.&quot;) ## [[1]] ## [1] &quot;O rato roeu&quot; &quot; Roeu a roupa&quot; ## [3] &quot; A roupa que era do rei&quot; &quot; Qual rei? O rei de Roma!&quot; # Vetores a partir da pontuação strsplit(rato_vetor, &quot;\\\\.|\\\\?&quot;) ## [[1]] ## [1] &quot;O rato roeu&quot; &quot; Roeu a roupa&quot; ## [3] &quot; A roupa que era do rei&quot; &quot; Qual rei&quot; ## [5] &quot; O rei de Roma!&quot; # Uma dica. Os elementos de vetores estão ficando com espaço em branco. Podemos resolver isso: strsplit(rato_vetor, &quot;(\\\\.|\\\\?) ?&quot;) ## [[1]] ## [1] &quot;O rato roeu&quot; &quot;Roeu a roupa&quot; &quot;A roupa que era do rei&quot; ## [4] &quot;Qual rei&quot; &quot;O rei de Roma!&quot; No pacote stringr o comando equivalente é o str_split(). Atenção: Ao tentar usar regex com espaços em branco (whitespace) e ele não funciona, é bom saber que há diversos tipos de espaços em branco. Há diferentes tipos de espaço em branco 6.2.5 Exemplo regex Voltando ao nosso exemplo anterior, podemos transformar as datas abaixo, que estão no formado dd/mm/aaaa (dia/mês/ano) para o formato aaaa-mm-dd (ano-mês-dia), para que possamos, por exemplo, ordenar as datas de modo fácil. Original Converter para 20/03/2020 “2020-03-20” 13/01/1990 “1990-01-13” 04/06/2001 “2001-06-04” datas &lt;- c(&#39;20/03/2020&#39;, &#39;13/01/1990&#39;, &#39;04/06/2001&#39;) gsub(&#39;([0-9]{2})/([0-9]{2})/([0-9]{4})&#39;,&#39;\\\\3-\\\\2-\\\\1&#39;, datas) ## [1] &quot;2020-03-20&quot; &quot;1990-01-13&quot; &quot;2001-06-04&quot; Vale lembrar, o R possui funções (como o strptime() ) e pacotes específicos para lidar com datas. O exemplo acima foi apenas um exercício. O R possui diversos pacotes para lidar com regex. As funções do base, que já vem com o R possuem diversas funções. Há ainda o pacote Google RE2 que é uma versão rápida desenvolvida pela Google. Outro pacote interessante é o stringr que veremos a seguir. 6.2.6 Lookaround lookadead e lookbehind É possível fazer com que um padrão passe a valer antes ou depois de outro padrão. Se um padrão deve ser considerado se antes de outro padrão, então usamos lookahead. Se um padrão deve ser considerado se depois de outro padrão, então usamos lookbehind. Para saber mais sobre, este manual pode ser útil. 6.3 Pacote stringr Logotipo do pacote Stringr Há problemas recorrentes com soluções recorrentes em regex. O pacote stringr nos ajuda nisso, tendo em si diversas funções para lidarmos com strings, com soluções simplificadas para problemas que recorrentemente precisamos de fazer algum regex mais complicado, ou uma sequência de comandos. O Stringr possui ainda uma sintaxe um pouco diferente. Toda função do stringr começa com str_. Na manipulação de strings, o pacote stringr possui diversas funcionalidades que nos poupam tempo x &lt;- &quot;João foi por ali! (Disse Daniel)&quot; stringr::str_to_upper(x) ## [1] &quot;JOÃO FOI POR ALI! (DISSE DANIEL)&quot; stringr::str_to_lower(x) ## [1] &quot;joão foi por ali! (disse daniel)&quot; Exemplos de algumas funções do stringr: Função Descrição Função correspondente str_c() Junta múltiplos strings em um único elemento paste() e stringi::stri_join() str_conv() Especifica o encoding de uma string. Ex.: str_conv(x, \"UTF-8\"). Para ver mais encodings disponíveis, rode stringi::stri_enc_list() str_count() Conta quantas vezes um padrão ocorre . str_detect() retorna booleanos TRUE ou FALSE da busca . str_replace() Substitui por padrões na string . str_subset() Como um grep( padrao, value=TRUE), retorna termos da busca grep( padrao, value=TRUE) Vejamos um trecho de A Divisão do Trabalho Social. durkheim.DTS &lt;- &quot;Quanto à questão que originou este trabalho, é a das relações entre a personalidade individual e a solidariedade social. Como é que, ao mesmo passo que se torna mais autônomo, o indivíduo depende mais intimamente da sociedade? Como pode ser, ao mesmo tempo, mais pessoal e mais solidário? Pois é inconteste que esses dois movimentos, por mais contraditórios que pareçam, seguem-se paralelamente.&quot; E exemplos com stringr: # contando quantas vezes um termo/padrão aparece stringr::str_count(durkheim.DTS, &quot;indiv&quot;) ## [1] 2 # extraindo um termo/padrão: todos os termos que comecem com &quot;indiv&quot; stringr::str_extract_all(durkheim.DTS, &quot;indiv\\\\w+&quot;) ## [[1]] ## [1] &quot;individual&quot; &quot;indivíduo&quot; # str_ retorna uma lista. Para termos o resultado em vetor, usamos `unlist()` stringr::str_extract_all(durkheim.DTS, &quot;indiv\\\\w+&quot;) |&gt; unlist() ## [1] &quot;individual&quot; &quot;indivíduo&quot; # substituição: apenas da primeira ocorrência do termo stringr::str_replace(durkheim.DTS, &quot;indiv\\\\w+&quot;, &quot;indiv&quot;) ## [1] &quot;Quanto à questão que originou este trabalho, é a das relações entre a personalidade indiv e a solidariedade social. Como é que, ao mesmo passo que se torna mais autônomo, o indivíduo depende mais intimamente da sociedade? Como pode ser, ao mesmo tempo, mais pessoal e mais solidário? Pois é inconteste que esses dois movimentos, por mais contraditórios que pareçam, seguem-se paralelamente.&quot; # substituição: todas as ocorrências do termo stringr::str_replace_all(durkheim.DTS, &quot;indiv\\\\w+&quot;, &quot;indiv&quot;) ## [1] &quot;Quanto à questão que originou este trabalho, é a das relações entre a personalidade indiv e a solidariedade social. Como é que, ao mesmo passo que se torna mais autônomo, o indiv depende mais intimamente da sociedade? Como pode ser, ao mesmo tempo, mais pessoal e mais solidário? Pois é inconteste que esses dois movimentos, por mais contraditórios que pareçam, seguem-se paralelamente.&quot; # podemos fazer várias substituições concatenadas, dentro de um vetor `c()` # no padrão `regex = substituição` stringr::str_replace_all(durkheim.DTS, c(&quot;indiv\\\\w+&quot; = &quot;indiv.&quot;, &quot;solid[aá]ri\\\\w+&quot; = &quot;solidar.&quot; )) ## [1] &quot;Quanto à questão que originou este trabalho, é a das relações entre a personalidade indiv. e a solidar. social. Como é que, ao mesmo passo que se torna mais autônomo, o indiv. depende mais intimamente da sociedade? Como pode ser, ao mesmo tempo, mais pessoal e mais solidar.? Pois é inconteste que esses dois movimentos, por mais contraditórios que pareçam, seguem-se paralelamente.&quot; 6.3.1 Normalização com stringr Um problema ao lidar com tabelas pode ser a normalização. Por exemplo, ao pegar dados da saúde em uma tabela que quem preenche o formulário, digita por extenso ao invés de escolher a partir de uma lista pré-definida (como são os fatores) pode ocorrer isto aqui: remedios &lt;- read.table(header = T, text = &#39; n remedio 1 bogripe 1 biozina 1 bidioxicloquina 1 betametasona 1 bengripe 1 benegripi 1 benegripe 1 BENEGRIPE 1 benegripe 1 Benegripe 1 BENEFRIPE 1 bena 1 bemegripe&#39;) Estes dados vieram da SRAG 2020, o banco de dados de Síndrome Respiratória aguda grave (salvo engano, na coluna OUT_ANTIV). Repare como um mesmo remédio está escrito de diferentes formas. Um modos de tentar padronizar esta variedade de nomes escritos de formas diferentes - mas não o único e não o mais indicado para este caso que tem muitas grafias diferentes, mas que é interessante em termos didáticos da ferramenta - é este a seguir. Podemos resolver isso com str_detect e dplyr::case_when remedios %&gt;% dplyr::mutate(remedio_novo = dplyr::case_when( stringr::str_detect( remedio,&quot;(?i)be[mn]egrip[ei]&quot;) ~ &quot;Benegripe&quot;)) ## n remedio remedio_novo ## 1 1 bogripe &lt;NA&gt; ## 2 1 biozina &lt;NA&gt; ## 3 1 bidioxicloquina &lt;NA&gt; ## 4 1 betametasona &lt;NA&gt; ## 5 1 bengripe &lt;NA&gt; ## 6 1 benegripi Benegripe ## 7 1 benegripe Benegripe ## 8 1 BENEGRIPE Benegripe ## 9 1 benegripe Benegripe ## 10 1 Benegripe Benegripe ## 11 1 BENEFRIPE &lt;NA&gt; ## 12 1 bena &lt;NA&gt; ## 13 1 bemegripe Benegripe O símbolo (?i) indica que a busca é case insenitive, isto é, não distingue maiúsclas de minúsculas e vice versa. Nosso padrão não detectou todos os casos, ficando para trás “bengripe” e “BENEFRIPE”. case_when(stringr::str_detect( nos dá a condição, se certo padrão for encontrado, faça algo \"(?i)be[mn]egrip[ei]\") o padrão a ser encontrado e ~ \"Benegripe\" diz qual valor será usado em substituição. Por fim, a intenção do exemplo acima é mostrar a ferramenta. Mas no caso deste problema, uma abordagem mais adequada seria usar alguma medida de minimum edit distance (como a de Levenstein), onde teríamos uma lista com os nomes corretos e encontraríamos as palavras mais próximas na lista com nomes com grafia errada. 6.4 Dicas/Sugestões: Regex no R Cheatsheet de expressões regulares no R (em inglês) RECOMENDADO cheatcheet stringr Introdução ao regex com R. Um manual online em portugês onde você pode rodar códigos na página à medida que aprende truques novos. “Regular Expressions as used in R”.Expressão regular no R. Documento oficial. (em inglês) Albert Y. Kim. Regular Expressions in R Regular Expressions with The R Language. Site dedicao às RegEx em várias linguagens de programação. Data Wrangling Cheatsheet em português, tradução de Augusto Queiroz de Macedo Regular expressions PENG, Roger. R Programming for Data Science. 2020. Bookdown online. J. Kyle Armstrong Fundamentals of Data Wrangling with R 2021. bookdown online O pacote regexplain ajuda a testar regexes e comandos que usam regex no R de modo interativo e fácil, utilizando interface baseada em shiny. "],["dataviz.html", "7 Visualização de dados (dataviz) 7.1 Base plots 7.2 Cores 7.3 O pacote Lattice 7.4 O pacote ggplot2 7.5 Gráficos de visualização textual", " 7 Visualização de dados (dataviz) Objetivos deste capítulo: Apresentar uma introdução à visualização de dados com pacote base nativo do R (em expansão) pacote lattice (em elaboração) pacote ggplot (em expansão) confecção de mapas simples (em elaboração) gráficos de visualização textual (em expansão) EM CONSTRUÇÃO Dicas Quer fazer gráficos no R sem programar? Tente o pacote esquisse. Vamos criar um dataframe com valores aleatórios e depois vamos criar alguns gráficos a partir dele. # criando um dataframe com valores aleatórios df &lt;- read.table( header = TRUE, sep = &quot;;&quot;, stringsAsFactors = TRUE, text = &#39; Col1;Col2;Col3 A;12;24 B;25;41 C;37;13 D;22;18&#39;) # convertendo o datatype das colunas para numeric df$Col2 &lt;- as.numeric(df$Col2) df$Col3 &lt;- as.numeric(df$Col3) 7.1 Base plots O R vem com um pacote básico, nativo de gráficos, como o hist() para histogramas, pie() para gráfico de pizza, barplot() para gráfico de barras, plot() para gráfico de dispersão (scatterplot) e gráficos de linha, e boxplot() para boxplot ou whiskerplot. 7.1.1 Histograma (histogram) no pacote base Criando um gráfico simples. Histogramas são interessantes para observar a distribuição de frequência de um determinado conjunto de valores numéricos, por exemplo, caso queira ver a distribuição de alturas das pessoas de uma sala de aula. Para tal, precisamos de valores numéricos. Vamos começar pelo histograma que necessita de apenas um vetor com dados numéricos, mas também podemos especificar o número (aproximado) de barras (breaks): dados &lt;- c(24, 22, 27, 17, 33) hist(as.integer(dados)) hist(as.integer(dados), breaks = 2) Repare que o número de colunas não bate com o número de barras no nosso gráfico. O histograma cria “breakpoints” ou “bins” usando uma fórmula (sturges). Isto permite termos valores mais arredondados ou mais precisos, a depender da visualização que quisermos. É possível especificar o número de barras com o argumento breaks = X, mas ainda assim como uma sugestão, já que o R ainda decide se o número é plausível. hist(df$Col2, breaks = 4, main=&quot;Meu Título&quot;, xlab=&quot;Descrição do eixo x&quot;, ylab=&quot;Descrição do eixo Y&quot;, labels= TRUE, # Adicionar os valores nas barras border=&quot;darkblue&quot;, col=&quot;lightblue&quot;, density=25, # adicionado hachuras xlim=c(10,40) # se quisermos delimitar os valores mínimos e máximos no eixo X ) 7.1.2 Gráfico de barras (barplot) Repetindo o exemplo da introdução ao R na seção sobre fatores, fazendo um gráfico de barras simples a partir de um vetor, mas agora explicando a parte gráfica e aproveitando o pipe do tidyverse. Perceba que o pacote já faz a soma para nós. # carregando o magritrr para usar os pipes library(magrittr) aeroportos &lt;- c(&quot;BSB&quot;, &quot;CON&quot;, &quot;BSB&quot;, &quot;VIC&quot;, &quot;GUA&quot;, &quot;FOR&quot;, &quot;MAO&quot;, &quot;GUA&quot;, &quot;CON&quot;, &quot;CON&quot;, &quot;REC&quot;, &quot;UDI&quot;, &quot;VIC&quot;, &quot;GUA&quot;) # Numa versão mínima do gráfico: aeroportos %&gt;% factor %&gt;% summary %&gt;% barplot E numa versão mais elaborada do gráfico de barras: aeroportos %&gt;% # transformando o vetor em fator para o barplot funcionar factor %&gt;% # somando os repetidos summary %&gt;% # ordendando com &quot;sort()&quot;, e de modo decrescente com &quot;decreasing = TRUE&quot; sort(., decreasing = TRUE) %&gt;% # plotando o gráfico barplot( main = &quot;Aeroportos Brasil&quot;, sub = &quot;Sublegenda&quot;, xlab = &quot;Legenda eixo x&quot;, ylab = &quot;Legenda eixo y&quot;, legend.text = &quot;Frequência&quot;, # cores col = c(&quot;darkred&quot;, &quot;indianred&quot;), # rotacionando as legendas do das barras. Bom para quando as legendas são extensas las=2, # fazer o gráfico de modo horizontal horiz = TRUE) O comando text() permite adicionar texto aos gráficos do R. e o comando legend() permite acrescentar e mudar diversos parâmetros da legenda. 7.1.3 Gráfico de Pizza (pie chart) Gráficos de pizza se tornaram controversos pois perdem o sentido de mostrar claramente diferentes proporções se tivermos muitos itens e/ou com valores próximos. Mas é necessário saber fazê-los: # Numa versão mínima do gráfico: aeroportos |&gt; factor() |&gt; summary() |&gt; pie() Gerando um gráfico de pizza mais elaborado # criando um vetor nomeado com as frequências de cada aeroporto aeroportos2 &lt;- aeroportos |&gt; factor() |&gt; summary() aeroportos2 ## BSB CON FOR GUA MAO REC UDI VIC ## 2 3 1 3 1 1 1 2 pie(aeroportos2, main = &quot;Aeroportos&quot;, labels = unname(aeroportos2), # percentual col = rainbow(length(aeroportos2)) # espectro de cores ) legend(&quot;topright&quot;, names(aeroportos2), cex = 0.8, fill = rainbow(length(aeroportos))) Se quiser fazer um gráfico de pizza 3d, dê uma olhada no pacote plotrix. 7.1.4 Gráfico de dispersão (scatterplot) Criando o gráfico com a função plot() do pacote base. Vamos precisar de especificar quais os valores de x e y de nosso gráfico: plot(x,y) plot(df$Col2, df$Col3) Um pouco de história O scattterplot foi inventado em 1833 por J. F. W. Herschel em dois papers no jornal Memoirs of the Royal Astronomical Society detalhando a órbita de estrelas gêmeas a partir da observação do ângulo relativo de sua posição e da sua distância angular. O pacote HistData possui os datasets Virginis e Virginis.interp com observações feitas por Herschel das estrelas virgens. Em 1973, o estatístico Francis Anscombe publicou o seguinte artigo: ANSCOMBE, Francis J. (1973). Graphs in statistical analysis. The American Statistician, 27, 17–21. https://doi.org/10.2307/2682899 Nele, o estatístico construiu diferentes datasets com mesmo número de elementos (onze), mesma média e desvio padrão, mas que ao virarem gráficos de dispersão, mostravam relações bem diferentes. Com isso queria chamar a atenção para a importância de visualizar relações na análise estatística e não somente a observação das medidas citados, o que poderia ser enganoso. Estes dados estão presentes no pacote datasets: anscombe e outros datasets com esta mesma intenção didática, mas com dados diferentes nos pacotes datasauRus e anscombiser. # Carregando o dataset data(anscombe) str(anscombe) ## &#39;data.frame&#39;: 11 obs. of 8 variables: ## $ x1: num 10 8 13 9 11 14 6 4 12 7 ... ## $ x2: num 10 8 13 9 11 14 6 4 12 7 ... ## $ x3: num 10 8 13 9 11 14 6 4 12 7 ... ## $ x4: num 8 8 8 8 8 8 8 19 8 8 ... ## $ y1: num 8.04 6.95 7.58 8.81 8.33 ... ## $ y2: num 9.14 8.14 8.74 8.77 9.26 8.1 6.13 3.1 9.13 7.26 ... ## $ y3: num 7.46 6.77 12.74 7.11 7.81 ... ## $ y4: num 6.58 5.76 7.71 8.84 8.47 7.04 5.25 12.5 5.56 7.91 ... # plotando o gráfico plot(anscombe$x1, anscombe$y1) Mudando o tipo de figura no gráfico através do parâmetro pch, sendo: 1: círculo 2: triângulo 3: sinal de mais 4: sinal de “x” 5: losango 6: triângulo reverso 7: Quadrado riscado com x plot(anscombe$x1, anscombe$y1, pch = 2) Se quisermos adicionar uma linha, usamos abline(). Como parâmetros: h é horizontal e v vertical. O valor a seguir define onde será o corte a significa a intersecção b é o slope plot(anscombe$x1, anscombe$y1, main= &quot;Dataset Anscombe&quot;, ylab = &quot;Rótulo eixo Y&quot;, xlab = &quot;Rótulo eixo X&quot;) abline(h=10) plot(anscombe$x1, anscombe$y1, main= &quot;Dataset Anscombe&quot;, ylab = &quot;Rótulo eixo Y&quot;, xlab = &quot;Rótulo eixo X&quot;) abline(h=9.5, #local de corte da linha col = &quot;red&quot;, # cor da linha= vermelho lty = &quot;dashed&quot;, # modifica o tipo de linha: tracejado lwd = 3 # espessura da linha ) 7.1.5 Gráfico de linha (line plot) Para o gráfico de linhas também utilizamos a função plot(), porém mudamos o tipo: l para linhas e o para linhas e pontos. plot(df$Col3, type = &quot;l&quot;) # apenas linha plot(df$Col3, type = &quot;o&quot;) # linha e ponto Gráficos de linha múltiplos plot(df$Col3, type = &quot;o&quot;, col = &quot;red&quot;) lines(df$Col2, type = &quot;o&quot;, col = &quot;blue&quot;) 7.2 Cores Para saber as cores disponíveis, usamos o comando colors(), que retorna 657 opções de cores. Se quisermos ver, por exemplo, somente as opções de vermelho: grep(&quot;red&quot;, colors(), value = T) ## [1] &quot;darkred&quot; &quot;indianred&quot; &quot;indianred1&quot; &quot;indianred2&quot; ## [5] &quot;indianred3&quot; &quot;indianred4&quot; &quot;mediumvioletred&quot; &quot;orangered&quot; ## [9] &quot;orangered1&quot; &quot;orangered2&quot; &quot;orangered3&quot; &quot;orangered4&quot; ## [13] &quot;palevioletred&quot; &quot;palevioletred1&quot; &quot;palevioletred2&quot; &quot;palevioletred3&quot; ## [17] &quot;palevioletred4&quot; &quot;red&quot; &quot;red1&quot; &quot;red2&quot; ## [21] &quot;red3&quot; &quot;red4&quot; &quot;violetred&quot; &quot;violetred1&quot; ## [25] &quot;violetred2&quot; &quot;violetred3&quot; &quot;violetred4&quot; Dicas: Cores no R Para mais opções de personalização dos gráficos de barra, veja Basic R barplot customization Folha de dicas (cheatsheet) de cores no R. Há dicas de pacotes para mais opções de cores, como o colorspace, grDevices e o colorRamps. Outra opção bem extensa sobre cores no R é Paletas de cores no R. CAPÍTULO EM CONSTRUÇÃO 7.3 O pacote Lattice CAPÍTULO EM CONSTRUÇÃO (Mas há um exemplo de uso do lattice no apêndice deste manual, sobre a CPI da Pandemia) 7.4 O pacote ggplot2 O ggplot2 é um dos pacotes da suíte de pacotes do tidyverse e segue a gramática dos gráficos de Hadley Wickham. O ggplot é um dos pacotes mais famosos do tidyverse ou mesmo do R. Instalando o pacote ggplot, caso não tenha instalado o tidyverse ou o ggplot: install.packages(&#39;ggplot2&#39;, ) Carregando o pacote ggplot: library(ggplot2) A chamada lógica da gramática dos gráficos, sob a qual o ggplot foi construído, possui 7 níveis: dados: isto é, a base de dados a ser usada. aesthetics aes(): o que você pecisa para plotar, define os eixos x e y. geoms ou geometria, geom_*(), qual tipo de gráfico queremos. Há cerca de 37 tipos disponíveis, como histograma, gráfico de linhas, dispersão, boxplot, etc. facets Estatística. Refere-e a algumas funções estatísticas. Pode-se, por exemplo, acrescentar uma reta de regressão. Coord ou Sistema de coordenadas. Temas, theme: muda o fundo do gráfico. Desse modo, vamos plotando o gráfico, nível por nível. A ordem pode mudar um pouco, alguns podem estar ausentes, mas são obrigatórios os três primeiros (dados, aes e geoms), e sem eles, não temos o gráfico. Para ilustar, vamos fazer um histograma, um gráfico simples, e que só precisa do eixo x. ggplot(data = MeuDataBase, aes(x=ColunaDoDatabase_que_será_o_eixo_x)) + geom_histogram() E agora um exemplo prático: ggplot(data = mtcars, aes(x=mpg)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Vamos fazer o mesmo barplot de aeroportos class(aeroportos) ## [1] &quot;character&quot; # aeroportos não pode ser um vetor char para usarmos no ggplot # temos de transformá-lo em um tibble aer.tibble &lt;- tibble::tibble(aer = aeroportos) ggplot(aer.tibble, aes(aer)) + geom_bar() Podemos reordenar com base não na ordem alfabética, mas em valores, crescentes ou decrescentes. Para tal, usamos a função fct_reorder() no nível aes, no seguinte modo: aes(fct_reorder( eixoX, eixoY), eixoY)). 7.4.1 ggplot: nível aes Como dito, o nível “aes” define os eixos x e y. É nele que definimos onde entram os dados. Também é aqui que podemos redefinir a ordem de apresentação dos dados. Por exemplo, no gráfico dos aeroportos, podemos reordená-los da seguinte forma: # plyr cria um data frame com a coluna &quot;x&quot; de nomes/fatores e &quot;freq&quot; com a frequência aeroportos.df &lt;- plyr::count(aeroportos) # gerando o gráfico ggplot(aeroportos.df, aes(reorder(x, -freq), freq)) + geom_col() Usamos reorder para reordenar conforme o valor de “freq”. Se quisermos em ordem crescente, usamos aes(reorder(x, freq), freq)). Se quisermos a ordem inversa, dos valores maiores aos menores, acrescentamos o sinal de - assim: aes(reorder(x, -freq), freq)). No caso abaixo, usamos um database já disponível sobre diamantes. Atribuímos, para cada coluna/variável, um aspecto no gráfico, como eixos x e y, cores diferentes e formas. ggplot(data = diamonds, aes(x=carat, y=price, # &#39;shape&#39; aqui diz que cada valor na variável &#39;cut&#39; terá uma forma (shape) diferente shape = cut, # &#39;color&#39; diz que cada valor na variável terá uma cor diferente. # &quot;clarity&quot; é o nome da variável/coluna com base na qual as cores variarão color = clarity )) + geom_point() ## Warning: Using shapes for an ordinal variable is not advised 7.4.1.1 ggplot:Argumento position Ao menos no caso de gráficos de barras, podemos especificar como gráficos com mais de uma variável terão as informações agrupadas. Primeiro, vamos criar um database com valores aleatórios meuDF &lt;- read.table(header = T, text = &#39; mes ano valor valor2 1 2019 23 14 1 2020 31 36 1 2021 28 13 2 2019 28 20 2 2020 29 21 2 2021 30 19 3 2019 19 20 3 2020 26 16 3 2021 24 24&#39;) Gerando o gráfico barras - geom_bar() - ou de colunas - geom_col() - é possível distribuir os gráficos de modos distintos. # para facilitar, vamos salvar parte do código numa variável p &lt;- ggplot(meuDF, aes(x=ano, y=valor, fill=as.factor(mes))) # Vamos agora testar diferentes posições (positions): # No caso, para cada ano, os valores dos meses # usando a posição &quot;dodge&quot;: &quot;desviar&quot; ou &quot;esquivar&quot; p + geom_col(position = &quot;dodge&quot;) # Fill que calcula percentualmente. O eixo y vai até 100% p + geom_col(position = &quot;fill&quot;) # stack vai o valor nominal p + geom_col(position = &quot;stack&quot;) Se está incomodado com a legenda “as.factor(mes)”, veremos mais adiante como personalizá-lo. 7.4.2 ggplot: Nível geom “geom”, ou “geometry” (geometria), define o tipo de gráfico que queremos, se gráfico de barras ou linhas, se boxplot, etc. Há 37 tipos diferentes. Olhando a folha de dicas (cheat sheet) do ggplot2 opção1, opção2 podemos ver as opções de gráficos (geom_alguma_coisa) quando temos uma variável somente, quando tempos duas e assim por diante. 7.4.2.1 geom_line: gráfico de linha Criando um data frame com duas variáveis numéricas para testarmos. dataf &lt;- read.table(header=TRUE, text=&#39; Letra Valor1 Valor2 A 9 4 B 14 6 C 11 8 D 12 5&#39;) Gerando um gráfico de linha simples ggplot(dataf, aes(x=Valor1, y=Valor2)) + geom_line() Na versão ampliada ggplot(dataf, aes(x=Valor1, y=Valor2)) + geom_line( # largura da linha size=1.5, # tipo de linha. pontilhada=dashed linetype = &quot;dotted&quot;, color = &quot;darkblue&quot;, # cor # adiciona uma seta. ver grid::arrow() arrow=arrow() ) + # adiciona pontos na interseção geom_point(size=3, color=&quot;red&quot;) O linetype pode ser “blank” (vazio), “solid”, “dashed” (tracejado), “dotted” (pontilhado), “dotdash” (ponto e traço), “longdash” (traços longos), “twodash”. Estes também podem ser especificados por números, sendo 1 para “blank”, 2 para “dashed” e assim por diante. Para o caso de várias linhas ggplot(meuDF, aes(x=mes, y=valor)) + geom_line(aes(group =ano, # cor conforme o ano color=as.factor(ano), # tipo de linha conforme a varivel ano linetype = as.factor(ano),), # size: espessura das linhas no gráfico size=1) Se não usarmos o as.factor em ano, os anos serão considerados contínuos e podem aparecer errados na escala, como “2019.5”. Para evitar isso, caso aconteça, usamos o as.factor() na variável. linetype = as.factor(ano), dentro de aes do geom_line, para gerar o gráfico como queremos. Forçamos a variável ano em as.factor() pois ao tentar rodar, vimos na mensagem de erro que estava como variável contínua, o que não funciona para este tipo de gráfico. Forçamos então a se tornar discreta com o comando as.factor e booom! funcionou. Mais à frente veremos opções de configurações de cores mais avançadas. Vejamos agora os níveis opcionais do ggplot. Não há nenhuma ordem certa entre eles. 7.4.2.2 geom_point: gráfico de dispersão Para um gráfico de dispersão precisamos de duas variáveis contínuas. No DF que criamos com valores aleatórios, temos as colunas “valor” e “valor2”. meuDF |&gt; ggplot(aes(x=valor, y=valor2)) + geom_point() 7.4.3 ggplot: nível facet_ Caso queiramos quebrar as informações em diferentes gráficos, usamos facet. Podemos usar facet_grid ou facet_wrap (este inverte os eixos x e y) ver “facetting” no sheet cheat. # salvando tudo em uma variável para facilitar o uso g &lt;- ggplot(data=diamonds, aes(x=carat, y=price)) +geom_point() g + facet_grid(.~cut) # se quiser compará-los na horizontal: g + facet_grid(cut~.) # cria um grid entre cut e clarity g + facet_grid(cut~clarity) Dica ggplot: facet_ ver seção Facets (ggplot2) do manual Cookbook R 7.4.4 ggplot: nível stat O nível “stat” refere-se a “estatística”. Pode-se acrescentar a linha de regressão em um gráfico de dispersão, por exemplo. Há opções de usar funções como lm (linear models, usado em modelos de regressão), glm, gam, loss, rlm. meuDF |&gt; ggplot(aes(x=valor, y=valor2)) + geom_point() + geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Se quisermos a linha da regressão linear, usamos o argumento method = \"lm\", lm significando “linear model”: meuDF |&gt; ggplot(aes(x=valor, y=valor2)) + geom_point() + geom_smooth(method = &quot;lm&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; O sombreamento ao redor da linha indica o erro padrão. Caso opte por retirá-la, acrescente o argumento se = FALSE ou se = 0. Dica: Demystifying stat_ layers in ggplot2 7.4.5 ggplot: nível sistema de coordenadas Caso queira limitar os dados que aparecem no gráfico, é possível com xlim e ylim(). Pode-se mudar a proporção, ampliando ou reduzindo os eixos através do ratio, como em coord_fixed(ratio=4) Para rotacionar o gráfico em 90º, acrescente o parâmetro + coord_flip(). 7.4.6 Cores no ggplot É possível utilizar paletas de cores diferentes do padrão. Um modo é usar o pacote RColorBrewer e o comando scale_fill_brewer() Gerando o gráfico usando apenas a coluna Valor1 e a letra ggplot(dataf, aes(x=Letra, y=Valor1, fill=Letra)) + geom_bar(stat=&quot;identity&quot;) + scale_fill_brewer(palette= &quot;BuGn&quot;) Há diversas outras paletas de cores. Há cores divergentes, como como BrBG, PiYG, PRGn, PuOr, RdBu, RdGy, RdYlBu, RdYlGn e Spectral; há sequência de cores qualitativas, como Accent, Dark2, Paired, Pastel1, Pastel2, Set1, Set2, Set3 Sequenciais como: Blues, BuGn, BuPu, GnBu, Greens, Greys, Oranges, OrRd, PuBu, PuBuGn, PuRd, Purples, RdPu, Reds, YlGn, YlGnBu, YlOrBr e YlOrRd. Fonte: cookbook-r.com Há ainda outras escalas de cores, como scale_alpha(), scale_colour_continuous(), scale_colour_gradient(), scale_colour_grey(), scale_colour_hue(), scale_colour_steps(), scale_colour_viridis_d(). 7.4.6.1 Cores definidas manualmente: scale_fill_manual Para usar sequência de cores personalizadas usa-se comando é scale_fill_manual e uma sequência de nomes de cores - como “green”, “blue”. Para ver a lista completa, digite colors() no console - ou use valores hexadecimais das cores. Cores = c(&quot;yellowgreen&quot;, &quot;#66CC99&quot;, &quot;#3CB371&quot;, &quot;seagreen4&quot;) ggplot(dataf, aes(x=Letra, y=Valor1, fill=Letra)) + geom_bar(stat=&quot;identity&quot;) + scale_fill_manual(values=Cores) 7.4.6.2 Cores contínuas Primeiro criando um data frame com valores aleatórios. df &lt;- read.table(header=TRUE, text=&#39; valorX valorY 3 3.4 2 4 2.5 5 1 2.3 3.5 5.4 1.7 2.7 2.3 2.6 2.7 3.1&#39;) Podemos acrescentar gradientes diferentes, como o comando rainbow(). O valor dentro dele refere-se ao número de cores dentro do espectro do arco-íris ggplot(df, aes(x=valorX, y=valorY, colour=valorX)) + geom_point(stat=&quot;identity&quot;) + scale_colour_gradientn(colours=rainbow(4)) 7.4.7 Anotações no ggplot: annotate() Caso queira adicionar anotação em um gráfico, a função annotate() permite adicionar comentários, destaques, linhas, etc. Em sua versão mínima para adicionar comentário de texto, precisamos dos argumentos: annotate(geom, x = valorX, y = valorY, label = \"texto\". O argumento geom da função annotate aceita os seguintes parâmetros: \"text\": para o texto sem caixa \"label\": para texto dentro de uma caixa \"segment\": para fazer um traço (parâmetros exigidos: x, xend, y, yend) \"pointrange\": para um traço com ponto (parâmetros exigidos: x, y, ymin, ymax) \"rect\": para destacar uma região com um retângulo. (parâmetros exigidos: xmin, xmax ymin, ymax) Vamos gerar um gráfico e salvá-lo como o objeto “grafico” grafico &lt;- meuDF |&gt; ggplot(aes(x=valor, y=valor2)) + geom_point() grafico grafico + annotate(geom = &quot;label&quot;, # ou &quot;text&quot;, caso não queira a caixa ao redor do texto x = 24, y = 25, # posições no eixo cartesiano colour = &quot;blue&quot;, size = 2, # tamanho da letra label = &quot;texto&quot;) A função annotate também permite destacar região no gráfico grafico + annotate(&quot;rect&quot;, # posições no eixo cartesiano: xmin = 25, xmax = 28.5, ymin = 15, ymax = 22, # colour=&quot;blue&quot;, # cor de contorno da caixa alpha = .2 # nível de transparência ) PARTE SOBRE GGPLOT AINDA EM EXPANSÃO 7.4.8 Dicas ggplot Dicas Quer fazer gráficos no R sem programar? Tente o pacote esquisse. https://dreamrs.github.io/esquisse/articles/get-started.html folha de dicas/cheat sheet ggplot2: opção1, opção2, com resumo dos comandos do ggplot. Para entender cada elemento do gráfico ggplot, como legenda, titulo, etc. de Claragrannel Cheatsheet de Christian Bukhard Cheatsheets de Interactive web visualizations for R com dicas do que fazer diversos pacotes, como DiagrammeR, Leflet, Dygraphs, ggiraph, Plotly, etc. Livro online gratuito em português Introdução a R para Visualização e Apresentação de Dados com geração automática de relatórios, mapas, e diferentes gráficos com o ggplot2. Saindo do básico no ggplot Exemplos de gráficos variados feitos com ggplot e com tutoriais ggplotr. Bom para sair do básico. É possível personalizar mais ainda o texto do ggplot. Para isso podemos usar o pacote ggtext, que nos permite usar com texto em markdown e html. https://github.com/wilkelab/ggtext https://www.r-bloggers.com/2020/06/ggplot2-text-customization-with-ggtext-data-visualization-in-r/ https://wilkelab.org/ggtext/articles/introduction.html Para gráficos animados Pacote tourr, para visualizar dados multivariados. Aqui um exemplo do tourr em ação. 7.4.9 Dicas de extensões e outros pacotes gráficos Dica para tirar dúvidas com o GGplot O pacote ggx permite digitar sua dúvida sobre o ggplot e este retornar a solução. Não faz milagres, mas pode ajudar. Para utilizá-lo, após instalá-lo através do install.packages('ggx'): library(ggx) # carregando o pacote gghelp(&quot;flip x and y axis&quot;) ## coord_flip() gghelp(&quot;label in darkblue&quot;) ## theme(axis.title.x=element_text(color=&#39;darkblue&#39;)) gghelp(&quot;rotate x-axis label 45 degree&quot;) ## theme(axis.text.x = element_text(angle = 45)) Dicas O pacote patchwork torna fácil fazer gráficos múltiplos. Há um exemplo de seu uso no apêndice sobre CPI da Pandemia. psych::pairs.panels() apresenta correlação de Pearson, histograma e regressão O ggplot possui diversos plugins de extensão, como: ggaly extensão do ggplot. ggtext: para diversas otimizações nos textos, como múltiplas cores e a possibilidade de usar markups como markdown ou LaTex nos títulos dos gráficos. ggpattern: para usar hachuras ou texturas nos gráficos gghighlight: para realçar algo no gráfico. gganimate: para fazer gráficos animados ggradar para construir gráficos de radar Há esta lista mais completa, com cerca de 111 extensões registradas para o ggplot, com a curadoria do pessoal envolvido no tidyverse. Há dicas de como fazer um pacote para o ggplot. 7.5 Gráficos de visualização textual Esta seção pretende mostar alguns gráficos mais peculiares da análise textual. Alguns exemplos de usos de gráficos ou indicação para seções posteriores, como nuvens de palavras, dedrogramas, pyramid plot, etc. Seção sobre visualização textual em expansão O pacote ggpage permite criar visualizações que destaca de modo panorâmico a incidência de certas palavras no texto. library(ggpage) library(dplyr) str(tinderbox) ## tibble [211 × 2] (S3: tbl_df/tbl/data.frame) ## $ text: chr [1:211] &quot;A soldier came marching along the high road: \\&quot;Left, right - left, right.\\&quot; He&quot; &quot;had his knapsack on his back, and a sword at his side; he had been to the wars,&quot; &quot;and was now returning home. As he walked on, he met a very frightful-looking old&quot; &quot;witch in the road. Her under-lip hung quite down on her breast, and she stopped&quot; ... ## $ book: chr [1:211] &quot;The tinder-box&quot; &quot;The tinder-box&quot; &quot;The tinder-box&quot; &quot;The tinder-box&quot; ... ggpage_quick(tinderbox) E usando um exemplo o próprio pacote: tinderbox %&gt;% ggpage_build() %&gt;% mutate(long_word = stringr::str_length(word) &gt; 8) %&gt;% ggpage_plot(aes(fill = long_word)) + labs(title = &quot;Palavras longas através do texto Tinder-box&quot;) + scale_fill_manual(values = c(&quot;grey70&quot;, &quot;blue&quot;), labels = c(&quot;8 ou menos&quot;, &quot;9 ou mais&quot;), name = &quot;Comprimento das palavras&quot;) Há mais opções de visualizações de exemplo na página do pacote, inclusive com animações, uso de dicionários (como em análise de sentimentos). Como exemplificado no apêndice deste manual, no exemplo da CPI da Pandemia, há também o gráfico de dispersão lexical (lexical dispertion), do pacote Quanteda, que também mostra a ocorrência de certo termo ao longo do(s) texto(s). "],["análise-textual-text-mining.html", "8 Análise Textual (text mining) 8.1 Introdução 8.2 Abordagens: saco de palavras (bag of words) e análise semântica (semantic parsing) 8.3 Abordagem Bag of words 8.4 Remoção de palavra vazia (stopwords) 8.5 Estemização (stemming) e lematização 8.6 Nuvem de palavras 8.7 Palavras em contexto (keyword-in-context KWIC)", " 8 Análise Textual (text mining) Orientações para ler este capítulo: Em vários dos códigos aqui presentes, usaremos a notação pacote::função, que dispensa carregar o pacote previamente. Apesar de desnecessária caso o pacote seja carregado anteriormente, ele facilitar saber qual função de que pacote está sendo usada, além de desambiguar, uma vez que há funções de nome idêntico em pacotes diferentes. Assim, sempre confira se o pacote utilizado no exemplo já está instalado em sua máquina. No Rstudio, basta ir à aba “packages” e fazer a busca na lupa para conferir. Se preferir usar o console do R, use o comando installed.packages()[,1] para listar todos os pacotes instalados, e grep(\"dplyr\", installed.packages()[,1], value=T) para checar se um pacote (no caso, o dplyr) está instalado. Há também uma listagem com datasets para usar na análise textual, algumas inclusive já no formato R. 8.1 Introdução A análise computacional de textos é praticamente um sinônimo de Mineração de Texto (text mining) e tem muito em comum com o campo de Processamento de Língua Natural ou Processamento de Linguagem Natural, mas não são exatamente a mesma coisa. Como vimos no capítulo sobre história da análise textual, esta existia antes antes da inteligência artifical e mesmo dos computadores. Há diversas funções nativas do R que usamos na mineração de texto/análise textual, mas também há diversas suítes de pacotes (pacotes com vários pacotes, com várias funções) focados em análise textual com diversas ferramentas, como o tidytext, quanteda (QUantitative ANalysis TExtual DAta), OpenNLP, Rweka, languageR, koRpus, RcmdrPlugin.temis, RKEA (R Keyphrase Extraction Algorithm), tm (Text Mining Package) e qdap (Quantitative Discourse Analysis Package). Estas são algumas das mais famosas suítes de pacotes, com diversas ferramentas, mas há alguns outros pacotes focados em funções mais específicas, como o pacote wordcloud, ggwordcloud (nuvem de palavras para o ggplot2, com mais opções), por exemplo. Há redundância entre estes pacotes, isto é, eles tem funções próprias muito semalhantes às funções de outros pacotes, o que não quer dizer que não existam diferenças significativas. Há também pacotes em R para análise textual em modo gráfico. Um software bem conhecido de análise textual e que possui interface gráfica é o iramuteq (Interface de R pour les Analyses Multidimensionnelles de Textes et de Questionnaires), criado em 2009 por Pierre Ratinaud. Apesar de ainda ser bastante utilizado, o Iramuteq tem diversas limitações. Vimos um pouco sobre o RCommander. Há um plugin para ele dedicado à análise textual, o RcmdrPlugin.temis. Porém, sua última atualização ocorreu em 2018. Dicas lista com diversos pacotes R, relacionados à Processamento de Linguagem Natural. Lista extensa, porém desatualizada. Grimmer e Stewart (2013, p.3) formularam quatro princípios da análise quantitativa de textos: Todos os modelos quantitativos de linguagem são falsos, mas alguns são úteis métodos quantitativos para texto amplificam recursos e aumentam humanos Não há método um método globalmente melhor para análise textual automática validar, validar, validar 8.2 Abordagens: saco de palavras (bag of words) e análise semântica (semantic parsing) Na análise textual podemos analisar levando ou não em consideração a ordem das palavras ou sua função gramatical. Se o ordenamento ou a função das palavras não é importante, e queremos saber, por exemplo, apenas a frequência de termos, então faremos uma abordagem tipo “saco de palavras” (bag of words). Se precisamos saber as classes gramaticais, então a ordem das palavras é importante. Vamos começar os exemplos com um pacote que pega dados do Google Ngram e nos retorna frequência de termos longitudinalmente, com base de dados do Google Books. 8.3 Abordagem Bag of words Na abordagem de “saco de palavras” (bag of words) a ordem dos termos não importa, bem como geralmente não importa a sua classe gramatical. 8.3.1 Frequência de palavras/termos e Ngrams Numa abordagem do tipo saco-de-palavras, a abordagem mais simples, mas sempre útil, é verificar a frequência de certos termos. Apesar de simples, análises mais sofisticadas podem começar com a análise de frequência e partir para abordagens mais sofisticadas. aprendizado instrumental de uma língua, ao identificar as palavras mais frequentes em certa área do conhecimento. detecção de língua detecção de termos mais frequentes em uma busca identificação de palavras compostas 8.3.1.1 n-gram: explicando o conceito Se partirmos do exemplo da frase “Ivo viu a uva” teremos unigram N=1 “a” “viu” “Ivo” “uva” bigrams N=2 “a uva” “Ivo viu” “viu a” trigrams N=3 “Ivo viu a” “viu a uva” ngram=4 N=4 “Ivo viu a uva” … … … Quando o N passa de 3, chamamos de ngram e seu valor. Agora um exemplo com o R. Primeiro veremos exemplos com o Google Ngram, que é mais simples, e depois montaremos nosso próprio ngram. 8.3.1.2 GoogleNgrams A Google pegou sua enorme base de dados dos milhares de livros do Google Books e extraiu os termos mais frequentes, e os colocou disponível para consulta no site Goolge Books Ngram Viewer. O Google Ngrams facilitou a busca por ngrams nesta base de dados, naquilo que chamavam de “culturonomics”. O nome não pegou, a ferramenta tem suas limitações, mas ainda assim pode ser bem útil. A base de dados possui 5.2 milhões de livros, cerca de 4% de todos os livros já publicados. Para mais informações sobre a base de dados e sobre o GoogleNgram no site. Tanto o Python (com o get-ngrams) como o R (ngramr) possuem pacotes que usam os dados do Google Ngram. Instalando o pacote ngramr install.packages(&#39;ngramr&#39;) Carregando os pacote library(ngramr) E um exemplo de uso ng &lt;- ngramr::ngram(c(&quot;Max Weber&quot;, &quot;Émile Durkheim&quot;), year_start = 1890) ggplot2::ggplot(ng, aes(x=Year, y=Frequency, colour=Phrase)) + geom_line() Um exemplo da página do ngramr no Github com mais opções, usando a função ggram() no ngramr, que pega dados do GoogleNgram e plota os dados com o ggplot2: ggram(c(&quot;monarchy&quot;, &quot;democracy&quot;), year_start = 1500, year_end = 2000, corpus = &quot;eng_gb_2012&quot;, ignore_case = TRUE, geom = &quot;area&quot;, geom_options = list(position = &quot;stack&quot;)) + labs(y = NULL) É possível mudar entre diferentes corpus, que neste caso representam as diferentes línguas, como “eng_us_2019”, “eng_gb_2019”, “chi_sim_2019”, “fre_2019”, “ger_2019”, “heb_2019”, “ger_2012”, “spa_2012”, “rus_2012”, “ita_2012”. Para ver todos os corpus disponíveis veja no site busque a sessão “Corpora”. Infelizmente, não há corpus em português no Google Ngram. classicos = c(&quot;Max Weber&quot;, &quot;Émile Durkheim&quot;, &quot;Karl Marx&quot;, &quot;Gabriel Tarde&quot;, &quot;Georg Simmel&quot;) ggram(classicos, year_start = 1980, year_end = 2000, # Para mudar lingua, mude o corpus # ignore case: se diferencia maiúsculo de minúsculo corpus = &quot;fre_2019&quot;, ignore_case = TRUE, # tipo de grafico em geom geom = &quot;line&quot;, geom_options = list()) + # labs: label do eixo y labs(y = NULL) Dicas Ngramr: Site do Books Ngram Viewer explicando seus parâmetros. PDF com a documentação do ngramr Instalação/Primeiros passos com o Ngramr na página do Github do ngramr Um projeto similar ao Google Ngram - inclusive usando parte do mesmo pessoal -, porém melhorado, é o bookworm:HalthiTrust do projeto Halthi Trust-Digital Livrary, com muito mais línguas, inclusive o português e mais opções de busca. Há uma API dedicada ao HalthiTrust, podendo baixar as bases de dados direto do R, o hathiTools 8.3.1.3 N-grams no R library(ngram) Vamos pegar um trecho de Alfred Shutz. txt=&quot;A Fenomenologia busca o início real de todo pensamento filosófico... Seu lugar é além - ou melhor, antes - de todas as distinções entre realismo e idealismo.&quot; Vamos quebrar o texto em ngrams. Geralmente usa-se valores entre 1 e 3. ng &lt;- ngram::ngram(txt, # n = valor do ngram n=3) # imprimindo o objeto que criamos, que mostra o total de ngrams ng ## An ngram object with 25 3-grams # imprimindo os ngrams gerados. ngram::get.ngrams(ng) ## [1] &quot;além - ou&quot; &quot;início real de&quot; ## [3] &quot;- de todas&quot; &quot;antes - de&quot; ## [5] &quot;busca o início&quot; &quot;entre realismo e&quot; ## [7] &quot;o início real&quot; &quot;A Fenomenologia busca&quot; ## [9] &quot;distinções entre realismo&quot; &quot;lugar é além&quot; ## [11] &quot;as distinções entre&quot; &quot;de todo pensamento&quot; ## [13] &quot;filosófico... Seu lugar&quot; &quot;é além -&quot; ## [15] &quot;- ou melhor,&quot; &quot;de todas as&quot; ## [17] &quot;Fenomenologia busca o&quot; &quot;melhor, antes -&quot; ## [19] &quot;todo pensamento filosófico...&quot; &quot;todas as distinções&quot; ## [21] &quot;ou melhor, antes&quot; &quot;realismo e idealismo.&quot; ## [23] &quot;real de todo&quot; &quot;Seu lugar é&quot; ## [25] &quot;pensamento filosófico... Seu&quot; Diversos outros pacotes::funções fazem a quebra em ngrams, como RWeka::NGramTokenizer ou o quanteda. A função de ngram do quanteda tem a vantagem de poder definir um escopo de valores de ngram de uma vez, podendo gerar unigramas, bigramas e trigramas com um só comando. Em outros pacotes isto é possível apenas com pós processamento. A quebra do texto em ngrams faz mais sentido quando, com eles, observamos os termos mais repetidos. Isso é o que vamos fazer a seguir. Num exemplo mais prático, fomos até o site gutenberg (site com vários livros gratuitos) e pegamos o link para o txt do livro “O Príncipe” de Maquiavel, em inglês. # link para o livro &quot;The Prince&quot; de Maquiavel. url.prince = &quot;https://www.gutenberg.org/files/1232/1232-0.txt&quot; # carregando o url num objeto R maquiavel &lt;- readLines(url(url.prince)) Observando a estrutura do objeto “maquiavel” que acabamos de criar: # Como o objeto importado está como um vetor com vários elementos: str(maquiavel) ## chr [1:5188] &quot;The Project Gutenberg eBook of The Prince, by Nicolo Machiavelli&quot; ... # vemos que é um vetor com 5.188 itens. Precisamos transformar estes vários vetores em um só elemento com o comando # paste0(var, collapse = &quot; &quot;) maquiavel2 &lt;- paste(maquiavel, collapse = &quot; &quot;) Por hora, usaremos o pacote tradicional ngram, escrito em C, e por isso, rápido. Podemos fazer a sumarização (summarizing) obtendo a frequência de vezes que um ngram apareceu no texto, bem como também a frequência relativa (proporcional) com a função ngram:: get.phrasetable, que retorna um data frame. Rode o ngram com diferentes valores para ver qual deles ertorna resultados mais informativos do conteúdo. prince_ngrams &lt;- ngram::get.phrasetable(ngram::ngram(maquiavel2, n = 3)) # restringindo aos trigramas mais frequentes prince_ngrams[1:16,] ## ngrams freq prop ## 1 he did not 20 0.0003770526 ## 2 it is necessary 20 0.0003770526 ## 3 in order to 19 0.0003582000 ## 4 the King of 19 0.0003582000 ## 5 Project Gutenberg-tm electronic 18 0.0003393473 ## 6 ought to be 18 0.0003393473 ## 7 in such a 18 0.0003393473 ## 8 prince ought to 18 0.0003393473 ## 9 so as to 17 0.0003204947 ## 10 that it was 15 0.0002827894 ## 11 those who have 15 0.0002827894 ## 12 if he had 15 0.0002827894 ## 13 for him to 14 0.0002639368 ## 14 the Project Gutenberg 14 0.0002639368 ## 15 such a way 14 0.0002639368 ## 16 the death of 13 0.0002450842 Dicas Sugestão de leitura JURAFSKY, Dan.; MARTIN, James H. cap.3 N-gram Language Models de Speech and Language Processing - An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition. 3ª Edição. 2020. Manual do pacote ngram 8.4 Remoção de palavra vazia (stopwords) 8.4.1 Criando lista com stopwords Ao analisarmos texto, o mais frequente são palavras bem pouco informativas, como artigos “o”, “a” “os”, “as”. Para termos uma noção melhor removemos as chamadas “stopwords”. manifesto &lt;- &quot;A História de toda a sociedade até hoje é a história da luta de classes.&quot; # criamos uma pequena lista de stopwords para nosso exemplo atual minhas_sw &lt;- c(&quot;a&quot;,&quot;o&quot;, &quot;e&quot;, &quot;da&quot;, &quot;de&quot;, &quot;do&quot;) # transformando o texto em vetor manif_vetor &lt;- manifesto %&gt;% # convertendo o texto todo para minúsculo tolower %&gt;% # quebrando o texto em vetores utilizando pontuação strsplit(., &quot;\\\\W&quot;) %&gt;% # o comando strplit retorna lista. Vamos forçar para retornar como vetor char unlist manif_vetor ## [1] &quot;a&quot; &quot;história&quot; &quot;de&quot; &quot;toda&quot; &quot;a&quot; &quot;sociedade&quot; ## [7] &quot;até&quot; &quot;hoje&quot; &quot;é&quot; &quot;a&quot; &quot;história&quot; &quot;da&quot; ## [13] &quot;luta&quot; &quot;de&quot; &quot;classes&quot; Quebramos a string com strplit usando todo não letra e não número, com \\\\W (ver regex). Por se tratar de vetor, podemos usar comando tradicionais, com os operadores ! que indica negação, e %in% que checa se algo está contido em um vetor. manif_vetor[!(manif_vetor) %in% minhas_sw] ## [1] &quot;história&quot; &quot;toda&quot; &quot;sociedade&quot; &quot;até&quot; &quot;hoje&quot; &quot;é&quot; ## [7] &quot;história&quot; &quot;luta&quot; &quot;classes&quot; Explicando: (manif_vetor) %in% minhas_sw checa se itens de “manif_vetor” estão contidos em “minhas_sw”. Retorna um booleanoo de “TRUE” e “FALSE”. ! inverte o comando anterior, checando agora quais itens de “manif_vetor” não estão contidos em “minhas_sw”, também retornando um vetor com booleanos de “TRUE” e “FALSE”. Para obter os valores (as palavras), jogamos esta fórmula anterior dentro de “manif_vetor[fórmula_anterior]”. As stopwords costumam ser as mesmas. E se já houvesse uma lista pronta? Existe. É possível encontrar listas prontas na internet, mas diversas funções no R já incluem em si tais listas. Para ver a lista padrão no R, use: library(tm, quietly = T) # pegando apenas as primeiras 20 stopwords em inglês tm::stopwords(&quot;en&quot;) %&gt;% head(.,20) ## [1] &quot;i&quot; &quot;me&quot; &quot;my&quot; &quot;myself&quot; &quot;we&quot; ## [6] &quot;our&quot; &quot;ours&quot; &quot;ourselves&quot; &quot;you&quot; &quot;your&quot; ## [11] &quot;yours&quot; &quot;yourself&quot; &quot;yourselves&quot; &quot;he&quot; &quot;him&quot; ## [16] &quot;his&quot; &quot;himself&quot; &quot;she&quot; &quot;her&quot; &quot;hers&quot; # vendo a lista em português tm::stopwords(&quot;pt&quot;) ## [1] &quot;de&quot; &quot;a&quot; &quot;o&quot; &quot;que&quot; &quot;e&quot; ## [6] &quot;do&quot; &quot;da&quot; &quot;em&quot; &quot;um&quot; &quot;para&quot; ## [11] &quot;com&quot; &quot;não&quot; &quot;uma&quot; &quot;os&quot; &quot;no&quot; ## [16] &quot;se&quot; &quot;na&quot; &quot;por&quot; &quot;mais&quot; &quot;as&quot; ## [21] &quot;dos&quot; &quot;como&quot; &quot;mas&quot; &quot;ao&quot; &quot;ele&quot; ## [26] &quot;das&quot; &quot;à&quot; &quot;seu&quot; &quot;sua&quot; &quot;ou&quot; ## [31] &quot;quando&quot; &quot;muito&quot; &quot;nos&quot; &quot;já&quot; &quot;eu&quot; ## [36] &quot;também&quot; &quot;só&quot; &quot;pelo&quot; &quot;pela&quot; &quot;até&quot; ## [41] &quot;isso&quot; &quot;ela&quot; &quot;entre&quot; &quot;depois&quot; &quot;sem&quot; ## [46] &quot;mesmo&quot; &quot;aos&quot; &quot;seus&quot; &quot;quem&quot; &quot;nas&quot; ## [51] &quot;me&quot; &quot;esse&quot; &quot;eles&quot; &quot;você&quot; &quot;essa&quot; ## [56] &quot;num&quot; &quot;nem&quot; &quot;suas&quot; &quot;meu&quot; &quot;às&quot; ## [61] &quot;minha&quot; &quot;numa&quot; &quot;pelos&quot; &quot;elas&quot; &quot;qual&quot; ## [66] &quot;nós&quot; &quot;lhe&quot; &quot;deles&quot; &quot;essas&quot; &quot;esses&quot; ## [71] &quot;pelas&quot; &quot;este&quot; &quot;dele&quot; &quot;tu&quot; &quot;te&quot; ## [76] &quot;vocês&quot; &quot;vos&quot; &quot;lhes&quot; &quot;meus&quot; &quot;minhas&quot; ## [81] &quot;teu&quot; &quot;tua&quot; &quot;teus&quot; &quot;tuas&quot; &quot;nosso&quot; ## [86] &quot;nossa&quot; &quot;nossos&quot; &quot;nossas&quot; &quot;dela&quot; &quot;delas&quot; ## [91] &quot;esta&quot; &quot;estes&quot; &quot;estas&quot; &quot;aquele&quot; &quot;aquela&quot; ## [96] &quot;aqueles&quot; &quot;aquelas&quot; &quot;isto&quot; &quot;aquilo&quot; &quot;estou&quot; ## [101] &quot;está&quot; &quot;estamos&quot; &quot;estão&quot; &quot;estive&quot; &quot;esteve&quot; ## [106] &quot;estivemos&quot; &quot;estiveram&quot; &quot;estava&quot; &quot;estávamos&quot; &quot;estavam&quot; ## [111] &quot;estivera&quot; &quot;estivéramos&quot; &quot;esteja&quot; &quot;estejamos&quot; &quot;estejam&quot; ## [116] &quot;estivesse&quot; &quot;estivéssemos&quot; &quot;estivessem&quot; &quot;estiver&quot; &quot;estivermos&quot; ## [121] &quot;estiverem&quot; &quot;hei&quot; &quot;há&quot; &quot;havemos&quot; &quot;hão&quot; ## [126] &quot;houve&quot; &quot;houvemos&quot; &quot;houveram&quot; &quot;houvera&quot; &quot;houvéramos&quot; ## [131] &quot;haja&quot; &quot;hajamos&quot; &quot;hajam&quot; &quot;houvesse&quot; &quot;houvéssemos&quot; ## [136] &quot;houvessem&quot; &quot;houver&quot; &quot;houvermos&quot; &quot;houverem&quot; &quot;houverei&quot; ## [141] &quot;houverá&quot; &quot;houveremos&quot; &quot;houverão&quot; &quot;houveria&quot; &quot;houveríamos&quot; ## [146] &quot;houveriam&quot; &quot;sou&quot; &quot;somos&quot; &quot;são&quot; &quot;era&quot; ## [151] &quot;éramos&quot; &quot;eram&quot; &quot;fui&quot; &quot;foi&quot; &quot;fomos&quot; ## [156] &quot;foram&quot; &quot;fora&quot; &quot;fôramos&quot; &quot;seja&quot; &quot;sejamos&quot; ## [161] &quot;sejam&quot; &quot;fosse&quot; &quot;fôssemos&quot; &quot;fossem&quot; &quot;for&quot; ## [166] &quot;formos&quot; &quot;forem&quot; &quot;serei&quot; &quot;será&quot; &quot;seremos&quot; ## [171] &quot;serão&quot; &quot;seria&quot; &quot;seríamos&quot; &quot;seriam&quot; &quot;tenho&quot; ## [176] &quot;tem&quot; &quot;temos&quot; &quot;tém&quot; &quot;tinha&quot; &quot;tínhamos&quot; ## [181] &quot;tinham&quot; &quot;tive&quot; &quot;teve&quot; &quot;tivemos&quot; &quot;tiveram&quot; ## [186] &quot;tivera&quot; &quot;tivéramos&quot; &quot;tenha&quot; &quot;tenhamos&quot; &quot;tenham&quot; ## [191] &quot;tivesse&quot; &quot;tivéssemos&quot; &quot;tivessem&quot; &quot;tiver&quot; &quot;tivermos&quot; ## [196] &quot;tiverem&quot; &quot;terei&quot; &quot;terá&quot; &quot;teremos&quot; &quot;terão&quot; ## [201] &quot;teria&quot; &quot;teríamos&quot; &quot;teriam&quot; Há também o pacote stopwords, que no momento possui, para o português, de fontes como snowball, nltk e stopwords-iso. Para instalar, basta rodar o já conhecido install.packages(\"stopwords\"). # vendo as linguagens disponiveis stopwords::stopwords_getlanguages(&quot;snowball&quot;) ## [1] &quot;da&quot; &quot;de&quot; &quot;en&quot; &quot;es&quot; &quot;fi&quot; &quot;fr&quot; &quot;hu&quot; &quot;ir&quot; &quot;it&quot; &quot;nl&quot; &quot;no&quot; &quot;pt&quot; &quot;ro&quot; &quot;ru&quot; &quot;sv&quot; # vendo as fontes de stopwords disponíveis stopwords::stopwords_getsources() ## [1] &quot;snowball&quot; &quot;stopwords-iso&quot; &quot;misc&quot; &quot;smart&quot; ## [5] &quot;marimo&quot; &quot;ancient&quot; &quot;nltk&quot; &quot;perseus&quot; # vendo um extrato das stopwords em português, fonte snowball head(stopwords::stopwords(&quot;pt&quot;, source = &quot;snowball&quot;), 20) ## [1] &quot;de&quot; &quot;a&quot; &quot;o&quot; &quot;que&quot; &quot;e&quot; &quot;do&quot; &quot;da&quot; &quot;em&quot; &quot;um&quot; &quot;para&quot; ## [11] &quot;com&quot; &quot;não&quot; &quot;uma&quot; &quot;os&quot; &quot;no&quot; &quot;se&quot; &quot;na&quot; &quot;por&quot; &quot;mais&quot; &quot;as&quot; # vendo um extrato das stopwords em português, fonte stopwords-iso head(stopwords::stopwords(&quot;pt&quot;, source = &quot;stopwords-iso&quot;), 20) ## [1] &quot;a&quot; &quot;acerca&quot; &quot;adeus&quot; &quot;agora&quot; &quot;ainda&quot; &quot;alem&quot; &quot;algmas&quot; ## [8] &quot;algo&quot; &quot;algumas&quot; &quot;alguns&quot; &quot;ali&quot; &quot;além&quot; &quot;ambas&quot; &quot;ambos&quot; ## [15] &quot;ano&quot; &quot;anos&quot; &quot;antes&quot; &quot;ao&quot; &quot;aonde&quot; &quot;aos&quot; E comparando o número de elementos das diferentes fontes de stopwords stopwords::stopwords(&quot;pt&quot;, source = &quot;stopwords-iso&quot;) %&gt;% length() ## [1] 560 stopwords::stopwords(&quot;pt&quot;, source = &quot;snowball&quot;) %&gt;% length() ## [1] 203 stopwords::stopwords(&quot;pt&quot;, source = &quot;nltk&quot;) %&gt;% length() ## [1] 204 Para aplicar esta função no nosso texto e retirar as stopwords, podemos usar diversas funções (lista mais abaixo). Diferentes pacotes de análise textual possuem diferentes formas de retirar as stopwords. Para adicionar novas palavras à lista de stopwords que vamos usar no momento, cria-se um novo vetor - chamamos aqui de “novas_stopwords” - com as novas palavras a serem retiradas, e em seguida o stopwords() # checando o tamanho do vetor stopwords disponível length(tm::stopwords(&quot;pt&quot;)) ## [1] 203 # criando novo vetor com mais palavras novas_stopwords &lt;- c(&quot;então&quot;, &quot;portanto&quot;, tm::stopwords(&quot;pt&quot;)) # checando se nossos termos foram incluídos length(novas_stopwords) ## [1] 205 Ou para facilitar a inclusão de novos termos, podemos fazer do seguinte modo: # Separamos nossos termos apenas por espaço novas &lt;- &quot;então portanto&quot; # quebrando o char em vetor de termos # ao invés de usarmos unlist, podemos usar [[1]] strsplit(novas, &quot; &quot;)[[1]] ## [1] &quot;então&quot; &quot;portanto&quot; # colocando os novos termos em um novo vetor novas_stopwords &lt;- c(strsplit(novas, &quot; &quot;)[[1]], tm::stopwords(&quot;pt&quot;)) length(novas_stopwords) ## [1] 205 Deste modo, podemos ir acrescentando mais facilmente novos termos à nossa lista de stopwords. Há outras funções com listas de stopwords, como: - qdap::stopwords e vários outras listas de stopwords no pacote - lexicon. - corpus::stopwords_pt (há stopwords em 13 línguas diferentes) 8.4.1.1 Removendo as stopwords Para remover stopwords, temos diferentes pacotes com diferentes funções, como por exemplo: dplyr::anti_join(stopwords(pt)), tm::tm_map(corpus, removeWords, stopwords(\"english\"), tm::removeWords(texto, stopwords(\"pt\")), [tau::remove_stopwords()](https://cran.r-project.org/web//packages/tau/tau.pdf) e qdap::rm_stopwords(). O dplyr possui ainda a função semi_join() que mostra termos em comum, que se repetem em x e y. Já anti_join faz o oposto, mostra todas linhas de ‘x’ sem match em ‘y’, e é com ela que retiramos as stopwords. 8.4.1.1.1 Modo 1: pacote base Se não quisermos carregar nenhum pacote, um modo possível de retirar stopwords de um vetorde palavras com o vetor de setopwords se dá com os operadores ! e %in% manif_vetor[!(manif_vetor) %in% minhas_sw] ## [1] &quot;história&quot; &quot;toda&quot; &quot;sociedade&quot; &quot;até&quot; &quot;hoje&quot; &quot;é&quot; ## [7] &quot;história&quot; &quot;luta&quot; &quot;classes&quot; 8.4.1.1.2 Modo 2: dplyr Outro modo de retirar stopwords usando o tidytext: 8.5 Estemização (stemming) e lematização Imagine que tenha palavras como “escrever, escrevi, escreveu” e você está interessado nos verbos mais frequentes. É útil considerar estas variações do verbo como uma palavra só. Isso pode ser obtido de dois modos, através da stemização e por lematização. Em ambos o objetivo é o mesmo, reduzir a flexão a uma base comum ou raiz. A estemização funciona cortando um pedaço do final da palavra, ao passo que lematização reduz as variações à raiz, podendo inclusive pegar verbos irregulares. Por que então usar estemização? A construção de lematizadores é mais complicada, além de ser um processo mais demorado e que consome mais recursos. 8.5.1 Estemização A estemização pode ser feita com o pacote SnowballC,que é baseado no snowball, que continua sendo desenvolvido no GitHub do projeto. Desenvolvida originalmente por Martin Porter, seu nome é um tributo ao SNOBOL, uma linguagem dos anos 1960 que lidava com strings. Para entender o algoritmo de estemização em português e alguns exemplos, veja aqui. Carregando o pacote SnowballC library(SnowballC) Exemplo de estemização # Vendo as línguas disponíveis SnowballC::getStemLanguages() ## [1] &quot;arabic&quot; &quot;basque&quot; &quot;catalan&quot; &quot;danish&quot; &quot;dutch&quot; ## [6] &quot;english&quot; &quot;finnish&quot; &quot;french&quot; &quot;german&quot; &quot;greek&quot; ## [11] &quot;hindi&quot; &quot;hungarian&quot; &quot;indonesian&quot; &quot;irish&quot; &quot;italian&quot; ## [16] &quot;lithuanian&quot; &quot;nepali&quot; &quot;norwegian&quot; &quot;porter&quot; &quot;portuguese&quot; ## [21] &quot;romanian&quot; &quot;russian&quot; &quot;spanish&quot; &quot;swedish&quot; &quot;tamil&quot; ## [26] &quot;turkish&quot; # criando um vetor de palavras palavras= c(&quot;plantar&quot;, &quot;plantei&quot;, &quot;ajudou&quot;, &quot;ajudarás&quot;, &quot;comer&quot;, &quot;comendo&quot;) # testando a stemização SnowballC::wordStem(palavras, language = &quot;portuguese&quot;) ## [1] &quot;plant&quot; &quot;plant&quot; &quot;ajud&quot; &quot;ajud&quot; &quot;com&quot; &quot;com&quot; Vamos testar com outras palavras: palavras= c(&quot;estou&quot;, &quot;está&quot;, &quot;estamos&quot;, &quot;sou&quot;, &quot;és&quot;) SnowballC::wordStem(palavras, language = &quot;portuguese&quot;) ## [1] &quot;estou&quot; &quot;está&quot; &quot;estam&quot; &quot;sou&quot; &quot;és&quot; Repare que verbos irregulares como “ser” e “estar” não funcionaram muito bem. Uma alternativa é usar a lematização ao invés da stemização. 8.5.2 Lematização EM CONSTRUÇÃO A lematização reduz variações/inflexões de uma palavra, de modo que sejam analisados como um termo único. A lematização chega à forma raiz da palavra, ainda que sejam, por exemplo, verbos irregulares. Funções como textstem::lemmatize_words() , koRpus::treetag, SnowballC::wordStem nlp_lemmatizer, e udpipe fazem este trabalho de lematização. goffman_stigma &lt;- c(&quot;The central feature of the stigmatized individual&#39;s situation in life can now be stated.&quot;, &quot;It is a question of what is often, if vaguely, called `acceptance&#39;.&quot;, &quot;How does the stigmatized person respond to his situation?&quot;) ## Default lexicon::hash_lemmas dictionary textstem::lemmatize_strings(goffman_stigma) ## [1] &quot;The central feature of the stigmatize individual&#39;s situation in life can now be state.&quot; ## [2] &quot;It be a question of what be often, if vague, call `acceptance&#39;.&quot; ## [3] &quot;How do the stigmatize person respond to his situation?&quot; 8.6 Nuvem de palavras Vários pacotes fazem as chamadas nuvem de palavras no R. Um deles é o wordcloud, que além de fazer nuvens de palavras, também é capaz de fazê-lo comparando documentos, Em sua forma mais simples texto_tocqueville &lt;- &quot;Em nosso tempo, a liberdade de associação tornou-se uma garantia necessária contra a tirania da maioria. Nos Estados Unidos, quando uma vez um partido se toma dominante. todo o poder público passa para as suas mãos; seus amigos particulares ocupam todos os empregos e dispõem de todas as forças organizadas. Como os homens mais distintos do partido contrário não podem atravessar a barreira que os separa do poder, é preciso que possam se estabelecer fora; é preciso que a minoria oponha sua força moral inteira ao poderio material que a oprime. Opõe-se, pois, um perigo a um perigo mais temível. A onipotência da maioria parece-me um risco tão grande para as repúblicas americanas que o meio perigoso que se usa para limitá-la parece-me, ainda assim, um bem. Exprimirei aqui um pensamento que lembrará o que disse em outra parte a respeito das liberdades comunais: não há país em que as associações sejam mais necessárias, para impedir o despotismo dos partidos ou a arbitrariedade do príncipe, do que aquele em que o estado social é democrático. Nas nações aristocráticas, os corpos secundários formam associações naturais que detêm os abusos de poder. Nos países em que semelhantes associações não existem, se os particulares não podem criar artificial e momentaneamente alguma coisa que se lhes assemelhe, não percebo mais nenhum dique contra nenhuma sorte de tirania, e um grande povo pode ser oprimido impunemente por um punhado de facciosos ou por um homem. ... Não podemos dissimular que a liberdade ilimitada de associação, em matéria política, é, de todas as liberdades, a última que um povo pode suportar. Se ela não o faz cair na anarquia, o faz tocá-la por assim dizer a cada instante. Essa liberdade, tão perigosa, oferece porém num ponto algumas garantias: nos países em que as associações são livres, as sociedades secretas são desconhecidas. Na América, há facciosos, mas não conspiradores.&quot; E para criar uma nuvem de palavras simples, basta usar o comando: wordcloud::wordcloud(texto_tocqueville) ## Warning in tm_map.SimpleCorpus(corpus, tm::removePunctuation): transformation ## drops documents ## Warning in tm_map.SimpleCorpus(corpus, function(x) tm::removeWords(x, ## tm::stopwords())): transformation drops documents E para usar num modo mais detalhado: wordcloud::wordcloud(texto_tocqueville, # se o input para esta função contém as frequências de palavras, o item abaixo deve ser descomentado # freq, # vetor com dois termos indicado o espectro de tamanho das palavras scale=c(2,.8), # número mínimo de repetições que uma palavra tem de ter para entrar no gráfico min.freq = 2, # número máximo de palavras a ser plotado na nuvem de palavras max.words = 70, # Não plotar palavras em ordem aleatória, mas sim em ordem decrescente random.order = FALSE, # cores, do menos frequente ao mais frequente colors = c(&quot;royalblue&quot;,&quot;blue&quot;, &quot;darkblue&quot;)) ## Warning in tm_map.SimpleCorpus(corpus, tm::removePunctuation): transformation ## drops documents ## Warning in tm_map.SimpleCorpus(corpus, function(x) tm::removeWords(x, ## tm::stopwords())): transformation drops documents Esta função aceita como entrada (input) tanto o texto puro como a tabela de frequência (e aí, deve-se usar o parâmetro freq). Para textos pequenos, como é o caso aqui, não há problema em usar texto diretamente na função, mas partir do momento que textos se tornam grandes, o ideal é fazer a contagem previamente e mandar a tabela para a função wordcloud. Além disso, fazer a contagem previamente permite fazer uma série de pré-processamentos no texto, como retirada de palavras pouco instrutivas (as stopwords), passar todas as palavras para minúsculo, etc. Vemos que algumas palavras frequentes não nos dizem muita coisa, como “que”, “nos”, “para”. Como queremos apreender algo do sentido do texto com a nuvem de palavras, seria interessante remover tais termos pouco significativos, as chamadas “palavras vazias” ou “stopwords”. Usamos aí um pequeno texto. Caso tivéssemos um texto grande, este procedimento de mandar o texto direto para a função wordcloud::wordcloud não seria indicado. Além disso, precisamos fazer alguns pré-processamentos no texto para obter algo mais significativo. Tocq &lt;- texto_tocqueville |&gt; as_tibble(falas = texto_tocqueville) # tudo.tokens &lt;- tudo %&gt;% tidytext::unnest_tokens(word, falas) # tudo.tokens &lt;- tidytext::unnest_tokens(tudo.df , falas) Tocq.tokens &lt;- Tocq %&gt;% tidytext::unnest_tokens(word, value) SW &lt;- c(stopwords::stopwords(&#39;pt&#39;), &#39;é&#39;, &#39;aqui&#39;, &#39;então&#39;, &#39;porque&#39;) # retirando as stopwords Tocq.semSW &lt;- Tocq.tokens$word[!(tudo.tokens$word) %in% SW] # Se quisermos observar algumas das palavras mais frequentes, sem as stopwords head(Tocq.semSW, 40) ## [1] &quot;tempo&quot; &quot;liberdade&quot; &quot;associação&quot; &quot;tornou&quot; &quot;garantia&quot; ## [6] &quot;necessária&quot; &quot;contra&quot; &quot;tirania&quot; &quot;maioria&quot; &quot;estados&quot; ## [11] &quot;unidos&quot; &quot;vez&quot; &quot;partido&quot; &quot;toma&quot; &quot;dominante&quot; ## [16] &quot;todo&quot; &quot;poder&quot; &quot;público&quot; &quot;passa&quot; &quot;mãos&quot; ## [21] &quot;amigos&quot; &quot;particulares&quot; &quot;ocupam&quot; &quot;todos&quot; &quot;empregos&quot; ## [26] &quot;dispõem&quot; &quot;todas&quot; &quot;forças&quot; &quot;organizadas&quot; &quot;homens&quot; ## [31] &quot;distintos&quot; &quot;partido&quot; &quot;contrário&quot; &quot;podem&quot; &quot;atravessar&quot; ## [36] &quot;barreira&quot; &quot;separa&quot; &quot;poder&quot; &quot;preciso&quot; &quot;possam&quot; Tocq.tibble &lt;- tibble(words = Tocq.semSW ) words.counts &lt;- Tocq.tibble %&gt;% dplyr::count(words, sort = TRUE) head(words.counts$words, 40) # Observando as palavras mais frequentes ## [1] &quot;associações&quot; &quot;liberdade&quot; &quot;poder&quot; &quot;assim&quot; ## [5] &quot;associação&quot; &quot;contra&quot; &quot;facciosos&quot; &quot;faz&quot; ## [9] &quot;grande&quot; &quot;la&quot; &quot;liberdades&quot; &quot;maioria&quot; ## [13] &quot;países&quot; &quot;parece&quot; &quot;particulares&quot; &quot;partido&quot; ## [17] &quot;perigo&quot; &quot;pode&quot; &quot;podem&quot; &quot;povo&quot; ## [21] &quot;preciso&quot; &quot;tão&quot; &quot;tirania&quot; &quot;todas&quot; ## [25] &quot;abusos&quot; &quot;ainda&quot; &quot;alguma&quot; &quot;algumas&quot; ## [29] &quot;américa&quot; &quot;americanas&quot; &quot;amigos&quot; &quot;anarquia&quot; ## [33] &quot;arbitrariedade&quot; &quot;aristocráticas&quot; &quot;artificial&quot; &quot;assemelhe&quot; ## [37] &quot;atravessar&quot; &quot;barreira&quot; &quot;bem&quot; &quot;cada&quot; # Para não poluir demais a nuvem de palavras, restringir às 150 mais frequentes: words.counts.head &lt;- head(words.counts, 150) Gerando a nuvem de palavras: wordcloud::wordcloud(words.counts.head$words, # se o input para esta função contém as frequências de palavras, o item abaixo deve ser descomentado freq = words.counts.head$n, # vetor com dois termos indicado o espectro de tamanho das palavras scale = c(3,1), min.freq = 2, # Frequência mínima de termos que serão usados # cores, do menos frequente ao mais frequente colors = c(&quot;royalblue&quot;,&quot;blue&quot;, &quot;darkblue&quot;, &quot;black&quot;) ) 8.7 Palavras em contexto (keyword-in-context KWIC) Podemos ver como certas palavras são usadas em diversas frases no texto para ter uma ideia melhor do contexto em que aparecem. No quanteda, usamos a função kwic(Dados, pattern = \"padrão\"), após o texto ter sido tokenizado. Se ainda não tiver carregado o pacote Quanteda: library(quanteda) Vamos para um exemplo do texto “Ciência como vocação” de Max Weber: texto = &quot;Por fim, é da sabedoria quotidiana que algo pode ser verdadeiro, embora não seja nem belo, nem sagrado, nem bom. Mas estes são apenas os casos mais elementares da luta que entre si travam os deuses dos ordenamentos e valores singulares. Como será possível pretender decidir &#39;cientificamente&#39; entre o valor da cultura francesa e o da alemã é coisa que não enxergo. Também aqui diferentes deuses lutam entre si, e para sempre. Acontece, embora noutro sentido, o mesmo que ocorria no mundo antigo, quando ainda se não tinha desencantado dos seus deuses e demónios: tal como os Gregos ofereciam sacrifícios, umas vezes, a Afrodite, outras a Apolo e, sobretudo, aos deuses da sua cidade, assim acontece ainda hoje, embora o culto se tenha desmistificado e careça da plástica mítica, mas intimamente verdadeira, daquela conduta. Sobre estes deuses e a sua eterna luta decide o destino, não decerto uma &#39;ciência&#39;. Apenas se pode compreender o que seja o divino para uma e outra ordem ou numa e noutra ordem&quot; # termos a serem buscados termos.vetor= c(&quot;deus*&quot;, &quot;divin*&quot;, &quot;luta&quot;) texto %&gt;% # precisamos primeiro tokenizar tokens%&gt;% # rodando a função de palavras chave em contexto kwic(., # termos a serem buscados. Pode ser um termo ou um vetor termos.vetor, # quantas palavras devem ser mostradas ao redor 4, # Para pegar tanto palavras minúsculas como as em maiúsculo. case_insensitive = TRUE) ## Keyword-in-context with 8 matches. ## [text1, 35] casos mais elementares da | luta | que entre si travam ## [text1, 41] entre si travam os | deuses | dos ordenamentos e valores ## [text1, 75] . Também aqui diferentes | deuses | lutam entre si, ## [text1, 106] tinha desencantado dos seus | deuses | e demónios: tal ## [text1, 131] , sobretudo, aos | deuses | da sua cidade, ## [text1, 162] conduta. Sobre estes | deuses | e a sua eterna ## [text1, 167] e a sua eterna | luta | decide o destino, ## [text1, 187] o que seja o | divino | para uma e outra No KWIC é possível ainda: usar regex como padrão de busca, através do parâmetro valuetype = \"regex em buscar por duas ou mais palavras em contexto com pattern = phrase( texto %&gt;% # precisamos primeiro tokenizar tokens%&gt;% # rodando a função de palavras chave em contexto kwic(., # termos a serem buscados. Pode ser um termo ou um vetor pattern = phrase(&quot;eterna luta&quot;), # quantas palavras devem ser mostradas ao redor 7, # Para pegar tanto palavras minúculas como as em maiúsculo. case_insensitive = TRUE) ## Keyword-in-context with 1 match. ## [text1, 166:167] . Sobre estes deuses e a sua | eterna luta | ## ## decide o destino, não decerto uma "],["text-mining-semantic-parsing.html", "9 Text mining: Semantic Parsing 9.1 POS - Part-of-speech tagging 9.2 Pacote UDPipe", " 9 Text mining: Semantic Parsing Até agora vimos abordagem onde a ordem das palavras e sua função gramatical não importava na análise, a chamada abordagem “saco-de-palavras”. Vamos agora para análise onde isso se faz importante, a abordagem semântica. 9.1 POS - Part-of-speech tagging É possível identificar a classe gramatical de cada palavras das frases. Vários pacotes no R fazem isso: openNLP, uma interface em R sobre o Apache OpenNLP tools, escrito em Java, com suporte a vários modelos de linguagem, inclusive o português coreNLP (Wrapper ao redor do Stanford CoreNLP Tools) Qdap com a função qdap::pos() (O pacote requer rJava, que requer JDE e JRE do Java instalados no seu computador, fora do R), RcppMeCab que é um wrapper da biblioteca “mecab”, spacyr, um wrapper ao redor do spaCy do Python (requer a instalação do Python e algumas de suas bibliotecas), UDPipe em C++(após a instalação, roda sem problemas no R) que possui modelos pré-treinados em 65 liguagens, inclusive o português. koRpus com suporte ao português e que possui integração com o programa gráfico rkward. No caso de pacotes que requerem o Rjava, uma dica é, ao menos no Linux, antes de instalar o Rjava, rodar o seguinte comando no terminal (não no console do R!). O comando a seguir faz com que o Rjava consiga encontrar o Java, e assim evita alguns tipos de erro na instalação. sudo R CMD javareconf 9.2 Pacote UDPipe Para o P.O.S (part-of-speech tagging), vamos usar o pacote UDPipe, já que ele não requer Python ou Java, sendo um wrapper do UDPipe C++, evitando boa parte das complicações de instalação, como em outros pacotes. O projeto é da Institute of Formal and Applied Linguistics, Faculty of Mathematics and Physics, Charles University da Reṕublica Tcheca. A função de POS é baseada no Google universal part-of-speech tags. UD vem de universal dependencies, um framework aberto para anotações gramaticais com 200 treebanks em amsi de 100 linguas. Milan Straka, Jan Hajiˇc, Jana Strakov. ́UDPipe: Trainable Pipeline for Processing CoNLL-U Files Performing Tokenization, Morphological Analysis, POS Tagging and Parsing Modelos de línguas no github de jwijffels Após instalar, carregando o pacote: library(udpipe) Vamos instalar um modelo de lingua não inglesa. Para ver a lista de linguagens disponíveis, rode o comando ?udpipe_download_model. Mas atente que nem todas as línguas estão disponíveis na última versão, sendo necessário a sua especificação. Para saber sobre os modelos em portuges utilizados no UD, veja aqui. Vamos utilisar a versão bosque, que funciona para PT-PT e PT-Br. Tal como está, o download ocorrerá no diretório de trabalho atual (digite getwd() para saber) # opção 1 dl &lt;- udpipe_download_model(language = &quot;portuguese-br&quot;, udpipe_model_repo = &#39;jwijffels/udpipe.models.ud.2.0&#39;) # Opção 2. &quot;bosque&quot; é a mais atual e mais utilisada dl &lt;- udpipe_download_model(language = &quot;portuguese-bosque&quot;) str(dl) Uma vez que o modelo foi baixado, vamos carregá-lo udmodel_ptBosque &lt;- udpipe_load_model(file = dl$file_model) Ou, caso tenha baixado em outro local: udmodel_ptBosque &lt;- udpipe_load_model(file = &quot;~/Documentos/R/portuguese-bosque-ud-2.5-191206.udpipe&quot;) E vamos aos testes: library(dplyr) texto &lt;- &quot;O rato Rogério roeu rapidamente a roupa roxa do rei Roberto de Roma.&quot; txt.anotado &lt;- udpipe::udpipe_annotate(udmodel_ptBosque, x = texto) %&gt;% as.data.frame() str(txt.anotado) ## &#39;data.frame&#39;: 16 obs. of 14 variables: ## $ doc_id : chr &quot;doc1&quot; &quot;doc1&quot; &quot;doc1&quot; &quot;doc1&quot; ... ## $ paragraph_id : int 1 1 1 1 1 1 1 1 1 1 ... ## $ sentence_id : int 1 1 1 1 1 1 1 1 1 1 ... ## $ sentence : chr &quot;O rato Rogério roeu rapidamente a roupa roxa do rei Roberto de Roma.&quot; &quot;O rato Rogério roeu rapidamente a roupa roxa do rei Roberto de Roma.&quot; &quot;O rato Rogério roeu rapidamente a roupa roxa do rei Roberto de Roma.&quot; &quot;O rato Rogério roeu rapidamente a roupa roxa do rei Roberto de Roma.&quot; ... ## $ token_id : chr &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## $ token : chr &quot;O&quot; &quot;rato&quot; &quot;Rogério&quot; &quot;roeu&quot; ... ## $ lemma : chr &quot;o&quot; &quot;rato&quot; &quot;Rogério&quot; &quot;roer&quot; ... ## $ upos : chr &quot;DET&quot; &quot;NOUN&quot; &quot;PROPN&quot; &quot;VERB&quot; ... ## $ xpos : chr NA NA NA NA ... ## $ feats : chr &quot;Definite=Def|Gender=Masc|Number=Sing|PronType=Art&quot; &quot;Gender=Masc|Number=Sing&quot; &quot;Gender=Masc|Number=Sing&quot; &quot;Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin&quot; ... ## $ head_token_id: chr &quot;2&quot; &quot;4&quot; &quot;2&quot; &quot;0&quot; ... ## $ dep_rel : chr &quot;det&quot; &quot;nsubj&quot; &quot;appos&quot; &quot;root&quot; ... ## $ deps : chr NA NA NA NA ... ## $ misc : chr NA NA NA NA ... # Para vermos apenas as classes gramaticais, apenas filtramos pela coluna: txt.anotado$upos ## [1] &quot;DET&quot; &quot;NOUN&quot; &quot;PROPN&quot; &quot;VERB&quot; &quot;ADV&quot; &quot;DET&quot; &quot;NOUN&quot; &quot;ADJ&quot; NA ## [10] &quot;ADP&quot; &quot;DET&quot; &quot;NOUN&quot; &quot;PROPN&quot; &quot;ADP&quot; &quot;PROPN&quot; &quot;PUNCT&quot; Na coluna “upos”: Roberto e Rogério são “PROPN”, ou seja, “proper names”, ou “nomes próprios” rato aparece como “NOUN”, isto é, substantivo. “VERB é verbo, ADJ é adjetivo, ADV é advérbio. Se quiser fazer só o POS Tagging, sem fazer lematização, e com isto ganhar tempo, pode-se restringir com o argumento tagger que pode ser um vetor: texto2 &lt;- &quot;O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa&quot; txt.anotado2 &lt;- udpipe_annotate(udmodel_ptBosque, x = texto2, tagger = &quot;default&quot; , parser = &quot;none&quot;) %&gt;% as.data.frame() txt.anotado2 ## doc_id paragraph_id sentence_id ## 1 doc1 1 1 ## 2 doc1 1 1 ## 3 doc1 1 1 ## 4 doc1 1 1 ## 5 doc1 1 1 ## 6 doc1 1 1 ## 7 doc1 1 1 ## 8 doc1 1 1 ## 9 doc1 1 1 ## 10 doc1 1 1 ## 11 doc1 1 1 ## 12 doc1 1 1 ## 13 doc1 1 1 ## 14 doc1 1 1 ## 15 doc1 1 1 ## 16 doc1 1 1 ## 17 doc1 1 1 ## 18 doc1 1 1 ## 19 doc1 1 1 ## 20 doc1 1 1 ## 21 doc1 1 1 ## 22 doc1 1 1 ## 23 doc1 1 1 ## 24 doc1 1 1 ## 25 doc1 1 1 ## 26 doc1 1 1 ## 27 doc1 1 1 ## sentence ## 1 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 2 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 3 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 4 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 5 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 6 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 7 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 8 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 9 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 10 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 11 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 12 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 13 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 14 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 15 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 16 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 17 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 18 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 19 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 20 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 21 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 22 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 23 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 24 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 25 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 26 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 27 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## token_id token lemma upos xpos ## 1 1 O o DET &lt;NA&gt; ## 2 2 fundamento fundamento NOUN &lt;NA&gt; ## 3 3 psicológico psicológico ADJ &lt;NA&gt; ## 4 4 sobre sobre ADP &lt;NA&gt; ## 5 5 o o DET &lt;NA&gt; ## 6 6 qual qual PRON &lt;NA&gt; ## 7 7 se se PRON &lt;NA&gt; ## 8 8 eleva elevar VERB &lt;NA&gt; ## 9 9 o o DET &lt;NA&gt; ## 10 10 tipo tipo NOUN &lt;NA&gt; ## 11 11-12 das &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 12 11 de de ADP &lt;NA&gt; ## 13 12 as o DET &lt;NA&gt; ## 14 13 individualidades individualidade NOUN &lt;NA&gt; ## 15 14-15 da &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 16 14 de de ADP &lt;NA&gt; ## 17 15 a o DET &lt;NA&gt; ## 18 16 cidade cidade NOUN &lt;NA&gt; ## 19 17 grande grande ADJ &lt;NA&gt; ## 20 18 é ser AUX &lt;NA&gt; ## 21 19 a o DET &lt;NA&gt; ## 22 20 intensificação intensificação NOUN &lt;NA&gt; ## 23 21-22 da &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 24 21 de de ADP &lt;NA&gt; ## 25 22 a o DET &lt;NA&gt; ## 26 23 vida vida NOUN &lt;NA&gt; ## 27 24 nervosa nervoso ADJ &lt;NA&gt; ## feats head_token_id dep_rel ## 1 Definite=Def|Gender=Masc|Number=Sing|PronType=Art &lt;NA&gt; &lt;NA&gt; ## 2 Gender=Masc|Number=Sing &lt;NA&gt; &lt;NA&gt; ## 3 Gender=Masc|Number=Sing &lt;NA&gt; &lt;NA&gt; ## 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 5 Gender=Masc|Number=Sing|PronType=Art &lt;NA&gt; &lt;NA&gt; ## 6 Gender=Masc|Number=Sing|PronType=Rel &lt;NA&gt; &lt;NA&gt; ## 7 Case=Acc|Gender=Masc|Number=Sing|Person=3|PronType=Prs &lt;NA&gt; &lt;NA&gt; ## 8 Mood=Ind|Number=Sing|Person=3|Tense=Imp|VerbForm=Fin &lt;NA&gt; &lt;NA&gt; ## 9 Definite=Def|Gender=Masc|Number=Sing|PronType=Art &lt;NA&gt; &lt;NA&gt; ## 10 Gender=Masc|Number=Sing &lt;NA&gt; &lt;NA&gt; ## 11 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 12 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 13 Definite=Def|Gender=Fem|Number=Plur|PronType=Art &lt;NA&gt; &lt;NA&gt; ## 14 Gender=Fem|Number=Plur &lt;NA&gt; &lt;NA&gt; ## 15 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 16 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 17 Definite=Def|Gender=Fem|Number=Sing|PronType=Art &lt;NA&gt; &lt;NA&gt; ## 18 Gender=Fem|Number=Sing &lt;NA&gt; &lt;NA&gt; ## 19 Gender=Fem|Number=Sing &lt;NA&gt; &lt;NA&gt; ## 20 Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin &lt;NA&gt; &lt;NA&gt; ## 21 Definite=Def|Gender=Fem|Number=Sing|PronType=Art &lt;NA&gt; &lt;NA&gt; ## 22 Gender=Fem|Number=Sing &lt;NA&gt; &lt;NA&gt; ## 23 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 24 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 25 Definite=Def|Gender=Fem|Number=Sing|PronType=Art &lt;NA&gt; &lt;NA&gt; ## 26 Gender=Fem|Number=Sing &lt;NA&gt; &lt;NA&gt; ## 27 Gender=Fem|Number=Sing &lt;NA&gt; &lt;NA&gt; ## deps misc ## 1 &lt;NA&gt; &lt;NA&gt; ## 2 &lt;NA&gt; &lt;NA&gt; ## 3 &lt;NA&gt; &lt;NA&gt; ## 4 &lt;NA&gt; &lt;NA&gt; ## 5 &lt;NA&gt; &lt;NA&gt; ## 6 &lt;NA&gt; &lt;NA&gt; ## 7 &lt;NA&gt; &lt;NA&gt; ## 8 &lt;NA&gt; &lt;NA&gt; ## 9 &lt;NA&gt; &lt;NA&gt; ## 10 &lt;NA&gt; &lt;NA&gt; ## 11 &lt;NA&gt; &lt;NA&gt; ## 12 &lt;NA&gt; &lt;NA&gt; ## 13 &lt;NA&gt; &lt;NA&gt; ## 14 &lt;NA&gt; &lt;NA&gt; ## 15 &lt;NA&gt; &lt;NA&gt; ## 16 &lt;NA&gt; &lt;NA&gt; ## 17 &lt;NA&gt; &lt;NA&gt; ## 18 &lt;NA&gt; &lt;NA&gt; ## 19 &lt;NA&gt; &lt;NA&gt; ## 20 &lt;NA&gt; &lt;NA&gt; ## 21 &lt;NA&gt; &lt;NA&gt; ## 22 &lt;NA&gt; &lt;NA&gt; ## 23 &lt;NA&gt; &lt;NA&gt; ## 24 &lt;NA&gt; &lt;NA&gt; ## 25 &lt;NA&gt; &lt;NA&gt; ## 26 &lt;NA&gt; &lt;NA&gt; ## 27 &lt;NA&gt; SpacesAfter=\\\\n Dicas POST - Part-of-Speech Tagging An introduction to part-of-speech tagging and the Hidden Markov Model do free code camp. Vignette do Udpipe no CRan. Para ir além do básico com o Udpipe, ver Jan Wijffels. UDPipe Natural Language Processing - Basic Analytical Use Cases. 2021. 9.2.1 Coocorrência de palavras A coocorrência de palavras pode nos auxiliar a pegar o sentido de uma grande quantidade de frases, nos mostrando palavras usadas na mesma sentença ou nas proximidades umas das outras. Por exemplo, podemos ver quantas vezes substantivos (nouns) aparecem junto a adjetivos (adj) na mesma sentença, ou junto a verbos (VERB), ou até alguma distância determinada. Vários pacotes fazem este trabalho, como o widyr::pairwise_count, mas usaremos o pacote do UDpipe. O comando udpipe::cooccurrence() aceita vetores, dataframes e objetos tipo “cooccurrence” como input. Usaremos um exemplo com vetor. O data frame requer formato especial. Para mais detalhes, conferir o help do termo com o comando ?udpipe::cooccurrence(). Num exemplo simples: coocor &lt;- udpipe::cooccurrence(c(&quot;Abacate&quot;, &quot;Banana&quot;, &quot;Abacate&quot;, &quot;Abacate&quot;, &quot;Banana&quot;, &quot;carambola&quot;, &quot;Banana&quot;, &quot;uva&quot;)) coocor ## term1 term2 cooc ## 1 Abacate Banana 2 ## 2 Banana Abacate 1 ## 3 Abacate Abacate 1 ## 4 Banana carambola 1 ## 5 carambola Banana 1 ## 6 Banana uva 1 Podemos fazer a rede de palavras a partir deste vetor. Primeiro, carregamos os pacotes de análise e de visualização de redes. library(igraph) library(ggraph) Rodando: rede.palavras &lt;- igraph::graph_from_data_frame(coocor) ggraph::ggraph(rede.palavras, layout = &quot;fr&quot;) + geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = &quot;lightskyblue&quot;) + geom_node_text(aes(label = name), col = &quot;darkgreen&quot;, size = 4) + theme_graph(base_family = &quot;Arial Narrow&quot;) + theme(legend.position = &quot;none&quot;) + labs(title = &quot;Coocorrência&quot;) Uma vantagem da função de coocorrência do UDpipe para o do widyr é a possibilidade de utilizar skipgram como parâmetros. Isto significa que podemos controlar uma janela de quantas palavras de contexto, as palavras ao redor da nossa palavra alvo, contará no nosso cálculo de coocorrência. Assim, skipgrams de valores maiores terão mais palavras vizinhas computadas. var1 &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;c&quot;) udpipe::cooccurrence(var1, skipgram = 0) ## term1 term2 cooc ## 1 A B 2 ## 2 B A 1 ## 3 A A 1 ## 4 B c 1 udpipe::cooccurrence(var1, skipgram = 1) ## term1 term2 cooc ## 1 A B 3 ## 2 B A 2 ## 3 A A 2 ## 4 B c 1 ## 5 A c 1 Vamos usar uma base de dados mais extensa, as Notas Taquigráficas da CPI da Pandemia. Elas foram estruturadas em data frames em csv e Rds. O modo mais fácil e indicado é importar o .Rds, que já está no formato do R. As notas foram separadas por pessoa, partido, bloco parlamentar, função na CPI, e estado de origem. NotasTaq &lt;- readRDS(url(&quot;https://github.com/SoaresAlisson/NotasTaquigraficas/raw/master/rds/NT_30-Oitiva-Luiz_Paulo_Dominguetti_Pereira.Rds&quot;)) # vamos transformar nosso dataframe em tibble NotasTaq = dplyr::as_tibble(NotasTaq) # pegando apenas as falas e transformando em um vetor de um elemento, para poder usar no udpipe falas &lt;- NotasTaq %&gt;% #filter(nome == &quot;Omar Aziz&quot;) %&gt;% select(fala) %&gt;% filter(nome == unique(NotasTaq$nome)[2]) %&gt;% select(fala) %&gt;% paste0() NT.ud &lt;- udpipe_annotate(udmodel_ptBosque, x = falas) %&gt;% as.data.frame() Podemos fazer uma frequência básica NT.ud.lemma &lt;- NT.ud %&gt;% filter(upos == &quot;ADJ&quot;) %&gt;% select(lemma) # pegando apenas substantivos (noun) da coluna upos estatisticasTexto &lt;- subset(NT.ud, upos %in% c(&quot;NOUN&quot;)) # txt_freq retorna um df com 3 colunas: 1) termo (key); 2) frequência (freq) e 3) frequencia percentual (freq_pct) estatisticasTexto &lt;- txt_freq(estatisticasTexto$token) estatisticasTexto$key &lt;- factor(estatisticasTexto$key, levels = rev(estatisticasTexto$key)) lattice::barchart(key ~ freq, # pegando os 25 casos mais frequentes data = head(estatisticasTexto, 25), col = &quot;indianred&quot;, main = &quot;Substantivos mais frequentes do depoente&quot;, xlab = &quot;Frequência&quot;) Perceba que há imprecisões: “Sr.” é pronome de tratamento, mas figura como substantivo. 9.2.2 Rede de palavras (wordnet) Montando a rede de palavras com os dados acima. coocorrencias &lt;- cooccurrence(x = subset(NT.ud, upos %in% c(&quot;NOUN&quot;, &quot;ADJ&quot;, &quot;VERB&quot;)), term = &quot;lemma&quot;, group = c(&quot;doc_id&quot;, &quot;paragraph_id&quot;, &quot;sentence_id&quot;)) wordnetwork &lt;- head(coocorrencias, 60) wordnetwork &lt;- igraph::graph_from_data_frame(wordnetwork) ggraph::ggraph(wordnetwork, layout = &quot;fr&quot;) + ggraph::geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = &quot;lightskyblue3&quot;) + geom_node_text(aes(label = name), col = &quot;darkgreen&quot;, size = 4) + theme_graph(base_family = &quot;Arial&quot;) + theme(legend.position = &quot;none&quot;) + labs(title = &quot;Coocorrência dentro da sentença&quot;, subtitle = &quot;60 substantivos, adjetivos e verbos no depoimento \\nde Dominguetti na CPI da Pandemia&quot;) É possível ainda fazer rede de palavras com a função qdap::word_associate que ao invés de mostrar ligações mais frequentes, destaca as palavras mais frequentes através de seu tamanho, como numa nuvem de palavras. 9.2.3 Análise de semelhanças 9.2.3.1 Algoritmo Smith_Waterman Se busca regiões similares entre dois textos, um modo de detectar tais semelhança é usando o algoritmo Smith-Waterman, desenvolvido inicialmente na Biologia para identificar sequências de moléculas no artigo: SMITH T.F.Smith, WATERMAN, M. S. Identification of common molecular subsequences. Journal of Molecular Biology. Volume 147, Issue 1, 25 March 1981, Pages 195-197 Dadas duas sequências de letras, o algoritmo encontra o local ótimo de alinhamento. O pacote text.alignment do UDPipe aplica o algoritmo a palavras e letras, tentando identificar regiões similares entre duas strings. Você pode conferir um vignette do pacote (no linguajar do R, “vignette” é um guia rápido, com exemplo passo a passo) ou o manual. Com este pacote é possível: - encontrar palavras em documentos mesmo com grafia incorreta - Encontrar sequências de um texto em outros. Bom para comparar traduções ou identificar plágios. # traducao Nelson Jahr Garcia txt1 = &quot;E esqueceu-se de acrescentar: a primeira vez como tragédia, a segunda como farsa.&quot; # traducao Boitempo txt2 = &quot;Ele se esqueceu de acrescentar: a primeira vez como tragédia, a segunda como farsa.&quot; text.alignment::smith_waterman(txt1, txt2) ## Swith Waterman local alignment score: 153 ## ---------- ## Document a ## ---------- ## E ###esqueceu-se de acrescentar: a primeira vez como tragédia, a ## segunda como farsa. ## ---------- ## Document b ## ---------- ## e se esqueceu### de acrescentar: a primeira vez como tragédia, a ## segunda como farsa. Repare que em ambas as frases, preencheu-se com o sinal de tralha (#) até os textos ficarem alinhados. O modo padrão da função é buscar por caracteres, mas podemos mudar o padrão para palavras, o que pode ser mais adequado para nosso caso. text.alignment::smith_waterman(txt1, txt2, type = &quot;words&quot;) ## Swith Waterman local alignment score: 23 ## ---------- ## Document a ## ---------- ## se ######## de acrescentar a primeira vez como tragédia a segunda como ## farsa ## ---------- ## Document b ## ---------- ## se esqueceu de acrescentar a primeira vez como tragédia a segunda como ## farsa Dicas Analise Textual Julia Silge Learn tidytext with my new learnr course. Um curso interativo do pacote Tidytext. Textos sobre análise textual CASTELFRANCHI, Yurij. A análise de textos auxiliada pelo computador: um laboratório a céu aberto para as ciências sociais. Journal of Science Communication 16(02)(2017)C04 GRIMMER, Justin.STEWART, Brandom. Text as Data: The Promise and Pitfalls of Automatic ContentAnalysis Methods for Political Texts. Political Analysis(2013) pp. 1–31. doi:10.1093/pan/mps028. TREADWELL, Donald. Content Analysis: Understanding Text and Image in Numbers. Understanding Text and Image in Numbers. In __ Introducing Communication Research: paths of Inquiry. Sage. 2014. (Capítulo sobre análise de conteúdo) Link para diversos artigos de Gary King sobre Automated Text Analysis. Vídeos BROWN, Taylor W. Workshop on automated text analysis no Summer Institute in Computational Social Science na Universidade de Oxford em 2019. Em inglês, sem legendas, usando o pacote Quanteda. Parte 1 e Parte 2. O material da aula no Google Drive. "],["sentiment.html", "10 Classificação: Dicionários e Análise de Sentimentos 10.1 Classificação 10.2 Análise de sentimentos (Sentiment Analysis) por meio de dicionário/léxico 10.3 Dicas finais: Bibliografia indicada", " 10 Classificação: Dicionários e Análise de Sentimentos Objetivos deste capítulo: Apresentar uma introdução à análise de sentimentos utilizando dicionários Apresentar como criar e utilizar dicionários/léxicos em classificação Como fazer isso com diferentes pacotes do R 10.1 Classificação Podemos classificar palavras, sentenças, textos, documentos, corpus, etc. através de diferentes métodos. A análise de sentimentos ou emoções é um sub-item particular dentre os tipos de classificação. Entre as diferentes formas de classificação, seguindo a tipologia de Welbers et al. (2017) temos: Welbers, K., Van Atteveldt, W., &amp; Benoit, K. (2017). Text Analysis in R. Communication Methods and Measures, 11 (4), 245-265. http://www.tandfonline.com/doi/10.1080/19312458.2017.1387238 Dicionários e métodos de contagem: as categorias e os termos que compõem estas categorias são disponibilizados previamente pelo pesquisador. Aprendizado de máquinas supervisionado, ou supervised machine learning: Um (ou vários) seres humanos classificam textos com base em categorias estabelecidas por estes e o algoritmo irá tentar aprender o padrão e aplicar na classificação subsequente. Aprendizado de máquinas não-supervisionado ou unsupervised machine learning. Exemplo é a modelagem de tópicos (topic modeling) que tem o algoritmo Latent Dirichlet Allocation (LDA) como exemplo mais famoso. O algoritmo estabelece as categorias e os termos, cabendo muitas vezes ao pesquisador apenas estabelecer o número de categorias. categorização estatística: não possui categorias prévias dadas por humanos, mas também não é machine learning. Há várias formas de analisar sentimentos. O modo mais simples e talvez mais recorrente se dá com o uso de dicionários/léxicos com termos/palavras e seu sentimento ou uma pontuação (score) correspondente. Mas há também formas mais elaboradas de análise sentimento, e também mais complicadas, que mixam outras técnicas de processamento de linguagem natural com a análise de sentimentos. 10.2 Análise de sentimentos (Sentiment Analysis) por meio de dicionário/léxico Como sempre, o R possui diversos pacotes também para análise de sentimentos utilizando léxico de palavras, como por exemplo, o SentimentAnalysis, syuzhet, o Tidytext e o Quanteda. De modo simplificado, a análise de sentimentos pode ser entendida como uma forma de classificação em categorias pré-definidas. Tais categorias podem ser “positivo” e “negativo” em sua versão mais simples e binária, trinaria se para além das duas se incorporar a categoria “neutro”, ou pode-se ainda utilizar de diversas outras categorias, como “espanto”, “indecisão”, etc. Pode-se ainda atribuir diferentes pesos aos termos dentro de um escopo de pontuação, por exemplo, entre -5 e +5, ou entre -10 a +10. Isto faz sentido nas situações onde queira se diferenciar termos como “ruim” de “péssimo” e de “pior”. 10.2.1 Datasets de sentimentos/emoções Além de possibilitar criar seu próprio dicionário, com suas próprias categorias, é possível usar léxicos de sentimentos já pronto, contendo dezenas de milhares de palavras já categorizadas. Diferentes pacotes vem com diferentes dicionários, alguns com dicionários em comum. Alguns pacotes são apenas de datasets de sentimentos a serem usados. O pacote tidytext vem com os seguintes dicionários: - Bing: palavras rotuladas em positivo e negativo - AFINN: palavras com escores entre -5 e 5. - NRC: Emoções são classificados em 8 emoções. Além de positivo e negativo: - “trust”, “fear”, “sadness”,“anger”, “surprise”, disgust”, “joy” e “anticipation” - Loughran-McDonald Sentiment lexicon. Requer licença para uso comercial. str(tidytext::get_sentiments(&quot;bing&quot;)) ## tibble [6,786 × 2] (S3: tbl_df/tbl/data.frame) ## $ word : chr [1:6786] &quot;2-faces&quot; &quot;abnormal&quot; &quot;abolish&quot; &quot;abominable&quot; ... ## $ sentiment: chr [1:6786] &quot;negative&quot; &quot;negative&quot; &quot;negative&quot; &quot;negative&quot; ... str(tidytext::get_sentiments(&quot;afinn&quot;)) ## spec_tbl_df [2,477 × 2] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ word : chr [1:2477] &quot;abandon&quot; &quot;abandoned&quot; &quot;abandons&quot; &quot;abducted&quot; ... ## $ value: num [1:2477] -2 -2 -2 -2 -2 -2 -3 -3 -3 -3 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. word = col_character(), ## .. value = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; str(tidytext::get_sentiments(&quot;loughran&quot;)) ## tibble [4,150 × 2] (S3: tbl_df/tbl/data.frame) ## $ word : chr [1:4150] &quot;abandon&quot; &quot;abandoned&quot; &quot;abandoning&quot; &quot;abandonment&quot; ... ## $ sentiment: chr [1:4150] &quot;negative&quot; &quot;negative&quot; &quot;negative&quot; &quot;negative&quot; ... str(tidytext::get_sentiments(&quot;nrc&quot;)) ## tibble [13,875 × 2] (S3: tbl_df/tbl/data.frame) ## $ word : chr [1:13875] &quot;abacus&quot; &quot;abandon&quot; &quot;abandon&quot; &quot;abandon&quot; ... ## $ sentiment: chr [1:13875] &quot;trust&quot; &quot;fear&quot; &quot;negative&quot; &quot;sadness&quot; ... O pacote SentimentAnalysis vem com os seguintes léxicos: - QDAP (somente seu dicionário, não o pacote QDAP), - Harvard IV, dicionário piscológico, com a lista utilizada no software General Inquirer. - Henry’s Financial dictionary (Henry 2008), dicionário focado em finanças - Loughran-McDonald Financial dictionary (Loughran and McDonald 2011) str(SentimentAnalysis::DictionaryGI) # Harvard-IV ## List of 2 ## $ negative: chr [1:2005] &quot;abandon&quot; &quot;abandonment&quot; &quot;abate&quot; &quot;abdicate&quot; ... ## $ positive: chr [1:1637] &quot;abide&quot; &quot;ability&quot; &quot;able&quot; &quot;abound&quot; ... str(SentimentAnalysis::DictionaryHE) # Henry’s Financial dictionary ## List of 2 ## $ negative: chr [1:85] &quot;below&quot; &quot;challenge&quot; &quot;challenged&quot; &quot;challenges&quot; ... ## $ positive: chr [1:105] &quot;above&quot; &quot;accomplish&quot; &quot;accomplished&quot; &quot;accomplishes&quot; ... str(SentimentAnalysis::DictionaryLM) # Loughran-McDonald Financial dictionary ## List of 3 ## $ negative : chr [1:2355] &quot;abandon&quot; &quot;abandoned&quot; &quot;abandoning&quot; &quot;abandonment&quot; ... ## $ positive : chr [1:354] &quot;able&quot; &quot;abundance&quot; &quot;abundant&quot; &quot;acclaimed&quot; ... ## $ uncertainty: chr [1:297] &quot;abeyance&quot; &quot;abeyances&quot; &quot;almost&quot; &quot;alteration&quot; ... SentimentAnalysis::loadDictionaryQDAP() |&gt; str() # polarity words do QDAP ## List of 2 ## $ positiveWords: chr [1:1280] &quot;a plus&quot; &quot;abound&quot; &quot;abund&quot; &quot;access&quot; ... ## $ negativeWords: chr [1:2952] &quot;abnorm&quot; &quot;abolish&quot; &quot;abomin&quot; &quot;abort&quot; ... ## - attr(*, &quot;class&quot;)= chr &quot;SentimentDictionaryBinary&quot; O pacote corpus possui o WordNet-Affect Lexicon, o “AFINN Sentiment Lexicon” O pacote lexicon possui muitos datasets, para citar alguns dos diversos: lexicon::key_sentiment_jockers: Jockers Sentiment Key lexicon::hash_sentiment_huliu: dataset com 6.874 termos com valores que vão de -2 a 1, baseado em Hu &amp; Liu’s lexicon::hash_emojis: com descrição de emojis lexicon::nrc_emotions: 14.182 linhas em 9 variáveis/categorias: term, anger, anticipation, disgust, fear, joy, sadness, surprise e trust. lexicon::profanity_* com vários datasets com palavrões, termos racistas/linguagem de ódio, ou hash_internet_slang com gírias de internet. É possível realizar uma busca do que há disponível no pacote com a função lexicon::available_data('termo') O pacote LexiconPT possui três léxicos de sentimentos em português (o utilizaremos mais à frente), dois deles mais de 30 mil termos. str(lexiconPT::oplexicon_v2.1) ## &#39;data.frame&#39;: 30673 obs. of 3 variables: ## $ term : chr &quot;ababadar&quot; &quot;ababelar-se&quot; &quot;ababelar&quot; &quot;abacanar&quot; ... ## $ type : chr &quot;vb&quot; &quot;vb&quot; &quot;vb&quot; &quot;vb&quot; ... ## $ polarity: int 0 1 -1 1 1 -1 -1 -1 -1 -1 ... str(lexiconPT::oplexicon_v3.0) ## &#39;data.frame&#39;: 32191 obs. of 4 variables: ## $ term : chr &quot;=[&quot; &quot;=@&quot; &quot;=p&quot; &quot;=P&quot; ... ## $ type : chr &quot;emot&quot; &quot;emot&quot; &quot;emot&quot; &quot;emot&quot; ... ## $ polarity : int -1 -1 -1 -1 -1 1 1 1 1 -1 ... ## $ polarity_revision: chr &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; ... str(lexiconPT::sentiLex_lem_PT02) ## &#39;data.frame&#39;: 7014 obs. of 5 variables: ## $ term : chr &quot;a-vontade&quot; &quot;abafado&quot; &quot;abafante&quot; &quot;abaixado&quot; ... ## $ grammar_category : chr &quot;N&quot; &quot;Adj&quot; &quot;Adj&quot; &quot;Adj&quot; ... ## $ polarity : num 1 -1 -1 -1 -1 1 -1 1 1 -1 ... ## $ polarity_target : chr &quot;N0&quot; &quot;N0&quot; &quot;N0&quot; &quot;N0&quot; ... ## $ polarity_classification: chr &quot;MAN&quot; &quot;JALC&quot; &quot;MAN&quot; &quot;JALC&quot; ... Mas é sempre bom ressaltar e repetir: o melhor é ter um dicionário adequado para o contexto de aplicação. A aplicação de dicionários tem de ser validada! “The main take-home message is, however, that a human eye is still required to guarantee the validity of measuring sentiment in content analysis… we strongly recommend that every automatic text analysis project should start with coding a validation set.” (van Atteveldt et al,2021, p.3) 10.2.2 Pacote SentimentAnalysis Pacote SentimentAnalysis, link no Cran, manual em pdf e página vignette (de exemplo/tutorial) Vamos iniciar criando nosso próprio dicionário de sentimentos para entender melhor o conceito de análise de sentimentos. Após instalar o pacote utilizando o comando install.packages(\"SentimentAnalysis\") # carregando o pacote library(SentimentAnalysis) # Vetor com frases a serem analisadas documentos &lt;- c(&quot;Café é bom&quot;, &quot;Rúcula é ruim&quot;, &quot;Bem mais ou menos&quot;, &quot;refrigerante é péssimo&quot;, &quot;ruim é ter de trabalhar&quot;) # Criando um dicionário (no caso, do tipo binário) dict_pt &lt;- SentimentDictionaryBinary( # vetor de palavras com conotação positiva c(&quot;bom&quot;,&quot;boa&quot;, &quot;excelente&quot;), # vetor de palavras com conotação negativa c(&quot;ruim&quot;, &quot;péssimo&quot;)) # realizando a análise de sentimento a partir do dicionário criado AnaliseSentimentos &lt;- analyzeSentiment(documentos, language=&quot;portuguese&quot;, rules=list(&quot;pontos&quot;=list(ruleSentiment, dict_pt))) # visualizando AnaliseSentimentos ## pontos ## 1 0.5000000 ## 2 -0.5000000 ## 3 0.0000000 ## 4 -0.5000000 ## 5 -0.3333333 # Apresentando este resultado de forma mais legível para humanos, como um data frame. sentimentosDF &lt;- data.frame(frase = documentos, score.sentimentos = AnaliseSentimentos, # convertendo os valores em direções: pos., neg. e neutro sentimento = convertToDirection(AnaliseSentimentos$pontos)) sentimentosDF ## frase pontos sentimento ## 1 Café é bom 0.5000000 positive ## 2 Rúcula é ruim -0.5000000 negative ## 3 Bem mais ou menos 0.0000000 neutral ## 4 refrigerante é péssimo -0.5000000 negative ## 5 ruim é ter de trabalhar -0.3333333 negative Para criar dicionário ponderado (weighted), isto é, com valores contínuos, onde cada termo recebe pontuação, fazemos da seguinte forma: colocamos como primeiro vetor os termos e o segundo com suas respectivas pontuações. # Opção 1: # Léxico de sentimentos a partir de dois vetores d &lt;- SentimentDictionaryWeighted(c(&quot;péssimo&quot;, &quot;ruim&quot;, &quot;bom&quot;, &quot;boa&quot;, &quot;excelente&quot;, &quot;top&quot;), c(-10, -5, +5, +5, +10, +10),) d ## Type: weighted (words with individual scores) ## Intercept: 0 ## -10.00 péssimo ## -5.00 ruim ## 5.00 bom ## 5.00 boa ## 10.00 excelente ## 10.00 top # Opção 2 # Montando o léxico de sentimentos a partir de uma tabela tabela &lt;- read.table(header = FALSE, text = &#39; péssimo -10 ruim -5 ok 0 bom +5 boa +5 excelente +10 top +10&#39;) d &lt;- SentimentDictionaryWeighted(tabela[,1], tabela[,2]) d ## Type: weighted (words with individual scores) ## Intercept: 0 ## -10.00 péssimo ## -5.00 ruim ## 0.00 ok ## 5.00 bom ## 5.00 boa ## 10.00 excelente ## 10.00 top Também é possível utilizar dicionários/léxicos de sentimentos prontos em português. É sempre bom lembrar que certos termos em um contexto, podem bem significar outra coisa em outros contextos. Por isso há diversidade de dicionários, e sempre deve-se procurar, sempre que possível, utilizar um dicionário que tenha sido criado num contexto parecido ao que vamos utilizar, quando não, criarmos nosso próprio léxico para a ocasião, ou adaptar algum. Além disso, o ideal é sempre validarmos o dicionário. O pacote LexiconPT disponibiliza alguns desses dicionários/léxicos na língua de Camões, alguns com mais de 30 mil termos já catalogados. Para padronização, os termos se encontram em minúsculo e sem acentuação, alguns até com emoticons catalogados ;). Após a instalação do pacote com install.pacakges(\"lexiconPT\"), vamos carregar a biblioteca/pacote com: library(lexiconPT) Para checar se uma palavra específica está presente nos léxicos do LexiconPT e quais pontuações recebe: lexiconPT::get_word_sentiment(&quot;comemorar&quot;) ## $oplexicon_v2.1 ## term type polarity ## 6622 comemorar vb 0 ## ## $oplexicon_v3.0 ## term type polarity polarity_revision ## 7298 comemorar vb 0 A ## ## $sentilex ## [1] &quot;Word not present in dataset&quot; lexiconPT::get_word_sentiment(&quot;gritar&quot;) ## $oplexicon_v2.1 ## term type polarity ## 16646 gritar vb 1 ## ## $oplexicon_v3.0 ## term type polarity polarity_revision ## 17567 gritar vb 1 A ## ## $sentilex ## [1] &quot;Word not present in dataset&quot; lexiconPT::get_word_sentiment(&quot;espernear&quot;) ## $oplexicon_v2.1 ## [1] &quot;Word not present in dataset&quot; ## ## $oplexicon_v3.0 ## [1] &quot;Word not present in dataset&quot; ## ## $sentilex ## [1] &quot;Word not present in dataset&quot; lexiconPT::get_word_sentiment(&quot;confrontar&quot;) ## $oplexicon_v2.1 ## term type polarity ## 7049 confrontar vb -1 ## ## $oplexicon_v3.0 ## term type polarity polarity_revision ## 7736 confrontar vb -1 A ## ## $sentilex ## term grammar_category polarity polarity_target ## 1582 confrontar V 1 N0:N1 ## polarity_classification ## 1582 MAN lexiconPT::get_word_sentiment(&quot;detestar&quot;) ## $oplexicon_v2.1 ## term type polarity ## 10573 detestar vb 1 ## ## $oplexicon_v3.0 ## term type polarity polarity_revision ## 11314 detestar vb 1 A ## ## $sentilex ## term grammar_category polarity polarity_target polarity_classification ## 2353 detestar V 0 N0:N1 MAN lexiconPT::get_word_sentiment(&quot;odiar&quot;) ## $oplexicon_v2.1 ## term type polarity ## 22371 odiar vb 1 ## ## $oplexicon_v3.0 ## term type polarity polarity_revision ## 23549 odiar vb 1 A ## ## $sentilex ## term grammar_category polarity polarity_target polarity_classification ## 5215 odiar V 0 N0:N1 MAN Para podermos trocar de dicionário de modo fácil, vamos criar um vetor com estes léxicos disponíveis no pacote LexiconPT # Escolhendo manuamente um léxico vamos usar. lex &lt;- oplexicon_v2.1 # Criando o vetor com léxicos lex &lt;- c(&quot;oplexicon_v2.1&quot;, &quot;oplexicon_v3.0&quot;, &quot;sentiLex_lem_PT02&quot;) # para que captemos não o nome, mas o conteúdo do dataframe, rodamos o seguinte # comando, que pega o segundo item do vetor lexico &lt;- eval(parse(text = lex[2])) str(lexico) ## &#39;data.frame&#39;: 32191 obs. of 4 variables: ## $ term : chr &quot;=[&quot; &quot;=@&quot; &quot;=p&quot; &quot;=P&quot; ... ## $ type : chr &quot;emot&quot; &quot;emot&quot; &quot;emot&quot; &quot;emot&quot; ... ## $ polarity : int -1 -1 -1 -1 -1 1 1 1 1 -1 ... ## $ polarity_revision: chr &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; ... Os dicionários do LexiconPT possuem vários termos duplicados, e isto impossibilita de usá-los diretamente no pacote SentimentAnalysis. Para identificar duplicados usamos o comando duplicated(), que retorna um vetor de mesmo tamanho, com valores lógicos. # &quot;True&quot; indica quantos valores duplicados nós temos summary(duplicated(lexico$term)) ## Mode FALSE TRUE ## logical 31951 240 # Se quisermos ver quais os termos duplicados: duplicados &lt;- lexico$term[duplicated(lexico$term)] duplicados[1:35] ## [1] &quot;;)&quot; &quot;;D&quot; &quot;;p&quot; &quot;:(&quot; ## [5] &quot;:)&quot; &quot;:@&quot; &quot;:*&quot; &quot;:P&quot; ## [9] &quot;:x&quot; &quot;aberrantes&quot; &quot;academica&quot; &quot;academicas&quot; ## [13] &quot;academico&quot; &quot;academicos&quot; &quot;acoitar&quot; &quot;acoutar&quot; ## [17] &quot;adultera&quot; &quot;adulteras&quot; &quot;adultero&quot; &quot;adulteros&quot; ## [21] &quot;afinar&quot; &quot;anacronica&quot; &quot;anacronicas&quot; &quot;anacronico&quot; ## [25] &quot;anacronicos&quot; &quot;anemica&quot; &quot;anemicas&quot; &quot;anemico&quot; ## [29] &quot;anemicos&quot; &quot;anomalo&quot; &quot;antiquissima&quot; &quot;antiquissimas&quot; ## [33] &quot;antiquissimo&quot; &quot;antiquissimos&quot; &quot;apedrar&quot; Uma solução rápida seria retirar estes termos duplicados, ficando apenas com os termos que apareceram uma única vez. Fazemos isto com a função unique(). EM EXPANSÃO Este trecho sobre uso do LexiconPT será expandido em breve A função lexiconPT::compareDictionaries() compara dicionários de dois em dois, mostrando total de palavras únicas e as compartilhadas. dic1 &lt;- SentimentAnalysis::SentimentDictionary(c(&quot;incerto&quot;, &quot;possível&quot;, &quot;provável&quot;, &quot;talvez&quot;)) dic2 &lt;- SentimentAnalysis::SentimentDictionary(c(&quot;preferencialmente&quot;, &quot;pretender&quot;, &quot;provável&quot;)) # Comparando os dicionários cmp &lt;- SentimentAnalysis::compareDictionaries(dic1, dic2) ## Comparing: wordlist vs wordlist ## ## Total unique words: 6 ## Matching entries: 1 (0.1666667%) cmp ## $totalUniqueWords ## [1] 6 ## ## $totalSameWords ## [1] 1 ## ## $ratioSameWords ## [1] 0.1666667 10.2.3 Pacote Tidytext Para ver exemplos de análise de sentimentos com princípios Tidy, em inglês: ver o capítulo 2 de “Text Mining with R: A Tidy Approach” da Julia Silge link aqui. Um exemplo de análise de sentimentos de Harry Potter (há um pacote no Github com sete livros da saga Harry Potter) com o Tidytext, ver este link. 10.2.4 Pacote Quanteda O Quanteda torna fácil o uso de dicionários, e a análise de sentimentos é um caso de dicionário. Veja aqui a sessão “Grouping words by dictionary or equivalence class” para se ter uma ideia. EM EXPANSÃO Este trecho será expandido 10.2.5 Pacote syuzhet O pacote syuzhet e seu vingette. Carregando o pacote library(syuzhet) Criando um dicionário personalizado texto &lt;- &quot;É comum adorar esta definição. Mas é fácil odiar esta outra. A beleza está nos detalhes&quot; # tokenizando token_frases &lt;- get_sentences(texto) # criando um data frame com sentimentos, usando vetores meu_lexicon &lt;- data.frame(word=c(&quot;adorar&quot;, &quot;odiar&quot;, &quot;beleza&quot;, &quot;feio&quot;), value=c(1,-1,1, -1)) # aplicando valores_frases &lt;- get_sentiment(token_frases, method = &quot;custom&quot;, lexicon = meu_lexicon) valores_frases ## [1] 1 -1 1 EM EXPANSÃO Este trecho será expandido Por fim, vale dizer que a análise de sentimentos não se resume ao uso de dicionários prontos, havendo toda uma miríade de outras técnicas/ técnicas mistas utilizadas ali, como word embedding, topic models, etc. 10.3 Dicas finais: Bibliografia indicada Dicas: Análise de Sentimentos Em termos teóricos, para saber mais sobre análise de sentimentos, recomendo ver - JURAFSKY, Dan.; MARTIN, James H. Speech and Language Processing - An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition. 3ª Edição. 2020. especialmente o cap.4 Naive Bayes and Sentiment Classification para entender os mecanismos de classificação e cap.20 Lexicons for Sentiment, Affect, and Connotation para entender os diferentes léxicos utilizados. Uma boa introdução à lógica da análise de sentimentos, sobre definições e conceitos básicos, problemas, entradas e saídas, e como os resultados podem ser usados na prática, ver: LIU, Bing. Sentiment Analysis and Subjectivity. 2010 Para uma visão mais panorâmica das diferentes abordagens em análise de sentimentos, recomenda-se: Wouter van Atteveldt, Mariken A. C. G. van der Velden &amp; Mark Boukes (2021) The Validity of Sentiment Analysis: Comparing Manual Annotation, Crowd-Coding, Dictionary Approaches, and Machine Learning Algorithms, Communication Methods and Measures, 15:2, 121-140, DOI: 10.1080/19312458.2020.1869198 pdf Sobre os diferentes tipos de dicionários/léxicos: Ribeiro, F.N., Araújo, M., Gonçalves, P. et al. SentiBench - a benchmark comparison of state-of-the-practice sentiment analysis methods. EPJ Data Sci. 5, 23 (2016). https://doi.org/10.1140/epjds/s13688-016-0085-1 Para uma lista extensa de bibliografia acerca do tema, ver Opinion Mining, Sentiment Analysis, and Opinion Spam Detection "],["análise-de-redes-sociais.html", "11 Análise de Redes Sociais 11.1 O pacote Igraph 11.2 Os pacotes ggraph e tidygraph . Construindo grafos com o tidyverse. 11.3 Redes de palavras 11.4 Redes de citação 11.5 Gráfico de centralidade 11.6 Comunidades 11.7 Sugestões de links", " 11 Análise de Redes Sociais O R possui diversos pacotes para análise de rede, como o igraph, statnet, e do tidyverse temos tydygraph e ggraph. 11.1 O pacote Igraph Instalando o pacote igraph: install.packages(&quot;igraph&quot;) # instalando o pacote chamando o pacote já instalado library(igraph) # chamando o pacote já instalado Pegando o famoso poema de Drummond: “João amava Teresa que amava Raimundo que amava Maria que amava Joaquim que amava Lili que não amava ninguém” E o transformando em um grafo (os “gráficos” em análise de rede recebem este nome): library(igraph) g &lt;- graph.empty(directed = TRUE) # &quot;directed&quot; implica distinguir entre &quot;de&quot; e &quot;para&quot; na relação entre os nós. # Adicionando os vértices. g &lt;- g + vertex(&quot;João&quot;) g &lt;- g + vertex(&quot;Teresa&quot;) g &lt;- g + vertex(&quot;Raimundo&quot;) g &lt;- g + vertex(&quot;Maria&quot;) g &lt;- g + vertex(&quot;Joaquim&quot;) g &lt;- g + vertex(&quot;Lili&quot;) # Especificando as relações entres os vértices, os edges g &lt;- g + edges(&quot;João&quot;, &quot;Teresa&quot;) g &lt;- g + edges(&quot;Teresa&quot;, &quot;Raimundo&quot;) g &lt;- g + edges(&quot;Raimundo&quot;, &quot;Maria&quot;) g &lt;- g + edges(&quot;Maria&quot;, &quot;Joaquim&quot;) g &lt;- g + edges(&quot;Joaquim&quot;, &quot;Lili&quot;) plot.igraph(g) # plotando o grafo Neste caso, poderíamos ter feito este mesmo grafo com código mais compacto: library(igraph) g &lt;- graph.empty(directed=TRUE) # Adicionando os vértices. g &lt;- g + vertex(c(&quot;João&quot;, &quot;Teresa&quot;, &quot;Raimundo&quot;, &quot;Maria&quot;, &quot;Joaquim&quot;, &quot;Lili&quot;)) # Adicionando os edges em pares g &lt;- g + edges(c(&quot;João&quot;, &quot;Teresa&quot;, &quot;Teresa&quot;, &quot;Raimundo&quot;, &quot;Raimundo&quot;, &quot;Maria&quot;,&quot;Maria&quot;, &quot;Joaquim&quot;,&quot;Joaquim&quot;, &quot;Lili&quot;)) plot.igraph(g) Se o grafo sobe ou desce, pouco importa para nós aqui, importa as pessoas e as relações entre elas. Repare que os edges são entendidos aos pares. Se fizéssemos um vetor sem as devidas repetições, teríamos um gráfico errado das relações: g &lt;- graph.empty(directed=TRUE) g &lt;- g + vertex(c(&quot;João&quot;, &quot;Teresa&quot;, &quot;Raimundo&quot;, &quot;Maria&quot;, &quot;Joaquim&quot;, &quot;Lili&quot;)) g &lt;- g + edges(c(&quot;João&quot;, &quot;Teresa&quot;, &quot;Raimundo&quot;, &quot;Maria&quot;,&quot;Joaquim&quot;, &quot;Lili&quot;)) plot.igraph(g) Ou com código modo mais econômico ainda: g &lt;- graph.formula( João --+ Teresa --+ Raimundo --+ Maria --+ Joaquim --+ Lili ) plot(g) &lt; https://economia.estadao.com.br/noticias/geral,incra-reduz-assentamento-no-para-mineracao-ouro,70003919108!-- g &lt;- g + edges(“Diana”, “William”) –&gt; : :mineração: Incra vai reduzir assentamento no PA para favorecer mineração de ouro em troca de fazenda em MT - Economia - Estadão Cerca de 600 famílias serão afetadas, mas Incra diz não ter encontrado terra legalizada que pudesse ser adquirada para os assentados Vamos visualizar o exemplo de Franzosi que vimos anteriormente na seção . Transcrevi os dados da tabela 5. franzosi &lt;- &quot;Subject Action Object Frequency Fascists violence Workers 871 Fascists violence People 677 Fascists violence Socialists 482 Fascists violence Individuals 324 Police violence Workers 207 Individuals violence Workers 174 Police violence People 129 Fascists violence workers_agric 126 Police violence Protesters 92 Police violence workers_agric 88 Fascists violence Communists 87 Police violence Socialists 84 Police Violence Individuals 81 Individuals Violence Individuals 69 Workers Violence Fascists 56&quot; Vamos carregar o texto como uma tabela tabelaFranzosi &lt;- read.table(text = franzosi, header = TRUE, sep = &quot; &quot;) str(tabelaFranzosi) ## &#39;data.frame&#39;: 15 obs. of 4 variables: ## $ Subject : chr &quot;Fascists&quot; &quot;Fascists&quot; &quot;Fascists&quot; &quot;Fascists&quot; ... ## $ Action : chr &quot;violence&quot; &quot;violence&quot; &quot;violence&quot; &quot;violence&quot; ... ## $ Object : chr &quot;Workers&quot; &quot;People&quot; &quot;Socialists&quot; &quot;Individuals&quot; ... ## $ Frequency: num 871 677 482 324 207 174 129 126 92 88 ... Plotando o grafo com o qgraph: library(ggraph) # a coluna Action não nos interessa,vamos retirá-la tabelaFranzosi2 &lt;- tabelaFranzosi[,c(1,3,4)] # transformando o dataframe em grafo grafoFranz &lt;- igraph::graph_from_data_frame(tabelaFranzosi2) # Usando o qgraph qgraph::qgraph(tabelaFranzosi2, title= &quot;Violência entre grupos durante o fascismo na Itália da primeira metade do século XX&quot;) tabelaFranzosi2 |&gt; ggraph::ggraph() + geom_edge_fan(aes(colour = stat(&quot;Subject&quot;)), show.legend = TRUE) + geom_node_point(aes(size = &quot;Object&quot;, colour = &quot;Frequency&quot;), show.legend = TRUE) + # scale_colour_gradient(low = &quot;steelblue&quot;, high = &quot;black&quot;) + theme_graph(foreground = &#39;steelblue&#39;, fg_text_colour = &#39;yellow&#39;) + labs(title= &quot;Violência intragrupos durante o fascismo na Itália na primeira metade do século XX&quot;, caption= &quot;Fonte: compilado com base em Franzosi ()&quot;) ## Using `stress` as default layout ## Warning: Using size for a discrete variable is not advised. qgraph::qgraph(tabelaFranzosi[,c(1,3,4)]) novodf &lt;- tabelaFranzosi[,c(1,3,4)] # renomeando as colunas colnames(novodf) &lt;- c(&quot;vertex&quot;, &quot;edges&quot;, &quot;n&quot;) igraph::plot.igraph(novodf) 11.1.1 Clusterização EM CONSTRUÇÃO 11.2 Os pacotes ggraph e tidygraph . Construindo grafos com o tidyverse. O pacote ggraph é um pacote elaborado por Thomas Lin Pedersen, o mesmo do ggplot2, e pretende ser uma extensão deste, usando a mesma gramática de gráficos, o que nos dá grande flexibilidade visual. Com o ggraph é possível construir graficamente redes, mas ele vai além dos grafos, construindo também dendogramas, diferentes tipos de árvores, matrizes, gráficos hierárquicos, diagrama de arc, sunburst, etc. Para inserir os dados no ggraph, é necessário colocá-lo no formato “tidy” do “tidyverse”, e fazemos isso com o pacote tidygraph: Para instalar, usamos os comandos: install.packages(&#39;ggraph&#39;) install.packages(&#39;tidygraph&#39;) Carregando os pacotes library(ggraph) library(tidygraph) Seguindo o exemplo na página do criador do GGraph Thomas Lin Pedersen, vamos usar o dataset highschool do ggraph, que contém dados sobre a evolução da amizade entre garotos numa escola do ensino médio no Illinois, que nos anos de 1957 e 1958 responderam à pergunta: “Com que colegas desta escola você anda mais frequentemente?”. Esta pesquisa apareceu originalmente nos livros “Introduction to Mathematical Sociology” e “The Adolescent Society”, ambos do sociólogo James Coleman. Para obter mais informações sobre este dataset, basta digitar no console: help(highschool). str(highschool) # observando a estrutura do data frame ## &#39;data.frame&#39;: 506 obs. of 3 variables: ## $ from: num 1 1 1 1 1 2 2 3 3 4 ... ## $ to : num 14 15 21 54 55 21 22 9 15 5 ... ## $ year: num 1957 1957 1957 1957 1957 ... head(highschool,10) # observado as primeiras linhas do data frame ## from to year ## 1 1 14 1957 ## 2 1 15 1957 ## 3 1 21 1957 ## 4 1 54 1957 ## 5 1 55 1957 ## 6 2 21 1957 ## 7 2 22 1957 ## 8 3 9 1957 ## 9 3 15 1957 ## 10 4 5 1957 Vemos que há na coluna 1 de (“from”) pessoa número X para (“to”) para pessoa Y no ano (“year”). Assim, a pessoa 1 teve contato com as pessoas 14, 15, 21, 54 e 55 em 1957. A pessoa 2 teve contato com as pessoas 21 e 22, e assim por diante. Vamos preparar os dados para plotar o grafo com ggraph. Antes, é necessário colocá-lo no formato “tidy” do “tidyverse”, e fazemos isso com o pacote tidygraph. A função as_tbl_graph() do pacote tidygraph funciona como a função grouped_df(), que agrupa nós (nodes) e arestas (edges). as_tbl_graph(highschool) ## # A tbl_graph: 70 nodes and 506 edges ## # ## # A directed multigraph with 1 component ## # ## # Node Data: 70 × 1 (active) ## name ## &lt;chr&gt; ## 1 1 ## 2 2 ## 3 3 ## 4 4 ## 5 5 ## 6 6 ## # … with 64 more rows ## # ## # Edge Data: 506 × 3 ## from to year ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 13 1957 ## 2 1 14 1957 ## 3 1 20 1957 ## # … with 503 more rows Vamos criar uma lista chamada “grafo”, adicionando um campo novo com mutate(), de nome “Popularidade” e que vai medir a centralidade de grau através da função centrality_degree() do tidygraph. Centralidade de grau é a medida mais simples de centralidade, que conta o número de conexões, as arestas (“edges”) de cada nó. grafo &lt;- as_tbl_graph(highschool) %&gt;% mutate(Popularidade = centrality_degree(mode = &#39;in&#39;)) grafo ## # A tbl_graph: 70 nodes and 506 edges ## # ## # A directed multigraph with 1 component ## # ## # Node Data: 70 × 2 (active) ## name Popularidade ## &lt;chr&gt; &lt;dbl&gt; ## 1 1 2 ## 2 2 0 ## 3 3 0 ## 4 4 4 ## 5 5 5 ## 6 6 2 ## # … with 64 more rows ## # ## # Edge Data: 506 × 3 ## from to year ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 13 1957 ## 2 1 14 1957 ## 3 1 20 1957 ## # … with 503 more rows Plotando o grafo: ggraph(grafo, layout = &#39;kk&#39;) + geom_edge_fan(aes(colour = stat(index)), show.legend = FALSE) + geom_node_point(aes(size = Popularidade, colour= Popularidade), show.legend = TRUE) + scale_colour_gradient(low = &quot;steelblue&quot;, high = &quot;black&quot;) + facet_edges(~year) + theme_graph(foreground = &#39;steelblue&#39;, fg_text_colour = &#39;yellow&#39;) + labs(title = &quot;Evolução da amizade entre adolescentes de uma escola no Illinóis&quot;, caption = &quot;Fonte: Elaboração própria a partir dos dados de Coleman(1961) apud ggraph (v.2.0.5; Pedersen)&quot;) layout define como os nós serão alocados. O layout do ggraph possui os mesmos do igraph e outros mais como, hive plots, treemaps e circle packing. kk indica que está sendo usado o algortimo Kamada-Kawai para dispersar os nós e facilitar nossa visualização. geom_edge_fan() desenha os laços de modo curvo. Há diversas outras opções possíveis neste caso, como: geom_edge_arc, geom_edge_bend, geom_edge_diagonal,geom_edge_elbow, geom_edge_fan, geom_edge_hive, geom_edge_link, geom_edge_parallel. Substitua geom_edge_fan() por alguns estes e vjea a diferença no grafo. colour indica que a intensidade das ligações será por um gradiente de cor, stat() indica que segue ali uma informação estatística. geom_node_point() mostra os nós como pontos/círculos e permite que sejam plotados em diferentes tamanhos, cores e formas em aes(). size = Popularidade indica que o tamanho dos nós é controlado pela variável “Popularidade” que criamos. colour = Popularidade indica que além do tamanho, a cor também vaira conforme a popularidade. Para mais opções, digite ?geom_node_point() no console ou consulte a documentação do ggraph. scale_colour_gradient(low = \"steelblue\", high = \"black\") é opcional, podendo ser retirada. Especifica o gradiente de cores do comando anterior (no caso, nós), qual o valor mais baixo até o mais elevado. Sua informação é redundante com relação ao tamanho dos nós, mas ajuda a reforçar tal informação. facet_edges() função de “faceting”, de criar facetas, gráficos multiplos, e o símbolo de til ~ seguido de year indica que o critério aqui são as categorias dentro de ano, que no caso, são 1957 e 1958. Os nós são repetidos em cada painel. Caso tivéssesmos usado facet_edges(1957) teríamos o grafo apenas do ano 1957. theme_graph especifica as cores das legendas nas facetas labs(title = \" indica o título e caption o rodapé. Veja também cheatsheet/folha de dicas do ggraph Para ver os diferentes layouts possíves do ggraph e seus respectivos códigos, para além dos grafos clique aqui CAPÍTULO A SER EXPANDIDO 11.3 Redes de palavras EM CONSTRUÇÃO 11.4 Redes de citação EM CONSTRUÇÃO 11.5 Gráfico de centralidade EM CONSTRUÇÃO 11.6 Comunidades EM CONSTRUÇÃO 11.7 Sugestões de links AQUINO, Jackson A. “Análise de redes sociais”, capítulo 12 de ___. R para cientistas sociais. Ilhéus, BA: EDITUS, 2014. 157 p. ISBN: 978-85-7455-369-6. (PDF gratuito bem introdutório com R) HIGGINS, Silvio S.; RIBEIRO, Antônio Carlos. Análise de redes em Ciências Sociais. Brasília: ENAP. 2018. 229p. (PDF Gratuito de livro. Bom para aprender os conceitos/teorias básicos de análise de rede). Static and dynamic network visualization with R Manual online do ipgraph para R; PDF do Manual do igraph para R (ambos em inglês) ggraph Documentation no Cran. d’ANDRÉA, Carlos Frederico de Brito. Pesquisando plataformas online: conceitos e métodos. EDUFBA. 2020. A obra visa introduzir os Estudos de Plataforma, um campo de estudos que, desde o início da década de 2010, discute as especificidades políticas e materiais das mídias sociais e de outras plataformas online. Datificação, algoritmos, governança e os modelos de negócio das plataformas são algumas das dimensões sintetizadas no livro. De modo didático, o autor apresenta um conjunto de leituras e de experimentações metodológicas conduzidas com um diversificado grupo de colaboradoras(es) no país e no exterior. (ebook PDF e Epub gratuitos) RECUERO, Raquel. Introdução à análise de redes sociais online. EDUFBA.2017. A Análise de Redes Sociais (ARS) é uma abordagem de pesquisa cuja popularidade tem aumentado nos últimos anos, principalmente, entre os pesquisadores da área de Comunicação. É nesse âmbito que várias obras, entre artigos e livros, vêm surgindo e introduzindo o estudo dessas estruturas a partir da análise de redes e da compreensão da representação dessas redes sociais na internet. Este livro é uma pequena compilação dos principais conceitos e elementos para a compreensão e a aplicação da ARS. É baseado em uma breve apresentação e histórico do paradigma, os principais conceitos, suas métricas e, finalmente, suas formas de representação e visualização. (ebook PDF e Epub gratuitos). LIZARDO Omar; JILBERT Isaac. Social Networks: An Introduction. 2021. (ebook online) "],["KE.html", "12 Extração de palavras-chave - Key term ou keyword extraction 12.1 Atribuição de palavra chave Keyword assignment 12.2 Key term ou keyword extraction 12.3 Colocação e Coocorrência", " 12 Extração de palavras-chave - Key term ou keyword extraction Dado um ou mais documentos, dá se o nome de keyword - ou key phrases, key terms, key segments - extraction (KE), ou ainda keyword detection e keyword analysis à extração automatizada de termos que descrevem estes documentos, podendo ser extraídas palavras, frases ou segmentos. Além de extração de palavras-chave (keywords), a KE pode ser utilizada, por exemplo, para fazer um resumo automático de um texto. Pode ser usado para identificar palavras chave relevantes a um determinado assunto em uma massa de dados, como jornais/revistas científicas. “Extração de palavras-chave é definida como a tarefa que identifica automaticamente um conjunto de termos que melhor descrevem o assunto do documento” “Keyword extraction (KE) is defined as the task that automatically identifies a set of the terms that best describe the subject of document” (BELIGA, 2014) Há quem use o termo “keyphrase” sob o argumento de que as palavras chave são compostas de uma, duas ou mais palavras. Há duas abordagens no problema da geração automática de termos: (1) atribuição de palavras chave (keyword assignment), e (2) keyword extraction. Na atribuição de palavras chave as palavras chave são definidos previamente pelo pesquisador através de um vocabulário controlado de termos, e tais termos chave não precisam estar presentes no documento. Os documentos podem então ser classificados conforme estas categorias. Já na extração de palavras chave (KE), os termos são escolhidos pelo processo de automatização e estão previamente no texto. 12.1 Atribuição de palavra chave Keyword assignment Busca-se com a atribuição de palavra chave selecionar frases ou termos presentes em um vocabulário pré-definido pelo pesquisador. 12.2 Key term ou keyword extraction Como dito, extração de termos ou palavras-chave consiste na extração automatizada de termos contidos no texto, sem o uso de vocabulário controlado externo. Segundo Beliga (2014) como exemplos de key word extraction temos: métodos estatísticos não supervisionados, como TF-IDF, KP-Miner, RAKE (Rapid Automatic Keyword Extraction). métodos baseados em grafos, como TextRank, SingleRank, ExpandRank TopicRank, PositionRank, TopologicalPageRank e MultipartitieRank. método supervisionado, como o KEA. Além destes, pode-se destacar ainda os seguintes algoritmos de KE: Os ngrams podem ser considerados uma forma de extrair palavras chave, bem como Parts-of-Speech tagging, ou POS (por exemplo, pegando os substantivos mais frequentes), colocação e Coocorrências, PAT tree (Patricia Tree), YAKE! (Yet Another Keyword Extractor), Selectivity-Based Keyword Extraction, Yum!, GenEx (o nome é uma junção entre Genitor, um algoritmo genético e Extractor, um algoritmo de extração de palavras chave parametrizado), KeyBERT. Os algoritmos GenEx e Kea estabeleceram a fundação dos métodos de extração de termos que vieram depois. Dicas Sobre o KEA. I. H. Witten, G. W. Paynter, E. Frank, C. Gutwin, C. G. Nevill-Manning, “Kea: Practical Automatic Keyphrase Extraction” in Proc. of the 4th ACM Conf. of the Digital Libraries, Berkeley, CA, USA, 1999. resumo dos diferentes métodos de KE O artigo que apresentou o GenEx: P. D. Turney, “Learning to Extract Keyphrases from Text” in Tech. Report, National Research Council of Canada, Institute for Information Technology, 1999. Chamamos de métodos não supervisionados aqueles que não utilizam fontes externas, utilizam somente os textos, documentos, etc. Chamamos de métodos supervisionados aqueles que utilizam alguma fonte externa - como modelos pré-treinados ou dicionários - para extração de palavras chave. 12.2.1 TF-IDF: Term-Frequency Inverse Document Frequency A “frequência do termo–inverso da frequência nos documentos”, do inglês “Term-Frequency Inverse Document Frequency”, ou “TF-IDF” é utilizado para medir a relevância de palavras em uma série de documentos. Para funcionar, requer que existam vários documentos, ou textos, ou capítulos, etc. Neste algoritmo, as palavras que aparecem em todos ou em muitos documentos - como as stopwords - serão “penalizadas” e terão pontuação baixa. Agora, se uma palavra aparece bastante em um documento, mas não em outros, terá pontuação alta, e isto pode indicar que seja relevante, significativa para entender a peculiaridade daquele documento/texto. TF-IDF é útil num processo chamado de keyword extraction” ou “extração de palavras chave”. Vamos para um exemplo: Documento 1: “Eu quero abacaxi” Documento 2: “Eu? Eu quero banana” Frequência de termos, ou TF, representa a proporção que uma palavra tem no documento em questão. Esta frequência pode ser apresentada no formato de matriz. Frequência Doc 1 Doc2 Eu 1 2 quero 1 1 abacaxi 1 0 banana 0 1 total palavras 3 4 12.2.1.1 TF: Frequência de termos (Term Frequency) O TF de um termo que ocorre em um documento é calculado da seguinte maneira: \\(tf(t,d)\\) : contagem de t(termo) em d(documento) / número de palavras no documento O documento 1 possui 3 palavras, o documento 2 possui 4 palavras, portanto, a frequência de termos (TF) fica assim: TF Doc 1 Doc2 Eu 1 / 3 = 0,33333 2 / 4 = 0,5 quero 1 / 3 = 0,33333 1 / 4 = 0,25 abacaxi 1 / 3 = 0,33333 0 banana 0 1 / 4 = 0,25 Total 1 1 No Documento 1, temos 3 palavras no total, cada uma, por ser única no documento, possui TF de 1/3, ou 0,33333. O Documento 2 possui 4 palavras no total. “quero” e “banana” possui um TF de 1/4 cada, ou 0,25, enquanto “eu”, que apareceu duas vezes, possui TF de 2/4 ou 0,5. TF me diz o quão frequente é uma palavra/termo em um documento. Isto pode ser feito em números absolutos bem como em termos proporcionais (bom olhar as documentações dos pacotes para entender qual o padrão utilizado) 12.2.1.2 IDF: Inverse Document Frequency IDF, inverse document frequency, mostra o peso de um termo em relação à coleção total de documentos/textos, dando um valor baixo para termos frequentes em todos os documentos e que por isso são pouco informativos sobre as peculiaridades daquele documento, bem como privilegia termos frequentes em poucos documentos. Assim, no nosso exemplo, as palavras “eu” e “quero” estão presentes em dois documentos de um total de dois documentos, tendo o IDF de 0. Já “abacaxi” e “banana”, termos que aparecem uma vez e somente em um documento cada, possuem IDF de 0,30102. O cálculo é feito da seguinte forma: Número de documentos no corpus (no caso acima, dois), dividido pelo número de documentos onde o termo aparece. Se o termo aparece uma vez somente ou 50 vezes em um documento, em ambos os casos será computado como um. O resultado disto é posto num logaritmo. IDF Eu Log(2/2) = 0 quero Log(2/2) = 0 abacaxi Log(2/1) = 0,30102 banana Log(2/1) = 0,30102 Com o IDF sabemos quais termos ocorrem em vários documentos e os que ocorrem em poucos. Para saber o peso de cada termo em cada documento, usamos então o TF-IDF. 12.2.1.3 Calculando TF-IDF TF-IDF é a multiplicação dos dois termos, TF * IDF. Ao multiplicar TF por IDF, obtemos o score da palavra no documento. TF Doc 1 TF Doc2 IDF TF-IDF Doc1 TF-IDF Doc2 Eu 1 / 3 = 0,33333 2 / 4 = 0,5 Log(2/2) = 0 0,33333 * 0 = 0 0,5 * 0 = 0 quero 1 / 3 = 0,33333 1 / 4 = 0,25 Log(2/2) = 0 0,33333 * 0 = 0 0,5 * 0 = 0 abacaxi 1 / 3 = 0,33333 0 Log(2/1) = 0,301 0,3 * 0,3 = 0,1003 0 * 0,3 = 0 banana 0 1 / 4 = 0,25 Log(2/1) = 0,301 0 * 0,3 = 0 0,25 * 0,3 = 0,0752 Matemática Esta é a fórmula do TF-IDF, e nos retorna o índice TF-IDF para cada palavra em cada documento. \\(W_{ij} = tf_{i,j} \\times \\log(\\frac{N}{df_i})\\) Destrinchando a fórmula: Fórmula Descrição \\(W_{ij}\\) um termo \\(i\\) num documento \\(j\\), para o qual vamos calcular o TF-IDF \\(tf_{i,j}\\) frequência do termo \\(i\\), no documento \\(j\\) \\(df_{ij}\\) Número de documentos que contenham o termo \\(i\\). Pouco importa se aparece apenas uma vez ou se 500 vezes num mesmo documento, seu valor em cada documento, se presente, é 1 \\(N\\) Número total de documentos \\(df_i\\) frequência de documentos que contenham o termo \\(i\\) Se o resultado encontrado se aproximar de “0”, então a palavra se encontra presente em vários documentos. Caso contrário, quanto mais se aproxima de “1”, mais rara é esta palavra em outros documentos e mais concentrada em poucos documentos. Vale ressaltar que o cálculo TF-IDF pode ser feito tanto com a frequência absoluta ou como com a relativa. Vimos exemplo de TF-IDF com apenas dois “documentos”. Vamos usar mais documentos para entender melhor o TF-IDF. Usamos um pacote de R nos bastidores para gerar a tabela à seguir, mas veremos o código que o gerou mais à frente. doc1 &lt;- &quot;Eu quero abacaxi!&quot; doc2 &lt;- &quot;Eu quero açaí!&quot; doc3 &lt;- &quot;Eu quero manga ou açaí! Eu quero manga! Manga!&quot; O exemplo acima possui diferentes configurações de palavras para observarmos o TF-IDF: “eu” e “quero” em todos os docs “abacaxi”, “manga” e “ou” que ocorrem uma vez. “açaí” que ocorre uma vez em dois documentos diferentes. “manga” que aparece várias vezes em um documento somente. ## Document-feature matrix of: 3 documents, 6 features (38.89% sparse) and 0 docvars. ## features ## docs eu quero abacaxi açaí manga ou ## text1 0 0 0.1590404 0 0 0 ## text2 0 0 0 0.05869709 0 0 ## text3 0 0 0 0.01956570 0.1590404 0.05301347 “eu” e “quero” ocorrem em todos os docs e possuem TF-IDF de 0 em todos os casos. “abacaxi” aparece somente na primeira frase e tem TF-IDF de 0.1590404 “açaí” ocorre uma vez em dois documentos possui TF-IDF em um doc com menos palavras no total. “ou” e “manga” só aparecem na frase 3, e manga aparece 3 vezes e tem TF-IDF maior que a palavra “ou”, que só aparece uma vez. No doc3, “ou” (que só aparece uma vez em um doc) possui TF-IDF maior que “açaí”, que aparece em mais de um doc. Podemos realizar o TF-IDF no R calculando manualmente, como neste exemplo em video ou neste tutorial, ou podemos usar alguns dos vários pacotes que tem já implementadas a função, como o Tidytext, Quanteda e TM, que veremos a seguir. Vale atentar que cálculos feitos com diferentes pacotes podem não bater entre si. Se for este o caso, atente para se usam frequência absoluta ou relativa, e qual a base do Logaritmo utilizado (se de base 2 ou 10). Em um exemplo real de uso de TF-IDF, este tutorial usou TF-IDF entre diferentes livros do Harry Potter: Fonte: Text Mining: Term vs. Document Frequency do AFIT Data Science Lab R Programming Guide 12.2.1.4 TF-IDF no R: Tidyverse Podemos realizar o TF-IDF no R com o tidytext com a função tidytext::bind_tf_idf. doc1 &lt;- &quot;Eu quero abacaxi&quot; doc2 &lt;- &quot;Eu? Eu quero banana&quot; # criando o data frame, onde cada linha é um documento. df &lt;- data.frame(&quot;texto&quot; = c(doc1,doc2), # ID de &quot;identificação&quot; &quot;ID&quot; = c(1,2), stringsAsFactors = F) df %&gt;% # quebrando o texto em tokens tidytext::unnest_tokens(output = &#39;word&#39;, token = &#39;words&#39;, # input = nome da coluna do dataframe input = texto) %&gt;% # contando os termos dplyr::count(ID, word, sort = TRUE) %&gt;% # TF-IDF tidytext::bind_tf_idf(word, ID, n) ## ID word n tf idf tf_idf ## 1 2 eu 2 0.5000000 0.0000000 0.0000000 ## 2 1 abacaxi 1 0.3333333 0.6931472 0.2310491 ## 3 1 eu 1 0.3333333 0.0000000 0.0000000 ## 4 1 quero 1 0.3333333 0.0000000 0.0000000 ## 5 2 banana 1 0.2500000 0.6931472 0.1732868 ## 6 2 quero 1 0.2500000 0.0000000 0.0000000 Em bind_tf_idf() sendo: word a coluna contendo termos, ID a coluna contendo os IDs dos docs, n a contagem de palavras produzido por count(). Dicas Exemplo/tutorial de TF-IDF com o tidyverse de Julia Silge e David Robinson. 12.2.1.5 TF-IDF no R: Quanteda TF-IDF com o pacote Quanteda é usado com a função quanteda::dfm_tfidf doc1 &lt;- &quot;Eu quero Elias&quot; doc2 &lt;- &quot;Eu? Eu quero Durkheim&quot; # criando um vetor com documentos para transformar em um corpus no quanteda meuvetor &lt;- c(doc1,doc2) # criando um objeto tipo corpus a partir do vetor meucorpus &lt;- quanteda::corpus(meuvetor) # criando uma matriz de frequência meudfm &lt;- meucorpus %&gt;% # quebrando em tokens quanteda::tokens( # removendo a pontuação remove_punct = TRUE) %&gt;% # transformando em um document frame matrix quanteda::dfm() meudfm ## Document-feature matrix of: 2 documents, 4 features (25.00% sparse) and 0 docvars. ## features ## docs eu quero elias durkheim ## text1 1 1 1 0 ## text2 2 1 0 1 # gerando o tf-idf (frequencia absoluta) quanteda::dfm_tfidf(meudfm) ## Document-feature matrix of: 2 documents, 4 features (25.00% sparse) and 0 docvars. ## features ## docs eu quero elias durkheim ## text1 0 0 0.30103 0 ## text2 0 0 0 0.30103 # TF-IDF usando frequência relativa, proporcional quanteda::dfm_tfidf(meudfm, scheme_tf = &quot;prop&quot;) ## Document-feature matrix of: 2 documents, 4 features (25.00% sparse) and 0 docvars. ## features ## docs eu quero elias durkheim ## text1 0 0 0.1003433 0 ## text2 0 0 0 0.0752575 quanteda::dfm_tfidf(x, scheme_tf = “count”, scheme_df = “inverse”, base = 2) Parâmetro Descrição x objeto de entrada, devendo ser um document-feature matrix scheme_tf esquema para dfm_weight(); sendo o padrão “count”. Para usar a frequência relativa, usa-se o “prop” scheme_df esquema para docfreq(); sendo o padrão “inverse”. base A base para logaritmo no dfm_weight() e docfreq(), sendo 10 o valor padrão. Outro valor comum é 2 12.2.1.6 TF-IDF no R: TM Vamos usar agora o pacote TM. doc1 &lt;- &quot;Eu quero Durkheim&quot; doc2 &lt;- &quot;Elias! Eu quero Elias&quot; vetor_vetores &lt;- c(doc1,doc2) # criando o objeto tipo corpus para o TM meu_corpus &lt;- tm::Corpus(tm::VectorSource(vetor_vetores)) # observando a estrutura do objeto criado, que é uma lista str(meu_corpus) ## Classes &#39;SimpleCorpus&#39;, &#39;Corpus&#39; hidden list of 3 ## $ content: chr [1:2] &quot;Eu quero Durkheim&quot; &quot;Elias! Eu quero Elias&quot; ## $ meta :List of 1 ## ..$ language: chr &quot;en&quot; ## ..- attr(*, &quot;class&quot;)= chr &quot;CorpusMeta&quot; ## $ dmeta :&#39;data.frame&#39;: 2 obs. of 0 variables Vamos a um pré-processamento do pacote tm com a função tm_map() e tm::removePunctuation. meu_corpus2 &lt;- # passando tudo para minúsculo tm::tm_map(meu_corpus, tolower) %&gt;% # removendo pontuações tm::tm_map(., tm::removePunctuation) ## Warning in tm_map.SimpleCorpus(meu_corpus, tolower): transformation drops ## documents ## Warning in tm_map.SimpleCorpus(., tm::removePunctuation): transformation drops ## documents meu_corpus2 ## &lt;&lt;SimpleCorpus&gt;&gt; ## Metadata: corpus specific: 1, document level (indexed): 0 ## Content: documents: 2 A matriz de termo por documento (document-term-matrix) computa quantas vezes um termo aparece por documento, que no nosso caso foi uma frase simples. “Durkheim” aparece uma vez no documento 1, “quero” aparece uma vez em cada documento e “Elias” aparece duas vezes no documento 2. Opção 1, mais simples dtm.tfidf &lt;- tm::DocumentTermMatrix(meu_corpus2, control = list(weighting = tm::weightTfIdf)) ## Warning in TermDocumentMatrix.SimpleCorpus(x, control): custom functions are ## ignored # vendo a estrutura dtm.tfidf ## &lt;&lt;DocumentTermMatrix (documents: 2, terms: 3)&gt;&gt; ## Non-/sparse entries: 2/4 ## Sparsity : 67% ## Maximal term length: 8 ## Weighting : term frequency - inverse document frequency (normalized) (tf-idf) # Para visualizar, transformamos nosso objeto em matriz as.matrix(dtm.tfidf) ## Terms ## Docs durkheim quero elias ## 1 0.5 0 0.0000000 ## 2 0.0 0 0.6666667 Opção 2: com normalização e retirada de stopwords doc1 &lt;- &quot;Eu quero Durkheim&quot; doc2 &lt;- &quot;Elias! Eu quero Elias&quot; vetor.docs &lt;- c(doc1,doc2) # criando o objeto tipo corpus para o TM # como nossa fonte são vetores, usamos VectorSource meu_corpus &lt;- tm::Corpus(tm::VectorSource(vetor.docs)) tm::DocumentTermMatrix(meu_corpus, # para concatenar várias transformacoes, vamos usar function control = list(weighting = function(x) tm::weightTfIdf(x, normalize = T), removePunctuation = TRUE, stopwords = TRUE)) %&gt;% as.matrix ## Warning in TermDocumentMatrix.SimpleCorpus(x, control): custom functions are ## ignored ## Terms ## Docs durkheim quero elias ## 1 0.5 0 0.0000000 ## 2 0.0 0 0.6666667 Dicas TF-IDF JONES, Karen Spärck. A statistical interpretation of term specificity and its application in retrieval. Journal of Documentation. (1972) Cap. 6.5 TF-IDF: Weighing terms in the vector in JURAFSKI,D; MARTIN,J. Speech and Language Processing. Gerard Salton and Christopher Buckley (1988). Term-weighting approaches in automatic text retrieval. Information Processing and Management, 24/5, 513-523. Tutorial vignette (exemplo) de TF-IDF com o tidytext Análise tf-idf com livros do Harry Potter usando o Tidyverse. Vídeos: Video em português Aula 5.4: Problema com Matrizes de Frequência e TF-IDF | Processamento de Língua Natural. do curso de “Processamento de Língua Natural” (LIG948B), ministrado na Faculdade de Letras da Universidade Federal de Minas Gerais (FALE-UFMG) em Python. Video tutorial de tf-idf no R (em inglês) “TF-IDF | Introduction to Text Analytics with R Part 5” do Data Science Dojo, fazendo o cálculo sem pacotes. 12.3 Colocação e Coocorrência Uma das vantagens de tokenizers, em relação à grande maioria dos tokenizadores - exceção ao Quanteda - é a possibilidade de criar ngrams de diferentes tamanhos, ao mesmo tempo, alterando apenas um parâmetro, sem demandar novas linhas de código. Esta facilidade se encontra também no pacote Quanteda (ambos pacotes possuem criadores em comum). Isto é chamado de “multi-word units” {#multiword_units}, ou “colocação” (collocation”) {#colocation} na linguística. São tokens que aparecem juntos muito frequentemente, como em nomes próprios ou junções frequentes entre substantivo e Por exemplo, “Minas Gerais”, “Machine Learning”, “ódio mortal”, “cena do crime”. É útil considerá-los como uma palavra só; O Quanteda, além de também realizar a tokenização “multi words”, oferece também as funções textstat_collocations() que nos retorna uma tabela com cáculo estatístico identificando colocações e a função tokens_compound() que adiciona um underscore entre as palavras da colocação, de modo que formem um único vocábulo, por exemplo, “machine_learning”, “New_York”. txt.raizesBr &lt;- &#39;A democracia no Brasil foi sempre um lamentável mal-entendido. Uma aristocracia rural e semifeudal importou-a e tratou de acomodá-la, onde fosse possível, aos seus direitos ou privilégios, os mesmos privilégios que tinham sido, no Velho Mundo, o alvo da luta da burguesia contra os aristocratas. E assim puderam incorporar à situação tradicional, ao menos como fachada ou decoração externa, alguns lemas que pareciam os mais acertados para a época e eram exaltados nos livros e discursos. É curioso notar-se que os movimentos aparentemente reformadores, no Brasil, partiram quase sempre de cima para baixo: foram de inspiração intelectual, se assim se pode dizer, tanto quanto sentimental. Nossa independência, as conquistas liberais que fizemos durante o decurso de nossa evolução política vieram quase de surpresa; a grande massa do povo recebeu-as com displicência, ou hostilidade. Não emanavam de uma predisposição espiritual e emotiva particular, de uma concepção da vida bem definida e específica, que tivesse chegado à maturidade plena. Os campeões das novas idéias esqueceram-se, com freqüência, de que as formas de vida nem sempre são expressões do arbítrio pessoal, não se &quot;fazem&quot; ou &quot;desfazem&quot; por decreto. A célebre carta de Aristides Lobo sobre o 15 de Novembro é documento flagrante do imprevisto que representou para nós, a despeito de toda a propaganda, de toda a popularidade entre os moços das academias, a realização da idéia republicana. &quot;Por ora&quot;, dizia o célebre paredro do novo regime, &quot;por ora a cor do governo é puramente militar e deverá ser assim. O fato foi deles, deles só, porque a colaboração de elemento civil foi quase nula. O povo assistiu àquilo bestializado, atônito, surpreso, sem conhecer o que significava.&#39; 12.3.1 Colocação com o Quanteda Vejamos um exemplo: corp &lt;- data_corpus_inaugural[1:2] head(cols &lt;- quanteda.textstats::textstat_collocations(corp, size = 2, min_count = 2), 10) ## collocation count count_nested length lambda z ## 1 have been 5 0 2 5.704259 7.354588 ## 2 has been 3 0 2 5.565217 6.409333 ## 3 of the 24 0 2 1.673501 6.382475 ## 4 i have 5 0 2 3.743580 6.268303 ## 5 which i 6 0 2 3.172217 6.135144 ## 6 will be 4 0 2 3.868500 5.930143 ## 7 less than 2 0 2 6.279494 5.529680 ## 8 public good 2 0 2 6.279494 5.529680 ## 9 you will 2 0 2 4.917893 5.431752 ## 10 may be 3 0 2 4.190711 5.328038 head(cols &lt;- quanteda.textstats::textstat_collocations(txt.raizesBr, size = 2, min_count = 2), 10) ## collocation count count_nested length lambda z ## 1 no brasil 2 0 2 6.626718 3.781401 ## 2 por ora 2 0 2 6.626718 3.781401 ## 3 toda a 2 0 2 5.770551 3.518257 ## 4 de toda 2 0 2 4.456670 2.827344 ## 5 de uma 2 0 2 4.456670 2.827344 quanteda.textstats::textstat_collocations(txt.raizesBr, size = 2, min_count = 2) ## collocation count count_nested length lambda z ## 1 no brasil 2 0 2 6.626718 3.781401 ## 2 por ora 2 0 2 6.626718 3.781401 ## 3 toda a 2 0 2 5.770551 3.518257 ## 4 de toda 2 0 2 4.456670 2.827344 ## 5 de uma 2 0 2 4.456670 2.827344 EM EXPANSÃO Este trecho será expandido 12.3.2 Colocação com Udpipe EM EXPANSÃO Este trecho será expandido Manning, C. D., Raghavan, P., &amp; Schütze, H. Introduction to Information Retrieval. Cambridge: Cambridge University Press. 2008. Dicas Video no YouTube OpenAI Outperforms Some Humans In Article Summarization! sobre um algoritmo de sumarização de textos, de 30 de março de 2021 sobre o paper “Learning to Summarize with Human Feedback”.. September 4, 2020. "],["links-úteis.html", "13 Links úteis 13.1 Humanidades digitais 13.2 Análise de redes sociais 13.3 Textos sobre análise textual 13.4 Processamento Linguagem Natural (PLN ou NLP) 13.5 Revistas / Journals acadêmicos 13.6 Dados Abertos 13.7 Vídeos 13.8 Sites / Blogs 13.9 Organizações 13.10 Estatística 13.11 Podcasts 13.12 Links Programação 13.13 Links de Cursos Online 13.14 Grupos de discussão/Forum 13.15 Links de folhas de dicas (Cheat-sheets)", " 13 Links úteis Segue aqui uma lista com links para livros, artigos, sites, manutais, tutoriais, vídeos e canais de vídeos. A maioria e a prioridade é de material gratuito, sobre temas gerais relacionados a humanidades digitais e em especial, à sociologia. Há também sugestões mais focadas nos assuntos específicos nos capítulos. O critério de seleção aqui foi o de contribuição para esclarecer o potencial das humanidades digitais para quem inicia na área de humanidades digitais/sociologia digital. Os itens não foram selecionados tanto pela contribuição científica, mas com base no critério de exemplos que podem ajudar a esclarecer o potencial das humanidades digitais para quem inicia. 13.1 Humanidades digitais Introduction to Digital Humanities Textbook A digital textbook designed for UCLA’s Introduction to Digital Humanities course website. (ebook disponível gratuito online) EDMOND, Jennifer Digital Technology and the Practices of Humanities Research. (Livro gratuito) The Data Journalism Handbook. Towards a Critical Data Practice. 415p. 2021. ISBN 9789048542079. (Livro GRATUITO). BAUER, Paul C. Computational Social Science: Theory &amp; Application. (livro online grátis). 13.2 Análise de redes sociais ver - Ver sugestões Ver sugestões no capítulo “Análise de Redes Sociais”. 13.3 Textos sobre análise textual GRIMMER, Justin.STEWART, Brandom. Text as Data: The Promise and Pitfalls of Automatic ContentAnalysis Methods for Political Texts. Political Analysis(2013) pp. 1–31. doi:10.1093/pan/mps028. CASTELFRANCHI, Yurij. A análise de textos auxiliada pelo computador: um laboratório a céu aberto para as ciências sociais. Journal of Science Communication 16(02)(2017)C04 TREADWELL, Donald. Content Analysis: Understanding Text and Image in Numbers. Understanding Text and Image in Numbers. In __ Introducing Communication Research: paths of Inquiry. Sage. 2014. (Capítulo sobre análise de conteúdo) 13.3.1 Sociologia Digital / Ciências Sociais Computacionais KATEMBERA, Serge. “Sociologia digital ou sociologia do digital?” V. 2 N. 1 (2020): Dossiê Ambiente e Sociedade. (Artigo) NASCIMENTO, Leonardo F. Sociologia digital : uma breve introdução. EDUFBA. 2020. (Livro gratuito em PDF e epub) Podcast “New Work in Digital Humanities”. Episódio: Neil Selwyn, “What is Digital Sociology?” (Polity, 2019) BERNAU, John A. Text Analysis with JSTOR Archives. November 12, 2018 https://doi.org/10.1177/2378023118809264. (Aplicação de métodos de humanidades digitais, usando a base do Jstor, para uma rápida análise de variação temática em revistas sociológicas). FUSSEY, Pete e ROTH, Silke (ed.) Digitizing Sociology. edição da revista “Sociology” da British Sociological Association. Evans J, Foster JG. Computation and the Sociological Imagination. Contexts. 2019;18(4):10-15. doi:10.1177/1536504219883850 Text Mining For Historians do International Standing Working Group in Medialization and Empowerment at the German Historical Institute London. 13.3.1.1 Netnografia, etnografia digital MARKHAM, Annette N. 2013. Fieldwork in Social Media. Qualitative Communication Research 2, 4 (Dec. 2013), 434–446. https://doi.org/10.1525/qcr.2013.2.4.434 SHAFFER, David Williamson. Quantitative Etnography. Boswell Press. 2017. “This is a book about understanding why, in the digital age, the old distinctions between qualitative and quantitative research methods, between the sciences and humanities, and between numbers and understanding, limit the kinds of questions we can ask, in some cases, and lead us accept superficial answers in others. Quantitative Ethnography is a research method that goes beyond those distinctions to help us understand how to make sense of our increasingly data-rich world…”. (PDF da Introdução disponível gratuitamente) 13.4 Processamento Linguagem Natural (PLN ou NLP) JURAFSKY, D.; MARTIN, J. Speech and language processing: An introduction to speech recognition, computational linguistics and natural language processing. Upper Saddle River, NJ: Prentice Hall, 2020. link pdf dos capítulos individuais, link livro completo. (Um manual bastante extenso e mais teórico sobre PLN) BIRD, S.; KLEIN, E.; LOPER., E. Natural language processing with Python – analyzing text with the natural language toolkit. (Livro online gratuito, baseado em Python 3.) Playlist no Youtube das aulas da Stanford University sobre PLN 13.5 Revistas / Journals acadêmicos SIGCAS Computers and Society da Association for Computing Machinery Digital Scholarship in the Humanities. Revista da Universidade de Oxford. Sessão com artigos gratuitos Journal: Digital humanities Quartely International Journal of Digital Humanities Socius: Sociological Research for a Dynamic World. Special Collection: Data Visualization. Jornal de acesso livre International Journal of Humanities and Arts Computing (Universidade de Edimburgo) magazén International Journal for Digital and Public Humanities. (open access). Revista do Dipartimento di Studi Umanistici da Università Ca’ Foscari de Veneza Journal of Digital Social Research. Open Access Reviews in Digital Humanities. (O Review já não publica desde 2014) Journal of Digital Humanities. (O Journal of DH tem uma sessão de resenha de novas ferramentas disponíveis). Journal of Cultural Analytics. Department of Languages, Literatures, and Cultures at McGill University, Canada. open-access journal dedicated to the computational study of culture 13.6 Dados Abertos Episódio 005 do Podcast “História FM Acesso à informação: como fazer uso da Lei de Acesso? com Maria Vitória Ramos e Luiz Fernando Toledo, do projeto Fiquem Sabendo. Ou ainda no Spotify. 5 estrelas dos dados abertos (Tim Berners-Lee, criador do termo “dados abertos”. Site explica o que são dados abertos e seus 5 níveis. Em português) Busca do Google por base de dados Abertos Dados abertos. (Se não sabe onde encontrar algum dado específico que você procura, veja aqui) SHIKIDA, Claudio D., MONASTERIO, Leonardo, NER, Pedro Fernando. Guia Brasileiro de Análise de Dados: Armadilhas e Soluções. Brasília. 2021. ( Tópicos: Causalidade, Pobreza e Desigualdade, Análise de dados em Saúde, Educação, Crimes e Violência, Macroeconomia, Mercado e Trabalho e Opinião Pública.) Dados Abertos: Fórum de discussão. Workshop do Henrique Xavier, no canal “Base dos Dados” sobre como explorar os dados do Diário Oficial da União. Como usar a biblioteca basedosdados no R. Pacote basedosdados do R para baixar diversas bases de dados. Data sets do Brasil.io, um “repositório de dados públicos disponibilizados em formato acessível” como “Eleições Brasil”, “Cursos e notas de corte do Prouni 2018”, gastos de deputados e do governo federal, salários de magistrados, etc. 13.7 Vídeos Vídeos do 1º Summerschool of Digital Humanities da Universidade de Heidelberg, Alemanha, ocorrido em 2017. link. (Em inglês) 13.8 Sites / Blogs Text analysis info Textual Analysis - University of Notre Dame Site Digital Humanities Now. (Agrega oportunidade de emprego, notícias, bolsas de pesquisa). site Go Digital Humanities. (Novidades sobre humanidades digitais, como eventos). Blog Digital Society Blog. (Blog do Institut für Internet und Gesellschaft da Alexander von Humboldt). Site The Programming Historian, com diverso conteúdos em português. 13.9 Organizações Alliance of Digital Humanities Organizations (ADHO). Global Network of Internet and Society Research Centers (NoC) i. (Catálogo com grupos de pesquisa sobre internet e sociedade ao redor do mundo). SICSS - The Summer Institutes in Computational Social Science. “The purpose of the Summer Institutes is to bring together graduate students, postdoctoral researchers, and beginning faculty interested in computational social science. The Summer Institutes are for both social scientists (broadly conceived) and data scientists (broadly conceived).” The Computational Social Science Society of the Americas. “(CSSSA) is a professional society that aims to advance the field of computational social science in all its areas, including basic and applied orientations, by holding conferences and workshops, promoting standards of scientific excellence in research, teaching, and publishing research findings and results.” 13.10 Estatística Statistics For Historians do International Standing Working Group in Medialization and Empowerment at the German Historical Institute London. 13.11 Podcasts Podcasts em português: Pizza de Dados. Podcast em portugês sobre ciência de dados. Let’s data. Podcast em português sobre ciência de dados e inteligência artificial. Links: Youtube, google podcast. Data hackers podcast de datascience. CTRL Enter Podcast dedicado a dados abertos, com alguns episódios focados em papers de análise empírica. Cidades Abertas Podcast sobre dados abertos e transparência. Podcasts em inglês: Digital Sociology Podcast. Diaries of Social Data Research New Work in Digital Humanities. New Books Network. Entrevistas com humanistas digitais acerca de seus trabalhos recentes. Episódio de Complexity: Peter Dodds on Text-Based Timeline Analysis &amp; New Instruments for The Science of Stories Lista com mais podcasts dedicados às digital humanities Podcast r-podcast 13.12 Links Programação 13.12.1 R introdução LENTE, Caio. Zen do R. “O objetivo deste livro é ensinar ao leitor que não costuma programar algumas formas simples de melhorar a organização de seus projetos de análise de dados em R”. (Livro online gratuito). AQUINO, Jackson A.. R para cientistas sociais. Ilhéus, BA: EDITUS, 2014. 157 p. ISBN: 978-85-7455-369-6. (PDF gratuito) Livro Curso-R (em construção) Blog: Curso-R KUBRUSLY, Jessica. Uma Introdução à Programação com o R. (ebook online em português) FREIRE, Sergio Miranda. Introdução ao R. (Livro online, no formato bookdown) WICKHAM, Hadley. Advanced R. 2nd edition. (Livro online gratuito, no formato bookdown, em inglês, para entender melhor os conceitos do R). 13.12.2 R e estatística FERREIRA, E.B.;. de OLIVEIRA, M.S. Introdução à Estatística com R. Unifal. 2020. (PDF gratuito). Gareth James; Daniela Witten; Trevor Hastie; Robert Tibshirani. An Introduction to Statistical Learning with Applications in R. Segunda edição. 2021. Possui conteúdo sobre regressões, métodos de classificação, deep learning, aprendizado não supervisionado, K-means, Naives Bayes. PDF gratuito 13.12.3 R - tópicos específicos WICKHAM, H. GROLEMUND, Garret. R for Data Science. O’Reilly. 2017. (Livro Online, em inglês). IRIZARRY, Rafael A. Introduction to Data Science. Livro Online, em inglês, feito como notas de aulas da HarvardX Data Science Series CLARK, Michael. R &amp; Social Science. Getting Started with applied use of R in the Social Sciences. (pequeno manual em PDF da Universidade de Notre Dame) ggplot2 on-line version of work-in-progress 3rd edition of “ggplot2: elegant graphics for data analysis” SILGE, Julia, ROBINSON, David Text Mining with R. (ebook online) RYDBERG-COX, Jeff. Statistical Methods for Studying Literature Using R da Universidade Missouri-Kansas City. R-tutor- An R Introduction to Statistics. Site com tutoriais diversos sobre R. Em Inglês. Diversos links para livros gratuitos de R no bookdown. 13.12.4 R - avançado WICHKAM, Hadley (2014). Advanced R. Ed: Chapman and Hall/CRC. Livro gratuito. 13.12.5 Links Python Aprenda Computação com Python. Livro online em português de introdução ao Python. A Whirlwind Tour of Python. PDF disponível gratuitamente. HEINOLD, Brian. A Practical Introduction to Python Programming. 2012. 263 p. (PDF de introdução ao Python) Jake VanderPlas. Python Data Science Handbook: Essential Tools for Working with data. (livro online). 13.12.6 Python - tópicos específicos KARSDORP, Folgert. Python Programming for the Humanities. Livro online. Pyhton Humanities. Site com introdução às DH em Python, com alguns tutoriais. Constellate What do you want to learn today?. (Projeto Constellate da JStor junto a diversas universidades. Conta com sessão com tutoriais sobre análise textual, a maioria em Python. 13.13 Links de Cursos Online EdX, Cursos online de Universidades como Harvard, MIT, etc. gratuitos como ouvinte. Paga-se pelo certificado. Há cursos grautuitos, como “Introducing Text Analytics and Natural Language Processing with Python”, “Introduction to Digital Humanities”, “Data Science: Visualization”, “Using Python for Research”. DataCamp. Cursos mais práticos, em R, Python, SQL, com exercícios, totalmente online (não é necessário instalar nada em seu computador). As partes iniciais dos cursos costumam ser gratuitas, mas há parte paga com anuidades. Udemy Possui cursos gratuitos e cursos pagos. É possível encontrar cursos pagos por volta de R$20,00. Coursera. Possui parceria com mais de 200 universidades e empresas como o Google e IBM. Cognitive Class Da IBM, Cursos gratuitos em data science, alguns gratuitos com certificado. Cursos em inglês e espanhol. Possui cursos como “Data Visualization with R”. Big Data University. Cursos em Portugues. Curso gratuito de Estatística e probabilidade da Khan Academy, em português. Inclui teste de hipótese, regressão. Curso-R em português, com cursos sobre Visualização, Machine learning, Web Scraping, Programação para Ciência de Dados, Dashbord com Shiny, etc. 13.14 Grupos de discussão/Forum 13.14.1 Telegram Processamento de Linguagem Natural em Português PT-Br Data Science - Python R Brasil R e Rstudio Humanidades Análise Textual-Humanidades Digitais 13.15 Links de folhas de dicas (Cheat-sheets) “Cheat sheets” seriam a tradução para “cola”, aquela feita para consulta em exames na escola. Em programação, refere-se a uma tabela muito bem resumida, com o que há de essencial em determinado assunto. Text Analysis Glossary Um glossário de termos usados na análise textual, em inglês, do projeto Constellate (JStor e diversas universidades) Lista com mais de 100 cheat-sheets em R e Python, sobre Machine Learning: link Git-GitHub, Git-Gitlab, Cheatsheets do R como: Base R, Base R em portguês, Data-table em português, Data Import, Data e horários com Lubridate, trabalhe com mais facilidade com listas e funções com o purr, stringr, dplyr, R Reference Card, Usar Python no R com reticulate, ggplot, R Markdown com o R Markdown Cheatsheet ou o Reference Guide, Regular Expressions, R Programming por Arianne Colton e Sean Chen, Uma lista com estes e outros cheatsheets aqui do Rstudio aqui. "],["excpi.html", "14 Apêndice: Exemplo Análise - CPI da Pandemia 14.1 Introdução 14.2 Pré-processamento 14.3 Primeiras observações 14.4 Entes 14.5 Palavras chave em contexto - KWIC 14.6 Opção 1: Separando apenas os senadores 14.7 Opção 2: Somente senadores, a partir do site do Senado 14.8 Nuvem de palavras comparativa Wordcloud comparision 14.9 Extração de palavras chave Keywords - colocação (collocation) 14.10 TF-IDF dos Senadores 14.11 Dicionário 14.12 Análise de coocorrência 14.13 Modelagem de tópicos", " 14 Apêndice: Exemplo Análise - CPI da Pandemia Apresento aqui um exemplo de análise possível utilizando várias técnicas de análise textual. Uma outra versão deste texto, porém não idêntico, se encontra neste link. 14.1 Introdução Atenção: O texto a seguir não tem o intuito de ser uma análise da CPI da Pandemia, mas sim de servir de breve tutorial, de fornecer ideias de uso de algumas das ferramentas disponíveis para análise textual, que podem permitir análises posteriores mais apuradas por quem se interessar pelo tema da CPI. Utilizamos aqui a base de dados feita a partir das notas taquigráficas das reuniões da CPI da pandemia, ocorrida no ano de 2021. Confira o readme. A primeira coisa a se fazer é colocar como opção global de nosso projeto que strings não sejam consideradas como fatores. # Opções globais options(stringsAsFactors = FALSE) Carregando alguns dos principais pacotes necessários. Outros serão incorporados. library(dplyr) # para manipular os dados library(magrittr) # para usamors os pipes: %&gt;% library(ggplot2) # para gerar gráficos mais agradáveis Vamos importar os dados, e temos duas opções. Opção 1: baixando a base de dados direto do site NotasTaq &lt;- readRDS(url(&quot;https://github.com/SoaresAlisson/NotasTaquigraficas/raw/master/rds/NT_todas_normalizado.rds&quot;)) Opção 2: utilizando arquivo no seu computador. É possível também baixar a base de dados em seu computador e carregá-la dali. Para tal, 1) baixe o aquivo pelo site e salve em seu computador; 2) copie o endereço do arquivo no seu computador 3) Use a função readRDS() com o endereço do arquivo, como no exemplo abaixo. Ali utiliza-se um endereço padrão Linux. NotasTaq &lt;- readRDS(&quot;~/Documentos/NT_todas_normalizado.rds&quot;) Caso utilize Windows, o caminho para o arquivo pode ser que tenha que ser algo como: NotasTaq &lt;- readRDS(&quot;c:\\Documentos\\NT_todas_normalizado.rds&quot;) Vamos transformar nosso dataframe em tibble, caso ainda não o seja NotasTaq &lt;- tibble::as_tibble(NotasTaq) # conferindo se é tibble class(NotasTaq) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; # &quot;tbl_df&quot; indica que é tibble # observando a estrutura de nossa tabela str(NotasTaq) ## tibble [94,272 × 9] (S3: tbl_df/tbl/data.frame) ## $ reuniao : num [1:94272] 1 1 1 1 1 1 1 1 1 1 ... ## $ data : Date[1:94272], format: &quot;2021-04-27&quot; &quot;2021-04-27&quot; ... ## $ nome : chr [1:94272] &quot;Otto Alencar&quot; &quot;Ciro Nogueira&quot; &quot;Otto Alencar&quot; &quot;Ciro Nogueira&quot; ... ## $ funcao_blocoPar: chr [1:94272] &quot;PRESIDENTE&quot; &quot;(Bloco Parlamentar Unidos pelo Brasil/PP - PI. Para questão de ordem.)&quot; &quot;PRESIDENTE&quot; &quot;(Bloco Parlamentar Unidos pelo Brasil/PP - PI)&quot; ... ## $ BlocoParl : chr [1:94272] &quot;(Otto Alencar. PSD - BA. Fala da Presidência.)&quot; &quot;Bloco Parlamentar Unidos pelo Brasil&quot; &quot;(Otto Alencar. PSD - BA)&quot; &quot;Bloco Parlamentar Unidos pelo Brasil&quot; ... ## $ partido : Named chr [1:94272] &quot;PSD&quot; &quot;PP&quot; &quot;PSD&quot; &quot;PP&quot; ... ## ..- attr(*, &quot;names&quot;)= chr [1:94272] &quot;PSD&quot; &quot;PP&quot; &quot;PSD&quot; &quot;PP&quot; ... ## $ estado : chr [1:94272] &quot;BA&quot; &quot;PI&quot; &quot;BA&quot; &quot;PI&quot; ... ## $ complemento : chr [1:94272] &quot;Fala da Presidência&quot; &quot;Para questão de ordem&quot; &quot;&quot; &quot;&quot; ... ## $ fala : chr [1:94272] &quot;Invocando a proteção de Deus, declaro aberta a sessão para eleição, já que temos quórum suficiente para a abert&quot;| __truncated__ &quot;Sr. Presidente, Sras. e Srs. Senadores, eu achava que nós deveríamos suspender a atual sessão até que seja sana&quot;| __truncated__ &quot;Senador Ciro Nogueira, esta é uma Comissão Parlamentar de Inquérito, V. Exa. sabe que não é temática. Então, é &quot;| __truncated__ &quot;Sr. Presidente, não é o caso de indeferir ou não. Isso aqui... &quot; ... Vemos que a coluna “BlocoParl” foi desmembrada em outras, como “partido”, “estado” e “complemento”, o que facilita se quisermos filtrar por estas variáveis. Se quisermos dar uma olhada na tabela completa, usamos a função View(). View(NotasTaq) 14.2 Pré-processamento O pré-processamento de texto é necessário para evitar certas dores de cabeça mediante a padronização dos dados. 14.2.1 Pré-processamento: substituição de abreviaturas Nosso primeiro passo no pré-processamento será substituir as abreviaturas mais comuns pelo seu equivalente por extenso. Vamos pegar abreviaturas usando um regex, pegando qualquer palavra que se inicie com maiúsculo e que termine com ponto final. Talvez este não seja o melhor regex para este caso, e teremos vários “falsos positivos”, mas será melhor que buscá-las manualmente, uma a uma lendo todo o texto (precisaríamos de dias para tal). Dali, observamos e montamos nossa tabela de abreviaturas. abrev &lt;- stringr::str_extract_all(NotasTaq$fala, &quot;([A-Z][a-z]{0,4}\\\\. )?([A-Z]+|[A-Z][a-z]{0,4})\\\\.&quot;) |&gt; unlist() abrev_cont &lt;- abrev |&gt; # contando com plyr::count temos um dataframe com a frequencia plyr::count() |&gt; arrange(desc(freq)) # Vamos observar os termos com maior frequência head(abrev_cont, 30) ## x freq ## 1 Sr. 12227 ## 2 V. Exa. 7453 ## 3 V. Sa. 4444 ## 4 Dr. 1917 ## 5 Dra. 1321 ## 6 Sim. 1263 ## 7 CPI. 1247 ## 8 O. 785 ## 9 Srs. 773 ## 10 Sra. 720 ## 11 Sras. 469 ## 12 Isso. 392 ## 13 Renan. 379 ## 14 Covid. 345 ## 15 Paulo. 205 ## 16 Dias. 201 ## 17 Risos. 197 ## 18 S. Exa. 183 ## 19 V. Exas. 174 ## 20 Claro. 159 ## 21 Certo. 157 ## 22 Costa. 133 ## 23 Bank. 127 ## 24 Omar. 127 ## 25 Log. 122 ## 26 Otto. 115 ## 27 China. 105 ## 28 S. 105 ## 29 Aziz. 103 ## 30 Vac. 101 Se quisermos olhar os termos no contexto, isto é, as palaras que o circundam, para termos uma noção do uso das abreviações, podemos usar o seguinte código: # Termo a ser buscado termo=&quot;MP&quot; stringr::str_extract_all(NotasTaq$fala, paste(&quot;(\\\\w+){2}&quot;, termo, &quot;(\\\\w+\\\\W{1,2}){3}&quot;) ) |&gt; unlist() |&gt; plyr::count() |&gt; arrange(-freq) |&gt; as_tibble() ## # A tibble: 23 × 2 ## x freq ## &lt;chr&gt; &lt;int&gt; ## 1 &quot;do MP de São Paulo, &quot; 2 ## 2 &quot;Essa MP deveria ser editada &quot; 2 ## 3 &quot;assina MP 1.003, que &quot; 1 ## 4 &quot;assina MP que libera mais &quot; 1 ## 5 &quot;da MP 1.003, e &quot; 1 ## 6 &quot;da MP 1.003, um &quot; 1 ## 7 &quot;da MP 1.026 dizia &quot; 1 ## 8 &quot;da MP 1.026; ou &quot; 1 ## 9 &quot;da MP da crise energética; &quot; 1 ## 10 &quot;da MP e do fato &quot; 1 ## # … with 13 more rows Podemos obter a frequência usando o comando table, mas ao usarmos plyr::count temos mais facilidades, uma vez que temos um dataframe. Criando um dataframe com termos a serem substituídos. header = true indica que a primeira linha será o cabeçalho, sep=\"\":\" indica que usarmos o caractere dois pontos para separa as colunas. Palavras compostas, como “Polícia Federal” recebe um sinal _ no lugar do espaço, para ser considerado um único termo. D.subs &lt;- read.table(header=TRUE, sep = &quot;:&quot;, text= &#39;abr:subs [a|A] MP:a Medida_Provisória app.:aplicativo Art.:artigo art.:artigo Cel.:coronel Dra.:Doutora Dr.:Doutor Drs.:doutores Exa.:Excelência Exma.|Ex.ma.:excelentíssima Exmo.|Ex.mo.:excelentíssimo Gen.:general Jr.:Júnior MPF:Ministério_Público_Federal Mr.:Mister [o|O] MP:o Ministério_Público [a|A] MP:a Medida_Provisória PF:Polícia_federal S. Exas.:Suas Excelências S. Exa.:Sua_Excelência Sgt.:sargento S. Paulo:São Paulo Sra.|sr.a.:Senhora Sras.:Senhoras Sr.:Senhor Srs.:Senhores Srta.|sr.a.:Senhorita STF|Supremo Tribunal Federal:Supremo_Tribunal_Federal V. Exas.:Vossas_Excelências V. Exa.:Vossa_Excelência V.? Sa.:Vossa_Senhoria V.:Vossa&#39;) |&gt; # para repassar múltiplos argumentos para uma mesma string # precisamos converter o dataframe para vetor nomeado tibble::deframe() # vamos agora substituir as abreviações no dataframe NotasTaq2 &lt;- NotasTaq |&gt; mutate(fala = stringr::str_replace_all(fala, D.subs)) 14.3 Primeiras observações Vamos observar os nomes das pessoas que falaram na CPI: participantes &lt;- NotasTaq2$nome |&gt; unique() # contando os participantes length(participantes) ## [1] 153 # vendo os nomes dos participantes, em ordem alfabética sort(participantes) ## [1] &quot;Airton Antonio Soligo&quot; ## [2] &quot;Alan Diniz Moreira Guedes de Ornelas&quot; ## [3] &quot;Alberto Zacharias Toron&quot; ## [4] &quot;Alessandro Vieira&quot; ## [5] &quot;Alexandre Figueiredo Costa Silva Marques&quot; ## [6] &quot;Alexandre Queiroz&quot; ## [7] &quot;Amilton Gomes de Paula&quot; ## [8] &quot;Andreia da Silva Lima&quot; ## [9] &quot;Angelo Coronel&quot; ## [10] &quot;Antonio Barra Torres&quot; ## [11] &quot;Antonio Carlos Alves de Sá Costa&quot; ## [12] &quot;Antônio Elcio Franco Filho&quot; ## [13] &quot;Antonio Manssur&quot; ## [14] &quot;Aristides Zacarelli&quot; ## [15] &quot;Arquivaldo Bites Leão Leite&quot; ## [16] &quot;Beno Brandão&quot; ## [17] &quot;Bia Kicis&quot; ## [18] &quot;Bruna Mendes dos Santos Morato&quot; ## [19] &quot;Carlos Murillo&quot; ## [20] &quot;Carlos Portinho&quot; ## [21] &quot;Carlos Roberto Wizard Martins&quot; ## [22] &quot;Ciro Nogueira&quot; ## [23] &quot;Cláudio Maierovitch&quot; ## [24] &quot;Cleber Lopes de Oliveira&quot; ## [25] &quot;Cristiano Alberto Hossri Carvalho&quot; ## [26] &quot;Daniel Freitas&quot; ## [27] &quot;Daniel Sampaio&quot; ## [28] &quot;Daniella Ribeiro&quot; ## [29] &quot;Danilo Berndt Trento&quot; ## [30] &quot;Dimas Tadeu Covas&quot; ## [31] &quot;Eduardo Braga&quot; ## [32] &quot;Eduardo de Vilhena Toledo&quot; ## [33] &quot;Eduardo Girão&quot; ## [34] &quot;Eduardo Pazuello&quot; ## [35] &quot;Eliana Maria dias Santiago&quot; ## [36] &quot;Eliane Nogueira&quot; ## [37] &quot;Eliziane Gama&quot; ## [38] &quot;Elton da Silva Chaves&quot; ## [39] &quot;Emanuel Ramalho Catori&quot; ## [40] &quot;Emanuela Batista de Souza Medrades&quot; ## [41] &quot;Emerson Paxá Pinto Oliveira&quot; ## [42] &quot;Eric Furtado Ferreira Borges&quot; ## [43] &quot;Ernesto Araújo&quot; ## [44] &quot;Fabiano Contarato&quot; ## [45] &quot;Fábio Henrique Ming Martini&quot; ## [46] &quot;Fabio Wajngarten&quot; ## [47] &quot;Fausto Vieira dos Santos Junior&quot; ## [48] &quot;Felipe Dantas de Araujo&quot; ## [49] &quot;Fernando Bezerra Coelho&quot; ## [50] &quot;Flávio Bolsonaro&quot; ## [51] &quot;Flavio Correa de Moraes&quot; ## [52] &quot;Francieli Fontana Sutile Fantinato&quot; ## [53] &quot;Francieli Fontana Sutile Tardetti Fantinato&quot; ## [54] &quot;Francisco Araújo Filho&quot; ## [55] &quot;Francisco Eduardo Cardoso Alves&quot; ## [56] &quot;Francisco Emerson Maximiano&quot; ## [57] &quot;Gina Moraes de Almeida&quot; ## [58] &quot;Giordano&quot; ## [59] &quot;Giovanna Gomes Mendes da Silva&quot; ## [60] &quot;Guilherme Cremonesi Caurin&quot; ## [61] &quot;Helcio Bruno de Almeida&quot; ## [62] &quot;Humberto Costa&quot; ## [63] &quot;Ivanildo Gonçalves da Silva&quot; ## [64] &quot;Izalci Lucas&quot; ## [65] &quot;Jailton Batista&quot; ## [66] &quot;Jean Paul Prates&quot; ## [67] &quot;João Carlos Gonçalves Krakauer Maia&quot; ## [68] &quot;Jorge Kajuru&quot; ## [69] &quot;Jorginho Mello&quot; ## [70] &quot;José Ricardo Santana&quot; ## [71] &quot;Jurema Werneck&quot; ## [72] &quot;Kátia Abreu&quot; ## [73] &quot;Katia Shirlene Castilho dos Santos&quot; ## [74] &quot;Leila Barros&quot; ## [75] &quot;Luana Araújo&quot; ## [76] &quot;Luciano Duarte Peres&quot; ## [77] &quot;Luciano Hang&quot; ## [78] &quot;Luis Carlos Heinze&quot; ## [79] &quot;Luis Miranda&quot; ## [80] &quot;Luis Ricardo Fernandes Miranda&quot; ## [81] &quot;Luiz Henrique Mandetta&quot; ## [82] &quot;Luiz Paulo Dominguetti Pereira&quot; ## [83] &quot;Mara Gabrilli&quot; ## [84] &quot;Marcellus Campelo&quot; ## [85] &quot;Marcellus José Barroso Campêlo&quot; ## [86] &quot;Marcelo Antônio Cartaxo Queiroga Lopes&quot; ## [87] &quot;Marcelo Blanco&quot; ## [88] &quot;Marcelo Blanco da Costa&quot; ## [89] &quot;Marcelo Queiroga&quot; ## [90] &quot;Márcio Antonio do Nascimento Silva&quot; ## [91] &quot;Marconny Nunes Ribeiro Albernaz de Faria&quot; ## [92] &quot;Marcos do Val&quot; ## [93] &quot;Marcos Rogério&quot; ## [94] &quot;Marcos Tolentino da Silva&quot; ## [95] &quot;Maria Jamile José&quot; ## [96] &quot;Maria José Ferreira Pessoa&quot; ## [97] &quot;Mayra Pinheiro&quot; ## [98] &quot;Mayra Pires Lima&quot; ## [99] &quot;Mecias de Jesus&quot; ## [100] &quot;Michel Saliba Oliveira&quot; ## [101] &quot;Milena Ramos Câmara&quot; ## [102] &quot;Natalia Pasternak&quot; ## [103] &quot;Nelsinho Trad&quot; ## [104] &quot;Nelson Luiz Sperle Teich&quot; ## [105] &quot;Nise Hitomi Yamaguchi&quot; ## [106] &quot;Omar Aziz&quot; ## [107] &quot;Osmar Terra&quot; ## [108] &quot;Otávio de Queiroga&quot; ## [109] &quot;Otávio Oscar Fakhoury&quot; ## [110] &quot;Otto Alencar&quot; ## [111] &quot;Paulo Roberto Vanderlei Rebello Filho&quot; ## [112] &quot;Paulo Rocha&quot; ## [113] &quot;Pedro Benedito Batista Júnior&quot; ## [114] &quot;Pedro Hallal&quot; ## [115] &quot;Pedro Henrique Medeiros de Araújo&quot; ## [116] &quot;Pedro Ivo Velloso&quot; ## [117] &quot;Priscila Pamela Cesario dos Santos&quot; ## [118] &quot;Raimundo Nonato Brasil&quot; ## [119] &quot;Randolfe Rodrigues&quot; ## [120] &quot;Regina Célia Silva Oliveira&quot; ## [121] &quot;Reguffe&quot; ## [122] &quot;Reinhold Stephanes Junior&quot; ## [123] &quot;Renan Calheiros&quot; ## [124] &quot;Ricardo Ariel Zimerman&quot; ## [125] &quot;Ricardo Barros&quot; ## [126] &quot;Roberto Ferreira dias&quot; ## [127] &quot;Roberto Pereira Ramos Júnior&quot; ## [128] &quot;Roberto Rocha&quot; ## [129] &quot;Rodrigo Cunha&quot; ## [130] &quot;Rogério Carvalho&quot; ## [131] &quot;Rosane Maria dos Santos Brandão&quot; ## [132] &quot;Rose de Freitas&quot; ## [133] &quot;Savio de Faria Caram Zuquim&quot; ## [134] &quot;Simone Tebet&quot; ## [135] &quot;Soraya Thronicke&quot; ## [136] &quot;Styvenson Valentim&quot; ## [137] &quot;Tadeu Frederico de Andrade&quot; ## [138] &quot;Tasso Jereissati&quot; ## [139] &quot;Telmário Mota&quot; ## [140] &quot;Thiago Leônidas&quot; ## [141] &quot;Ticiano Figueiredo&quot; ## [142] &quot;Ticiano Figueiredo de Oliveira&quot; ## [143] &quot;Túlio Silveira&quot; ## [144] &quot;Vanderlan Cardoso&quot; ## [145] &quot;Vinicius Luiz Ferreira&quot; ## [146] &quot;Wagner de Campos Rosário&quot; ## [147] &quot;Wagner Lima da Costa&quot; ## [148] &quot;Walter Correa de Souza Neto&quot; ## [149] &quot;Walter José Faiad de Moura&quot; ## [150] &quot;Weverton&quot; ## [151] &quot;William Amorim Santana&quot; ## [152] &quot;Wilson Witzel&quot; ## [153] &quot;Zenaide Maia&quot; Aparecem 154 nomes diferentes, mas repare que alguns destes são uma mesma pessoa, porém com mais de uma grafia, como o ex-ministro da saúde Eduardo Pazuello, que aparece uma com e sem “gen.”, bem como Francieli, o deputado Luis Miranda e Marcelo Blanco, também aparecem com nomes grafado de duas formas distintas. Além disso, o nome de Marcelo Queiroga ocupa bastante espaço. Vamos ajustar estes nomes. NotasTaq3 &lt;- NotasTaq2 %&gt;% mutate(nome = gsub(&quot;Marcelo Antônio Cartaxo Queiroga Lopes&quot;, &quot;Marcelo Queiroga&quot;, nome) %&gt;% gsub(&quot;Gen. Eduardo Pazuello&quot;, &quot;Eduardo Pazuello&quot;, .) %&gt;% gsub(&quot;Francieli Fontana Sutile Tardetti Fantinato|Francieli Fontana Sutile Fantinato&quot;, &quot;Francieli Fantinato&quot;, .) %&gt;% gsub(&quot;Luis Ricardo Fernandes Miranda&quot;, &quot;Luis Miranda&quot; , .) %&gt;% gsub(&quot;Marcelo Blanco da Costa&quot;, &quot;Marcelo Blanco&quot;, .) ) Conferindo se deu certo. O número de participantes deve diminuir dos 153 anteriores após as substituições: NotasTaq3$nome |&gt; unique() |&gt; length() ## [1] 149 Numa primeira observação, vamos ver a quantidade de intervenções de cada pessoa, isto é, quantas vezes uma pessoa iniciou uma fala, independente da quantidade de palavras ditas. Para tal, vamos fazer um gráfico de barras com o barplot do pacote base, que já vem por padrão no R: # Para usar o barplot, precisamos antes converter para table intervencoes &lt;- NotasTaq3$nome %&gt;% table intervencoes %&gt;% barplot(., main=&quot;Quantidade de Intervenções&quot;, # titulo ylab=&quot;Frequência&quot;, # eixo y cex.lab=1, # tamanho do label cex.axis=0.1, # tamanho do texto nos eixos cex.names=0.5, # tamanho dos nomes eixo x las=2 # rotacionando ) Podemos usar o ggplot e termos mais opções de controle. Se formos olhar a frequência das cerca de 150 pessoas, teremos um gráfico não muito compreensível: ggplot(data = NotasTaq3, aes(x = nome, y = )) + geom_bar() + labs(title = &quot;Quantidade de intervenções&quot;, x = &quot;&quot;,y = &quot;&quot;) + theme(axis.text.x = element_text(angle = 90)) Vamos melhorar este gráfico restringindo apenas aos casos mais frequentes e rotacionando seu eixo. # criando um novo dataframe com nomes e contagem intervencoes &lt;- NotasTaq3$nome %&gt;% plyr::count() # renomeando as colunas colnames(intervencoes) &lt;- c(&quot;nome&quot;,&quot;freq&quot;) # Ordenando pela coluna &quot;freq&quot; de modo decrescente. dplyr::arrange(intervencoes, desc(freq)) %&gt;% # restringindo aos 30 mais frequentes head(30) %&gt;% ggplot( aes(x = reorder(nome, freq), y = freq)) + geom_col() + labs(title = &quot;Quantidade de intervenções na CPI&quot;, x = &quot;&quot;,y = &quot;&quot;) + theme(axis.text.x = element_text(angle = 90)) + # girando o gráfico coord_flip() Alguns comentários sobre este gráfico acima: Nenhuma grande surpresa na distribuição de intervenções. Os nomes mais frequentes são o presidente e o relator. Para contar as intervenções pode-se usar o table - como usamos para gerar o barplot anterior - mas uma opção mais prática é usar o plyr::count(), que nos retorna um dataframe, o que torna mais fácil lidar com dados gerados. Apesar de termos ordenado nossos dados com base na frequência, o ggplot organiza os dados com base na ordem alfabética dos nomes. Mas se quisermos organizar na ordem das intervenções, usamos aes(x = reorder(eixoX, eixoY), y = eixoY)) ou no caso aes(x = reorder(nome, freq), y = freq)). Para contar as intervenções pode-se usar o table - como usamos para gerar o barplot anterior - mas uma opção mais prática é usar o plyr::count(), que nos retorna um dataframe, o que torna mais fácil lidar com dados gerados. Apesar de termos ordenado nossos dados com base na frequência, o ggplot organiza os dados com base na ordem alfabética dos nomes. Mas se quisermos organizar na ordem das intervenções, usamos aes(x = reorder(eixoX, eixoY), y = eixoY)) ou no caso aes(x = reorder(nome, freq), y = freq)). O resultado do gráfico faz todo o sentido, entre os nomes mais frequentes estarem o depoente, o presidente e relator. Vamos ver agora o ranking por quantidade de palavras ditas. Vamos utilizar números mais gerais dos parlamentares, referentes à todas as sessões. Para tal, vamos agregar as falas de diferentes dias em uma linha por nome através dos comandos do dplyr group_by e summarize, e como o que queremos é que junte todas as falas em uma só célula, vamos usar de paste() com o parâmetro collapse = \" \" que indica que entre uma fala e outra que serão condensadas em uma só célula, entre cada elemento será inserido um espaço vazio, para evitar que uma palavra final de uma célula fique colada à palavra inicial da célula seguinte. NotasTaq_falas.agrupadas &lt;- NotasTaq3 %&gt;% group_by(nome) %&gt;% summarize(falas = paste(fala, collapse = &quot; &quot;)) # se quisermos observar a estrutura de nosso dataframe str(NotasTaq_falas.agrupadas) ## tibble [149 × 2] (S3: tbl_df/tbl/data.frame) ## $ nome : chr [1:149] &quot;Airton Antonio Soligo&quot; &quot;Alan Diniz Moreira Guedes de Ornelas&quot; &quot;Alberto Zacharias Toron&quot; &quot;Alessandro Vieira&quot; ... ## $ falas: chr [1:149] &quot;Senhor Presidente, acredito que, em função da decisão, eu não sou obrigado, mas estou aqui para dizer a verdade&quot;| __truncated__ &quot;Presidente, só uma questão de ordem então? Para deixar o registro, então, de que, com a liminar do Supremo_Tri&quot;| __truncated__ &quot;Perfeitamente, Excelência, Senhor Presidente em exercício. Senhor Presidente, o despacho, se Vossa Excelência &quot;| __truncated__ &quot;Pela ordem, Presidente Otto Alencar. Senhor Presidente... Obrigado, Senhor Presidente. Apenas quero contradit&quot;| __truncated__ ... # contando as palavras NT_falasJuntasCount &lt;- NotasTaq_falas.agrupadas %&gt;% mutate(N_palavras = stringr::str_count(falas, &quot;\\\\W&quot;), .after = 1) |&gt; # reordenar pelo número de palavras (arrange) dos maiores valores aos menores (desc) arrange(desc(N_palavras)) # plotando o gráfico NT_falasJuntasCount %&gt;% # restringindo aos primeiros resultados head(30) %&gt;% ggplot( aes(x = reorder(nome, N_palavras), y = N_palavras)) + geom_col() + labs(title = &quot;Quantidade de palavras ditas na CPI&quot;, x = &quot;&quot;,y = &quot;&quot;) + theme(axis.text.x = element_text(angle = 90)) + # girando o gráfico coord_flip() Vamos colocar os gráficos lado a lado para facilitar a comparação. Para tal, vamos utilizar o pacote patchwork que torna bem fácil colocar múltiplos gráficos juntos, nas mais diferentes configurações de layout. Para instalar, podemos usar o comando install.packages('patchwork'). # carregando o pacote library(patchwork) Para usá-lo, vamos primeiro salvar os gráficos como objetos R e depois vamos organizá-los no patchwork. Os gráficos são os mesmos que usamos mais acima. # Número de corte do máximo de itens a aparecer no gráfico n_max &lt;- 20 graf1 &lt;- dplyr::arrange(intervencoes, desc(freq)) %&gt;% # restringindo aos 20 mais frequentes head(n_max) %&gt;% ggplot( aes(x = reorder(nome, freq), y = freq)) + geom_col() + labs(title = &quot;Número de intervenções&quot;, x = &quot;&quot;,y = &quot;&quot;) + theme(axis.text.x = element_text(angle = 90), # Deslocando o título do gráfico para ficar mais visível plot.title.position = &quot;plot&quot;) + # girando o gráfico coord_flip() graf2 &lt;- NT_falasJuntasCount %&gt;% # restringindo aos primeiros resultados top_n(20, N_palavras) %&gt;% ggplot( aes(x = reorder(nome, N_palavras), y = N_palavras)) + geom_col() + labs(title = &quot;Número de palavras ditas&quot;, x = &quot;&quot;,y = &quot;&quot;) + #labs(title = &quot;palavras ditas&quot;, x = &quot;&quot;,y = &quot;&quot;) + theme(axis.text.x = element_text(angle = 90), plot.title.position = &quot;plot&quot;) + # girando o gráfico coord_flip() graf1 + graf2 + # adicionando titulo ao gráfico plot_annotation(title = &#39;CPI da Pandemia: falas quantificadas de todas as reuniões&#39;, caption = &#39;*Apenas os 20 mais frequentes aparecem nos gráficos\\nElaboração: Alisson Soares&#39;) Vamos examinar a diferença entre o número de intervenções e o número de palavras ditas bem como o inverso, a diferença entre número de palavras ditas e o númerode intervenções. Vamos igualar os valores da coluna utilisando percentual: dividir os valores da coluna pela soma de seus valores com sum() # criando nova coluna/variável com os percentuais intervencoes2 &lt;- intervencoes %&gt;% mutate(interven_percent = intervencoes$freq / sum(intervencoes$freq)) # testando o cálculo: deve dar 1 sum(intervencoes2$interven_percent) ## [1] 1 NT_falasJuntasCount2 &lt;- NT_falasJuntasCount %&gt;% mutate(N_palavras_percent = NT_falasJuntasCount$N_palavras / sum(NT_falasJuntasCount$N_palavras)) %&gt;% select(nome, N_palavras_percent) # selecionando apenas duas colunas # testando o cálculo: deve dar 1 sum(NT_falasJuntasCount2$N_palavras_percent) ## [1] 1 # Vamos juntar os dois dataframes DF_falas_intervencoes &lt;- merge(intervencoes2, NT_falasJuntasCount2, by = &quot;nome&quot;) # Vamos encontrar a diferenca entre as duas colunas DF_falas_interv &lt;- DF_falas_intervencoes %&gt;% mutate(diferenca = N_palavras_percent - interven_percent) %&gt;% arrange(-diferenca) # vamos gerar um gráfico DF_falas_interv %&gt;% top_n(20, diferenca) %&gt;% ggplot(aes(x = reorder(nome, diferenca), y = diferenca)) + geom_col() + labs(title = &quot;Diferença: nº de palavras - nº intervenções&quot;, x = &quot;&quot;,y = &quot;&quot;, caption = &#39;*Apenas os 20 mais frequentes aparecem nos gráficos Elaboração: Alisson Soares&#39;) + coord_flip() # girando o gráfico Vamos fazer a subtração inversa agora: DF_falas_interv &lt;- DF_falas_intervencoes %&gt;% mutate(diferenca = interven_percent - N_palavras_percent) %&gt;% arrange(-diferenca) # vamos gerar um gráfico DF_falas_interv %&gt;% top_n(20, diferenca) %&gt;% ggplot(aes(x = reorder(nome, diferenca), y = diferenca)) + geom_col() + labs(title = &quot;Diferença: nº de intervenções - nº de palavras&quot;, x = &quot;&quot;,y = &quot;&quot;, caption = &#39;*Apenas os 20 mais frequentes aparecem nos gráficos\\ Elaboração: Alisson Soares&#39;) + coord_flip() # girando o gráfico 14.4 Entes Dado que o texto das notas taquigráficas possuem certa formalização, um modo simples de pegar nomes próprios - não dos mais eficientes, é verdade, mas relativamente rápido - é utilizando expresões regulares. Um dos principais problemas de usar regex neste caso, é que este padrão irá pegar também palavras em maiúsculo que iniciam frases, teremos vários falsos positivos. Esta ferramenta, portanto, pode servir apenas como um auxílio na exploração do texto. # Todas palavras únicas que comecem em maiúsculo entes1 &lt;- NT_falasJuntasCount$falas |&gt; stringr::str_extract_all(&quot;[:upper:][:alpha:]+(\\\\-[:alpha:]+)?&quot;) |&gt; unlist() |&gt; plyr::count() |&gt; arrange(-freq) # Palavras em maiúsculo, com duas ou mais palavras em maiúsculo entes &lt;- NT_falasJuntasCount$falas |&gt; stringr::str_extract_all(&quot;([:upper:][:alpha:]+(\\\\-[:alpha:]+| d[aeiou]s?)? ?){2,}&quot; ) |&gt; unlist() |&gt; plyr::count() |&gt; arrange(-freq) # Dando uma olhada entes1[1:15,] ## x freq ## 1 Eu 18795 ## 2 Senador 18658 ## 3 Presidente 17598 ## 4 Não 15772 ## 5 Senhor 13285 ## 6 Então 10054 ## 7 Excelência 8866 ## 8 Vossa 7748 ## 9 Saúde 6620 ## 10 CPI 6237 ## 11 Brasil 6076 ## 12 Ministério 5855 ## 13 Governo 5309 ## 14 Mas 4933 ## 15 Ministro 4660 entes[1:15,] ## x freq ## 1 Senhor Presidente 5325 ## 2 Vossa Excelência 5025 ## 3 Ministério da Saúde 2466 ## 4 Ministério da Saúde 1804 ## 5 Vossa Excelência 1682 ## 6 Comissão Parlamentar de Inquérito 1213 ## 7 Presidente da República 1068 ## 8 Senhor Relator 1066 ## 9 Senador Renan 984 ## 10 Presidente da República 896 ## 11 Senador Marcos Rogério 781 ## 12 Senador Randolfe 706 ## 13 Governo Federal 699 ## 14 Governo Federal 690 ## 15 Comissão Parlamentar de Inquérito 663 Destrinchando o regex: [:upper:][:alpha:]+ indica palavras iniciadas em maiúscula, seguida de letras minúsculas ou maiúsculas, uma ou várias vezes (\\\\-[:alpha:]+ hífen seguido de letras, uma ou mais vezes, | ou o padrão d[aeiou]s? que busca captar “da”, “de”, “dos”, etc. )? indica que estes padrões podem aparecer uma vez ou nenhuma. Segue ainda um espaço, também opcional: ?) ){2,} o padrão deve repetir duas vezes ou mais. 14.5 Palavras chave em contexto - KWIC Para entender melhor como certas palavras foram utilizadas, em que contexto elas apareceram, podemos utilizar o Keywords in context # Carregando o pacote library(quanteda) # Transformando o dataframe em corpus do quanteda Corpus.NT.falas.agrupadas &lt;- corpus(NotasTaq_falas.agrupadas, docnames = &quot;nome&quot;, # docid_field = 1:nrow(NotasTaq_falas.agrupadas), docid_field = &quot;nome&quot;, text_field = &quot;falas&quot; ) ## Warning: docnames argument is not used. # observando nosso corpus print(Corpus.NT.falas.agrupadas) ## Corpus consisting of 149 documents. ## Airton Antonio Soligo : ## &quot;Senhor Presidente, acredito que, em função da decisão, eu nã...&quot; ## ## Alan Diniz Moreira Guedes de Ornelas : ## &quot;Presidente, só uma questão de ordem então? Para deixar o re...&quot; ## ## Alberto Zacharias Toron : ## &quot;Perfeitamente, Excelência, Senhor Presidente em exercício. ...&quot; ## ## Alessandro Vieira : ## &quot;Pela ordem, Presidente Otto Alencar. Senhor Presidente... ...&quot; ## ## Alexandre Figueiredo Costa Silva Marques : ## &quot;Senador, eu vou me valer do habeas corpus que me foi deferid...&quot; ## ## Alexandre Queiroz : ## &quot;Presidente, bom dia. Bom dia, eminente Relator, Senhora. Se...&quot; ## ## [ reached max_ndoc ... 143 more documents ] summary(Corpus.NT.falas.agrupadas, 5) ## Corpus consisting of 149 documents, showing 5 documents: ## ## Text Types Tokens Sentences ## Airton Antonio Soligo 2707 19499 1372 ## Alan Diniz Moreira Guedes de Ornelas 236 630 60 ## Alberto Zacharias Toron 233 662 65 ## Alessandro Vieira 6976 73547 4083 ## Alexandre Figueiredo Costa Silva Marques 1324 7169 459 # tokenizando para poder usar no kwic corpus.token &lt;- Corpus.NT.falas.agrupadas |&gt; # precisamos primeiro tokenizar tokens() # termos a serem buscados termos.vetor &lt;- c(&quot;WhatsApps&quot;, &quot;Quickcard&quot;) # rodando a função de palavras chave em contexto kwic(corpus.token , # termos a serem buscados. Pode ser um termo ou um vetor termos.vetor, # quantas palavras devem ser mostradas ao redor 4, # Para pegar tanto palavras minúsculas como as em maiúsculo. case_insensitive = TRUE) ## Keyword-in-context with 10 matches. ## [Humberto Costa, 18024] já recebi hoje alguns | WhatsApps | ## [Humberto Costa, 36653] , vários telefonemas e | WhatsApps | ## [Jean Paul Prates, 12701] conhece um serviço chamado | QuickCard | ## [Jean Paul Prates, 12703] serviço chamado QuickCard? | QuickCard | ## [Jean Paul Prates, 12716] . Vossacê emite um | QuickCard | ## [Jean Paul Prates, 12749] para esse serviço chamado | QuickCard | ## [Jean Paul Prates, 12986] é o pagamento à | QuickCard | ## [Jean Paul Prates, 13003] adiante: a quem | QuickCard | ## [Randolfe Rodrigues, 210553] Abdul, vamos aos | WhatsApps | ## [Randolfe Rodrigues, 210707] Pode continuar com os | WhatsApps | ## ## e tal. O ## de assessores do ministério ## ? QuickCard é uma ## é uma espécie de ## para alguém e essa ## . O senhor tem ## . O que a ## pagou, ou quem ## . O Senhor Abdul ## . Pode continuar, Vamos restringir inicialmente nossa base de dados para explorar melhor. Vamos começar somente com os senadores. 14.6 Opção 1: Separando apenas os senadores Como somente os senadores que participaram possuem campo com partido e bloco parlamentar preenchido, uma opção para filtrarmos apenas os senadores seria excluindo as linhas cuja célula da coluna “BlocoParl” esteja vazia. O campo “funcao_blocoPar” possui, além da função e bloco parlamentar, possui também questões de ordem O campo “part” melhora a situação, mas podendo haver parlamentar sem partido, não resolve nosso problema o campo “estado” pode resolver nosso problema. siglas &lt;- c(&quot;AC&quot;, &quot;AL&quot;, &quot;AP&quot;, &quot;AM&quot;, &quot;BA&quot;, &quot;CE&quot;, &quot;DF&quot;, &quot;ES&quot;, &quot;GO&quot;, &quot;MA&quot;, &quot;MT&quot;, &quot;MS&quot;, &quot;MG&quot;, &quot;PA&quot;, &quot;PB&quot;, &quot;PR&quot;, &quot;PE&quot;, &quot;PI&quot;, &quot;RJ&quot;, &quot;RN&quot;, &quot;RS&quot;, &quot;RO&quot;, &quot;RR&quot;, &quot;SC&quot;, &quot;SP&quot;, &quot;SE&quot;, &quot;TO&quot;) DFsenadores &lt;- NotasTaq %&gt;% #filter(siglas %in% estado) filter(estado %in% siglas) DFsenadores ## # A tibble: 67,220 × 9 ## reuniao data nome funcao_blocoPar BlocoParl partido estado complemento ## &lt;dbl&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 2021-04-27 Otto… PRESIDENTE (Otto Al… PSD BA &quot;Fala da P… ## 2 1 2021-04-27 Ciro… (Bloco Parlame… Bloco Pa… PP PI &quot;Para ques… ## 3 1 2021-04-27 Otto… PRESIDENTE (Otto Al… PSD BA &quot;&quot; ## 4 1 2021-04-27 Ciro… (Bloco Parlame… Bloco Pa… PP PI &quot;&quot; ## 5 1 2021-04-27 Otto… PRESIDENTE (Otto Al… PSD BA &quot;&quot; ## 6 1 2021-04-27 Ciro… (Bloco Parlame… Bloco Pa… PP PI &quot;&quot; ## 7 1 2021-04-27 Otto… PRESIDENTE (Otto Al… PSD BA &quot;&quot; ## 8 1 2021-04-27 Ciro… (Bloco Parlame… Bloco Pa… PP PI &quot;&quot; ## 9 1 2021-04-27 Otto… PRESIDENTE (Otto Al… PSD BA &quot;&quot; ## 10 1 2021-04-27 Ciro… (Bloco Parlame… Bloco Pa… PP PI &quot;&quot; ## # … with 67,210 more rows, and 1 more variable: fala &lt;chr&gt; unique(DFsenadores$nome) ## [1] &quot;Otto Alencar&quot; &quot;Ciro Nogueira&quot; ## [3] &quot;Jorginho Mello&quot; &quot;Izalci Lucas&quot; ## [5] &quot;Alessandro Vieira&quot; &quot;Eduardo Braga&quot; ## [7] &quot;Eduardo Girão&quot; &quot;Marcos Rogério&quot; ## [9] &quot;Omar Aziz&quot; &quot;Humberto Costa&quot; ## [11] &quot;Rogério Carvalho&quot; &quot;Weverton&quot; ## [13] &quot;Eliziane Gama&quot; &quot;Randolfe Rodrigues&quot; ## [15] &quot;Paulo Rocha&quot; &quot;Flávio Bolsonaro&quot; ## [17] &quot;Renan Calheiros&quot; &quot;Fernando Bezerra Coelho&quot; ## [19] &quot;Luis Carlos Heinze&quot; &quot;Angelo Coronel&quot; ## [21] &quot;Marcos do Val&quot; &quot;Simone Tebet&quot; ## [23] &quot;Leila Barros&quot; &quot;Tasso Jereissati&quot; ## [25] &quot;Zenaide Maia&quot; &quot;Fabiano Contarato&quot; ## [27] &quot;Vanderlan Cardoso&quot; &quot;Telmário Mota&quot; ## [29] &quot;Soraya Thronicke&quot; &quot;Jean Paul Prates&quot; ## [31] &quot;Mara Gabrilli&quot; &quot;Reguffe&quot; ## [33] &quot;Mecias de Jesus&quot; &quot;Roberto Rocha&quot; ## [35] &quot;Kátia Abreu&quot; &quot;Daniella Ribeiro&quot; ## [37] &quot;Jorge Kajuru&quot; &quot;Carlos Portinho&quot; ## [39] &quot;Styvenson Valentim&quot; &quot;Nelsinho Trad&quot; ## [41] &quot;Giordano&quot; &quot;Osmar Terra&quot; ## [43] &quot;Luis Miranda&quot; &quot;Rodrigo Cunha&quot; ## [45] &quot;Eliane Nogueira&quot; &quot;Reinhold Stephanes Junior&quot; ## [47] &quot;Ricardo Barros&quot; &quot;Rose de Freitas&quot; ## [49] &quot;Daniel Freitas&quot; &quot;Bia Kicis&quot; No entanto, esta opção captou também os deputados que lá falaram, como Luis Miranda e Bia Kicis. Poderíamos continuar aqui separando manualmente os deputados de nosso dataframe, mas vamos tentar outra opção. 14.7 Opção 2: Somente senadores, a partir do site do Senado Vamos pegar os nomes dos senadores no site do Senado. O endereço para a atual legislatura é o https://www25.senado.leg.br/web/senadores/em-exercicio/. Porém, quem for pegar estes dados tempos depois pode encontrar dados divergentes. Baixei esta página e ela está disponível em: library(rvest) minhaURL=&quot;https://github.com/SoaresAlisson/analisetextual/tree/master/Senadores_em_Exercício-SenadoFederal.html&quot; pagina_senadores &lt;- rvest::read_html(url(minhaURL)) senadores &lt;- pagina_senadores %&gt;% html_elements(&quot;#senadoresemexercicio-tabela-senadores a&quot;) %&gt;% html_text() #html_attr(&quot;name&quot;) head(senadores, 20) ## [1] &quot;&quot; &quot;Mailza Gomes&quot; &quot;Marcio Bittar&quot; ## [4] &quot;SÃ©rgio PetecÃ£o&quot; &quot;&quot; &quot;Fernando Collor&quot; ## [7] &quot;Renan Calheiros&quot; &quot;Rodrigo Cunha&quot; &quot;&quot; ## [10] &quot;Eduardo Braga&quot; &quot;Omar Aziz&quot; &quot;PlÃ­nio ValÃ©rio&quot; ## [13] &quot;&quot; &quot;Davi Alcolumbre&quot; &quot;Lucas Barreto&quot; ## [16] &quot;Randolfe Rodrigues&quot; &quot;&quot; &quot;Angelo Coronel&quot; ## [19] &quot;Jaques Wagner&quot; &quot;Otto Alencar&quot; # Vimos que há muitos elementos vazios no vetor. Vamos retirá-los com o comando: senadores &lt;- senadores[senadores!= &quot;&quot;] senadores ## [1] &quot;Mailza Gomes&quot; &quot;Marcio Bittar&quot; ## [3] &quot;SÃ©rgio PetecÃ£o&quot; &quot;Fernando Collor&quot; ## [5] &quot;Renan Calheiros&quot; &quot;Rodrigo Cunha&quot; ## [7] &quot;Eduardo Braga&quot; &quot;Omar Aziz&quot; ## [9] &quot;PlÃ­nio ValÃ©rio&quot; &quot;Davi Alcolumbre&quot; ## [11] &quot;Lucas Barreto&quot; &quot;Randolfe Rodrigues&quot; ## [13] &quot;Angelo Coronel&quot; &quot;Jaques Wagner&quot; ## [15] &quot;Otto Alencar&quot; &quot;Cid Gomes&quot; ## [17] &quot;Eduardo GirÃ£o&quot; &quot;Tasso Jereissati&quot; ## [19] &quot;Izalci Lucas&quot; &quot;Leila Barros&quot; ## [21] &quot;Reguffe&quot; &quot;Fabiano Contarato&quot; ## [23] &quot;Marcos do Val&quot; &quot;Rose de Freitas&quot; ## [25] &quot;Jorge Kajuru&quot; &quot;Luiz Carlos do Carmo&quot; ## [27] &quot;Vanderlan Cardoso&quot; &quot;Eliziane Gama&quot; ## [29] &quot;Roberto Rocha&quot; &quot;Weverton&quot; ## [31] &quot;Alexandre Silveira&quot; &quot;Carlos Viana&quot; ## [33] &quot;Rodrigo Pacheco&quot; &quot;Nelsinho Trad&quot; ## [35] &quot;Simone Tebet&quot; &quot;Soraya Thronicke&quot; ## [37] &quot;Carlos FÃ¡varo&quot; &quot;Fabio Garcia&quot; ## [39] &quot;Wellington Fagundes&quot; &quot;Jader Barbalho&quot; ## [41] &quot;Paulo Rocha&quot; &quot;Zequinha Marinho&quot; ## [43] &quot;Daniella Ribeiro&quot; &quot;Nilda Gondim&quot; ## [45] &quot;Veneziano Vital do RÃªgo&quot; &quot;Fernando Bezerra Coelho&quot; ## [47] &quot;Humberto Costa&quot; &quot;Jarbas Vasconcelos&quot; ## [49] &quot;Eliane Nogueira&quot; &quot;Elmano FÃ©rrer&quot; ## [51] &quot;Marcelo Castro&quot; &quot;Alvaro Dias&quot; ## [53] &quot;FlÃ¡vio Arns&quot; &quot;Oriovisto GuimarÃ£es&quot; ## [55] &quot;Carlos Portinho&quot; &quot;FlÃ¡vio Bolsonaro&quot; ## [57] &quot;RomÃ¡rio&quot; &quot;Jean Paul Prates&quot; ## [59] &quot;Styvenson Valentim&quot; &quot;Zenaide Maia&quot; ## [61] &quot;Acir Gurgacz&quot; &quot;ConfÃºcio Moura&quot; ## [63] &quot;Marcos RogÃ©rio&quot; &quot;Chico Rodrigues&quot; ## [65] &quot;Mecias de Jesus&quot; &quot;TelmÃ¡rio Mota&quot; ## [67] &quot;Lasier Martins&quot; &quot;Luis Carlos Heinze&quot; ## [69] &quot;Paulo Paim&quot; &quot;DÃ¡rio Berger&quot; ## [71] &quot;EsperidiÃ£o Amin&quot; &quot;Jorginho Mello&quot; ## [73] &quot;Alessandro Vieira&quot; &quot;Maria do Carmo Alves&quot; ## [75] &quot;RogÃ©rio Carvalho&quot; &quot;Giordano&quot; ## [77] &quot;JosÃ© Serra&quot; &quot;Mara Gabrilli&quot; ## [79] &quot;Eduardo Gomes&quot; &quot;IrajÃ¡&quot; ## [81] &quot;KÃ¡tia Abreu&quot; De posse dos nomes dos senadores, vamos ver quais nomes no nosso dataframe da CPI tem intersecção com a lista de senadores. Lembrando, esta lista foi gerada pouco tempo depois da CPI, assim, dependendo de quanto tempo você for tentar reproduzir o exemplo, o link pode ter expirado ou a lista de senadores pode já ter mudado. Já havíamos gerado um vetor com os nomes de todos que participaram da CPI: “participantes”. Vamos fazer a intersecção dos participantes da CPI com os senadores para pegar apenas os senadores que participaram da CPI. senadoresNaCPI &lt;- participantes[participantes %in% senadores] senadoresNaCPI ## [1] &quot;Otto Alencar&quot; &quot;Jorginho Mello&quot; ## [3] &quot;Izalci Lucas&quot; &quot;Alessandro Vieira&quot; ## [5] &quot;Eduardo Braga&quot; &quot;Omar Aziz&quot; ## [7] &quot;Humberto Costa&quot; &quot;Weverton&quot; ## [9] &quot;Eliziane Gama&quot; &quot;Randolfe Rodrigues&quot; ## [11] &quot;Paulo Rocha&quot; &quot;Renan Calheiros&quot; ## [13] &quot;Fernando Bezerra Coelho&quot; &quot;Luis Carlos Heinze&quot; ## [15] &quot;Angelo Coronel&quot; &quot;Marcos do Val&quot; ## [17] &quot;Simone Tebet&quot; &quot;Leila Barros&quot; ## [19] &quot;Tasso Jereissati&quot; &quot;Zenaide Maia&quot; ## [21] &quot;Fabiano Contarato&quot; &quot;Vanderlan Cardoso&quot; ## [23] &quot;Soraya Thronicke&quot; &quot;Jean Paul Prates&quot; ## [25] &quot;Mara Gabrilli&quot; &quot;Reguffe&quot; ## [27] &quot;Mecias de Jesus&quot; &quot;Roberto Rocha&quot; ## [29] &quot;Daniella Ribeiro&quot; &quot;Jorge Kajuru&quot; ## [31] &quot;Carlos Portinho&quot; &quot;Styvenson Valentim&quot; ## [33] &quot;Nelsinho Trad&quot; &quot;Giordano&quot; ## [35] &quot;Rodrigo Cunha&quot; &quot;Eliane Nogueira&quot; ## [37] &quot;Rose de Freitas&quot; Tendo agora a lista dos senadores que participaram da CPI, vamos filtrar as falas somente destes. senadoresdf &lt;- NotasTaq %&gt;% filter(nome %in% senadoresNaCPI ) str(senadoresdf) ## tibble [56,263 × 9] (S3: tbl_df/tbl/data.frame) ## $ reuniao : num [1:56263] 1 1 1 1 1 1 1 1 1 1 ... ## $ data : Date[1:56263], format: &quot;2021-04-27&quot; &quot;2021-04-27&quot; ... ## $ nome : chr [1:56263] &quot;Otto Alencar&quot; &quot;Otto Alencar&quot; &quot;Otto Alencar&quot; &quot;Otto Alencar&quot; ... ## $ funcao_blocoPar: chr [1:56263] &quot;PRESIDENTE&quot; &quot;PRESIDENTE&quot; &quot;PRESIDENTE&quot; &quot;PRESIDENTE&quot; ... ## $ BlocoParl : chr [1:56263] &quot;(Otto Alencar. PSD - BA. Fala da Presidência.)&quot; &quot;(Otto Alencar. PSD - BA)&quot; &quot;(Otto Alencar. PSD - BA)&quot; &quot;(Otto Alencar. PSD - BA)&quot; ... ## $ partido : Named chr [1:56263] &quot;PSD&quot; &quot;PSD&quot; &quot;PSD&quot; &quot;PSD&quot; ... ## ..- attr(*, &quot;names&quot;)= chr [1:56263] &quot;PSD&quot; &quot;PSD&quot; &quot;PSD&quot; &quot;PSD&quot; ... ## $ estado : chr [1:56263] &quot;BA&quot; &quot;BA&quot; &quot;BA&quot; &quot;BA&quot; ... ## $ complemento : chr [1:56263] &quot;Fala da Presidência&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ fala : chr [1:56263] &quot;Invocando a proteção de Deus, declaro aberta a sessão para eleição, já que temos quórum suficiente para a abert&quot;| __truncated__ &quot;Senador Ciro Nogueira, esta é uma Comissão Parlamentar de Inquérito, V. Exa. sabe que não é temática. Então, é &quot;| __truncated__ &quot;Eu indeferi. Sou Presidente e posso indeferir. &quot; &quot;Por que V. Exa. não questionou à época essa questão de ordem? &quot; ... # criando um DF das falas de cada senador todas reunidas senadores_falasJuntas &lt;- NT_falasJuntasCount %&gt;% filter(nome %in% senadoresNaCPI ) str(senadores_falasJuntas) ## tibble [37 × 3] (S3: tbl_df/tbl/data.frame) ## $ nome : chr [1:37] &quot;Omar Aziz&quot; &quot;Renan Calheiros&quot; &quot;Randolfe Rodrigues&quot; &quot;Humberto Costa&quot; ... ## $ N_palavras: int [1:37] 413495 349545 341685 138419 125102 124592 114902 89020 76785 76585 ... ## $ falas : chr [1:37] &quot;Como é que é? Peço só um minutinho, só um minutinho! Senhor Presidente... Eu acho que Vossa Excelência.. Nós &quot;| __truncated__ &quot;Quer dizer que há outros impedimentos a serem... Acredito não ser o caso de VossaExcelência, mas o Estado de A&quot;| __truncated__ &quot;Presidente... Presidente, qual a ordem? Presidente, só para declinar a ordem, quem são? Agora é a Eliziane? &quot;| __truncated__ &quot;Senhor Presidente, eu queria, inicialmente, aqui, corrigir algumas colocações que foram feitas. O Presidente Ro&quot;| __truncated__ ... E outro tibble apenas com quem não for senador nao_senadores &lt;- NotasTaq %&gt;% filter(BlocoParl == &quot;&quot;) nao_senadores ## # A tibble: 24,296 × 9 ## reuniao data nome funcao_blocoPar BlocoParl partido estado complemento ## &lt;dbl&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 10 2021-05-20 Edua… &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ## 2 10 2021-05-20 Edua… &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ## 3 10 2021-05-20 Edua… &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ## 4 10 2021-05-20 Edua… &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ## 5 10 2021-05-20 Edua… &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ## 6 10 2021-05-20 Edua… &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ## 7 10 2021-05-20 Edua… &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ## 8 10 2021-05-20 Edua… &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ## 9 10 2021-05-20 Edua… &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ## 10 10 2021-05-20 Edua… &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ## # … with 24,286 more rows, and 1 more variable: fala &lt;chr&gt; unique(nao_senadores$nome) ## [1] &quot;Eduardo Pazuello&quot; ## [2] &quot;Mayra Pinheiro&quot; ## [3] &quot;Dimas Tadeu Covas&quot; ## [4] &quot;Nise Hitomi Yamaguchi&quot; ## [5] &quot;Luana Araújo&quot; ## [6] &quot;Marcelo Antônio Cartaxo Queiroga Lopes&quot; ## [7] &quot;Antônio Elcio Franco Filho&quot; ## [8] &quot;Natalia Pasternak&quot; ## [9] &quot;Cláudio Maierovitch&quot; ## [10] &quot;Marcellus José Barroso Campêlo&quot; ## [11] &quot;Marcellus Campelo&quot; ## [12] &quot;Wilson Witzel&quot; ## [13] &quot;Francisco Eduardo Cardoso Alves&quot; ## [14] &quot;Ricardo Ariel Zimerman&quot; ## [15] &quot;Osmar Terra&quot; ## [16] &quot;Jurema Werneck&quot; ## [17] &quot;Pedro Hallal&quot; ## [18] &quot;Luis Ricardo Fernandes Miranda&quot; ## [19] &quot;Fausto Vieira dos Santos Junior&quot; ## [20] &quot;Wagner Lima da Costa&quot; ## [21] &quot;Gina Moraes de Almeida&quot; ## [22] &quot;Carlos Roberto Wizard Martins&quot; ## [23] &quot;Alberto Zacharias Toron&quot; ## [24] &quot;Guilherme Cremonesi Caurin&quot; ## [25] &quot;Luiz Henrique Mandetta&quot; ## [26] &quot;Luiz Paulo Dominguetti Pereira&quot; ## [27] &quot;Flavio Correa de Moraes&quot; ## [28] &quot;Regina Célia Silva Oliveira&quot; ## [29] &quot;Pedro Henrique Medeiros de Araújo&quot; ## [30] &quot;Roberto Ferreira dias&quot; ## [31] &quot;Maria Jamile José&quot; ## [32] &quot;Francieli Fontana Sutile Tardetti Fantinato&quot; ## [33] &quot;Francieli Fontana Sutile Fantinato&quot; ## [34] &quot;Thiago Leônidas&quot; ## [35] &quot;William Amorim Santana&quot; ## [36] &quot;Emanuela Batista de Souza Medrades&quot; ## [37] &quot;Ticiano Figueiredo de Oliveira&quot; ## [38] &quot;Pedro Ivo Velloso&quot; ## [39] &quot;Cristiano Alberto Hossri Carvalho&quot; ## [40] &quot;Fábio Henrique Ming Martini&quot; ## [41] &quot;Amilton Gomes de Paula&quot; ## [42] &quot;Otávio de Queiroga&quot; ## [43] &quot;Marcelo Blanco da Costa&quot; ## [44] &quot;Marcelo Blanco&quot; ## [45] &quot;Eric Furtado Ferreira Borges&quot; ## [46] &quot;Nelson Luiz Sperle Teich&quot; ## [47] &quot;Airton Antonio Soligo&quot; ## [48] &quot;Emerson Paxá Pinto Oliveira&quot; ## [49] &quot;Helcio Bruno de Almeida&quot; ## [50] &quot;João Carlos Gonçalves Krakauer Maia&quot; ## [51] &quot;Jailton Batista&quot; ## [52] &quot;Ricardo Barros&quot; ## [53] &quot;Alexandre Figueiredo Costa Silva Marques&quot; ## [54] &quot;Eduardo de Vilhena Toledo&quot; ## [55] &quot;Túlio Silveira&quot; ## [56] &quot;Francisco Emerson Maximiano&quot; ## [57] &quot;Ticiano Figueiredo&quot; ## [58] &quot;Emanuel Ramalho Catori&quot; ## [59] &quot;Michel Saliba Oliveira&quot; ## [60] &quot;Roberto Pereira Ramos Júnior&quot; ## [61] &quot;Alexandre Queiroz&quot; ## [62] &quot;José Ricardo Santana&quot; ## [63] &quot;Marcelo Queiroga&quot; ## [64] &quot;Alan Diniz Moreira Guedes de Ornelas&quot; ## [65] &quot;Ivanildo Gonçalves da Silva&quot; ## [66] &quot;Francisco Araújo Filho&quot; ## [67] &quot;Cleber Lopes de Oliveira&quot; ## [68] &quot;Marcos Tolentino da Silva&quot; ## [69] &quot;Luciano Duarte Peres&quot; ## [70] &quot;Marconny Nunes Ribeiro Albernaz de Faria&quot; ## [71] &quot;Wagner de Campos Rosário&quot; ## [72] &quot;Pedro Benedito Batista Júnior&quot; ## [73] &quot;Aristides Zacarelli&quot; ## [74] &quot;Maria José Ferreira Pessoa&quot; ## [75] &quot;Vinicius Luiz Ferreira&quot; ## [76] &quot;Danilo Berndt Trento&quot; ## [77] &quot;Bruna Mendes dos Santos Morato&quot; ## [78] &quot;Antonio Barra Torres&quot; ## [79] &quot;Luciano Hang&quot; ## [80] &quot;Beno Brandão&quot; ## [81] &quot;Otávio Oscar Fakhoury&quot; ## [82] &quot;Antonio Manssur&quot; ## [83] &quot;Milena Ramos Câmara&quot; ## [84] &quot;Raimundo Nonato Brasil&quot; ## [85] &quot;Andreia da Silva Lima&quot; ## [86] &quot;Walter José Faiad de Moura&quot; ## [87] &quot;Paulo Roberto Vanderlei Rebello Filho&quot; ## [88] &quot;Walter Correa de Souza Neto&quot; ## [89] &quot;Tadeu Frederico de Andrade&quot; ## [90] &quot;Priscila Pamela Cesario dos Santos&quot; ## [91] &quot;Rosane Maria dos Santos Brandão&quot; ## [92] &quot;Mayra Pires Lima&quot; ## [93] &quot;Antonio Carlos Alves de Sá Costa&quot; ## [94] &quot;Giovanna Gomes Mendes da Silva&quot; ## [95] &quot;Katia Shirlene Castilho dos Santos&quot; ## [96] &quot;Márcio Antonio do Nascimento Silva&quot; ## [97] &quot;Elton da Silva Chaves&quot; ## [98] &quot;Fabio Wajngarten&quot; ## [99] &quot;Carlos Murillo&quot; ## [100] &quot;Ernesto Araújo&quot; 14.7.1 Frequência e wordcloud (Nuvem de palavras) Vamos observar os assuntos mais frequentes ali através da frequência de palavras. Vamos ver, de forma geral, as palavras mais frequentes. A função wordcloud do pacote wordcloud permite enviar o texto diretamente para processamento, mas esta opção não é indicada, por não permitir pré-processamento e por ser muito, bastante, extremamente lenta com a quantidade de texto que temos aqui. (Eu descobri isto testando antes). Vamos contar os valores únicos no nosso vetor “tudo.tokens” tudo &lt;- paste(NotasTaq_falas.agrupadas$falas, collapse = &quot; &quot;) # Cuidado, não rode o objeto &quot;tudo&quot;, pelo seu tamanho pode tavar o R # ao invés disso, confira sua estrutura para conferir se está ok str(tudo) ## chr &quot;Senhor Presidente, acredito que, em função da decisão, eu não sou obrigado, mas estou aqui para dizer a verdade&quot;| __truncated__ class(tudo) ## [1] &quot;character&quot; typeof(tudo) ## [1] &quot;character&quot; # vamos contar quantas palavras foram ditas no total sapply(strsplit(tudo, &quot; &quot;), length) ## [1] 3245368 tudo.df &lt;- as_tibble(tudo, falas = tudo) # tudo.tokens &lt;- tudo %&gt;% tidytext::unnest_tokens(word, falas) # tudo.tokens &lt;- tidytext::unnest_tokens(tudo.df , falas) tudo.tokens &lt;- tudo.df %&gt;% tidytext::unnest_tokens(word, value) A função wordcloud do pacote wordcloud permite enviar o texto diretamente para processamento, mas esta opção não é indicada por não permitir pré-processamento e por ser muito, bastante, extremamente lenta com a quantidade de texto que temos aqui. (Eu descobri isto testando antes). Vamos contar os valores únicos no nosso vetor “tudo.tokens” # contando todas as palavras tudo.tokens %&gt;% dplyr::count() ## # A tibble: 1 × 1 ## n ## &lt;int&gt; ## 1 3096643 # contando termos repetidos na coluna &quot;word&quot; # Isto é, gerar a tabela de frequência de termos. tudo.freq &lt;- tudo.tokens %&gt;% dplyr::count(word, sort = TRUE) # observando um pedaço head(tudo.freq, 25) ## # A tibble: 25 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 que 114473 ## 2 o 104602 ## 3 de 103575 ## 4 a 102690 ## 5 não 69265 ## 6 e 64079 ## 7 é 51658 ## 8 eu 50458 ## 9 da 43082 ## 10 do 42750 ## # … with 15 more rows O que nos dá uma lista das palavras mais frequentes, porém bem pouco informativas do assunto. Vamos retirar as palavras vazias, ou stopwords. Eu testei previamente e acrescentei à lista algumas palavras frequentes, porém pouco informativas. Vamos criar dois objetos com a mesma lista de stopwords, um como vetor e outro como dataframe, pois vamos precisar de ambos formatos. Vamos montar nossa lista de stopwords. Como testei anteriormente, vi palavras que faltaram em nossa listagem. Vamos acrescentá-las a esta listagem as stopwords já existente em stopwords(pt). # montando nossa nova listagem de stopwords # juntando a lista de `stopwords::stopwords(&#39;pt&#39;)` com a nossa SW &lt;- c(stopwords::stopwords(&#39;pt&#39;), &#39;é&#39;, &#39;aqui&#39;, &#39;então&#39;, &#39;porque&#39;, &#39;pra&#39;) SW.df &lt;- tibble(words = SW) # retirando as stopwords quase.tudo &lt;- tudo.tokens$word[!(tudo.tokens$word) %in% SW] # observando as palavras mais frequentes atuais, sem algumas stopwords head(quase.tudo, 40) ## [1] &quot;senhor&quot; &quot;presidente&quot; &quot;acredito&quot; &quot;função&quot; ## [5] &quot;decisão&quot; &quot;obrigado&quot; &quot;dizer&quot; &quot;verdade&quot; ## [9] &quot;gostaria&quot; &quot;usá&quot; &quot;los&quot; &quot;senhor&quot; ## [13] &quot;presidente&quot; &quot;primeiramente&quot; &quot;bom&quot; &quot;dia&quot; ## [17] &quot;todos&quot; &quot;gostaria&quot; &quot;iniciar&quot; &quot;fala&quot; ## [21] &quot;cumprimentando&quot; &quot;todos&quot; &quot;senadores&quot; &quot;senhora&quot; ## [25] &quot;senadoras&quot; &quot;desta&quot; &quot;comissão&quot; &quot;pessoa&quot; ## [29] &quot;senhor&quot; &quot;presidente&quot; &quot;omar&quot; &quot;aziz&quot; ## [33] &quot;eminente&quot; &quot;senador&quot; &quot;relator&quot; &quot;renan&quot; ## [37] &quot;calheiros&quot; &quot;senhora&quot; &quot;senadoras&quot; &quot;nome&quot; # testando a retirada de stopwords: &quot;tudo.tokens&quot; é maior que &quot;quase.tudo&quot;? length(tudo.tokens$word) &gt; length(quase.tudo) ## [1] TRUE length(tudo.tokens$word) ## [1] 3096643 length(quase.tudo) ## [1] 1615975 # Contando a frequência wordCount_semSW &lt;- quase.tudo %&gt;% plyr::count() |&gt; arrange(-freq) |&gt; as_tibble() wordCount_semSW ## # A tibble: 42,447 × 2 ## x freq ## &lt;chr&gt; &lt;int&gt; ## 1 senhor 32394 ## 2 senador 18665 ## 3 presidente 18033 ## 4 saúde 9071 ## 5 excelência 8916 ## 6 gente 8695 ## 7 vossa 7790 ## 8 ministério 7391 ## 9 fazer 6420 ## 10 cpi 6239 ## # … with 42,437 more rows E agora gerando nossa nuvem de palavras # pegando apenas as palavras mais frequentes pre.wc &lt;- wordCount_semSW[1:150,] wordcloud::wordcloud(pre.wc$x, # se o input para esta função contém as frequências de palavras, o item abaixo deve ser descomentado pre.wc$freq, # vetor com dois termos indicado o espectro de tamanho das palavras scale=c(3,.6), # cores, do menos frequente ao mais frequente colors = c(&quot;royalblue&quot;,&quot;blue&quot;, &quot;darkblue&quot;, &quot;black&quot;)) Vamos observar os ngramas - no caso bigramas e trigramas - que nos dão uma ideia melhor do sentido das discussões do que unigramas utilizados anteriormente. multi.palavras &lt;- tokenizers::tokenize_ngrams(tudo, # valores máximo e mínimo dos ngrams n=3, n_min = 2, stopwords = SW ) |&gt; unlist() tudo.freq &lt;- multi.palavras |&gt; plyr::count() |&gt; arrange(-freq) novas_sw &lt;- c(&quot;senador&quot;, &quot;vossa&quot;, &quot;presidente&quot;) # arrumando nossas stopwords para serem usadas com filter e grepl # Ela será um único elemento com vários operadores &quot;ous&quot; novas_sw &lt;- paste(novas_sw, collapse = &quot;|&quot; ) # retirando as novas stop words # opção 1 tudo.freq2 &lt;- tudo.freq %&gt;% filter(!grepl(novas_sw, .$x)) # opção 2 tudo.freq2 &lt;- tudo.freq[!grepl(novas_sw, tudo.freq$x),] # Observando nosso dataframe tudo.freq2[1:35,] ## x freq ## 3 ministério saúde 4703 ## 5 comissão parlamentar 2128 ## 6 comissão parlamentar inquérito 2103 ## 7 parlamentar inquérito 2103 ## 9 governo federal 1476 ## 13 senhor relator 1210 ## 14 covid 19 1166 ## 15 intervenção microfone 1162 ## 16 marcos rogério 1104 ## 17 milhões doses 991 ## 21 tratamento precoce 889 ## 22 prevent senior 791 ## 25 desta comissão 780 ## 26 questão ordem 744 ## 29 pode ser 724 ## 30 renan calheiros 722 ## 32 estados municípios 688 ## 34 polícia federal 657 ## 35 todo respeito 637 ## 36 estados unidos 628 ## 37 desta cpi 620 ## 38 fib bank 611 ## 40 todo mundo 604 ## 41 naquele momento 602 ## 42 senhor senhor 602 ## 43 obrigado senhor 588 ## 44 neste momento 585 ## 45 quer dizer 579 ## 46 roberto dias 578 ## 47 ricardo barros 571 ## 50 senhor pode 555 ## 51 alguma coisa 531 ## 52 sim senhor 523 ## 53 ministro saúde 522 ## 54 ministério público 512 Gerando uma nuvem de palavras # pegando apenas as palavras mais frequentes pre.wc &lt;- tudo.freq2[1:80,] wordcloud::wordcloud(pre.wc$x, # se o input para esta função contém as frequências de palavras, o item abaixo deve ser descomentado pre.wc$freq, # vetor com dois termos indicado o espectro de tamanho das palavras scale=c(3,.6), # cores, do menos frequente ao mais frequente colors = c(&quot;royalblue&quot;,&quot;blue&quot;, &quot;darkblue&quot;, &quot;black&quot;)) Criando um ggplot com os ngrams mais frequentes g.ngram.1 &lt;- tudo.freq2[1:30,] %&gt;% ggplot( aes(x = reorder(x, freq), y = freq)) + geom_col() + labs(title = &quot;30 ngrams mais frequentes&quot;, x = &quot;&quot;,y = &quot;&quot;) + theme(axis.text.x = element_text(angle = 90), # Deslocando o título do gráfico para ficar mais visível plot.title.position = &quot;plot&quot;) + # girando o gráfico coord_flip() g.ngram.1 g.ngram.2 &lt;- tudo.freq2[31:60,] %&gt;% ggplot( aes(x = reorder(x, freq), y = freq)) + geom_col() + labs(title = &quot;30 ngrams subsequentes&quot;, x = &quot;&quot;,y = &quot;&quot;) + theme(axis.text.x = element_text(angle = 90), # Deslocando o título do gráfico para ficar mais visível plot.title.position = &quot;plot&quot;) + # girando o gráfico coord_flip() g.ngram.1 + g.ngram.2 + plot_annotation(title = &#39;60 Bigramas e trigramas mais frequentes na CPI da Pandemia&#39;, caption = &#39;*Os gráficos estão em escalas diferentes\\nElaboração: Alisson Soares&#39;) Podemos ver que alguns termos como os trigramas “comissão parlamentar (de) inquérito” e “supremo tribunal federal” estão repetidos em bigramas. Mas bigramas e trigramas podem ser melhoradas, afim de manter termos mais significativos. 14.8 Nuvem de palavras comparativa Wordcloud comparision Vamos usar o Quanteda para alguns gráficos, como o wordcloud comparision. Para trabalhar com o Quanteda, devemos primeiro criar um objeto tipo corpus, fazer alguma restrição/filtragem, tokenizar. Alguns procedimentos exigem que se converta para Document Term Matrix. library(&quot;quanteda.textplots&quot;) #senCorpus &lt;- corpus(NT_falasJuntasCount[2:5,], # docid_field = &quot;nome&quot;, # text_field = &quot;falas&quot;) # Criando um corpus senCorpus &lt;- corpus(NT_falasJuntasCount, docid_field = &quot;nome&quot;, text_field = &quot;falas&quot;) # Pegando a fala de senadores senTokens &lt;- tokens(corpus_subset(senCorpus[c(2:5)])) # senTokens &lt;- tokens(senCorpus) dfmat2 &lt;- dfm(senTokens, dfm_remove = stopwords(&quot;portuguese&quot;), remove_punct = TRUE, dfm.group = &quot;nome&quot;) %&gt;% dfm_trim(min_termfreq = 3) ## Warning: &#39;...&#39; should not be used for tokens() arguments; use &#39;tokens()&#39; first. ## Warning: dfm_remove, dfm.group arguments are not used. ## Warning: dfm_remove, dfm.group arguments are not used. textplot_wordcloud(dfmat2, comparison = TRUE, max_words = 300) ## Warning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, : ## informações could not be fit on page. It will not be plotted. 14.8.1 Gráfico de dispersão lexical (Lexical dispersion plot) O Gráfico de dispersão lexical (Lexical dispersion plot) mostra como um ou mais termos aparecem ao longo de um documento. No caso aqui, o documento é a fala de cada senador ao longo da CPI. A plotagem demora alguns bons minutos para rodar, mais que topic modeling. # Vamos restringir o número de pessoas para nosso gráfico senTokens &lt;- tokens(corpus_subset(senCorpus[c(1:6)])) # Gerando o gráfico. Esta parte é demorada grafico &lt;- textplot_xray( kwic(senTokens, pattern = &quot;vacina&quot;), kwic(senTokens, pattern = &quot;corrupção&quot;)) Como o objeto gerado é um ggplot, podemos personalizá-lo grafico + aes(color = keyword) + scale_color_manual(values = c(&quot;darkblue&quot;, &quot;darkgreen&quot;)) + theme(legend.position = &quot;none&quot;) + #ggtitle(&quot;Dispersão Lexical&quot;) labs(title = &quot;Gráfico de dispersão lexical&quot;, x=&quot;Aparição dos termos ao longo da fala de cada um na CPI&quot;, y=&quot;&quot;, caption = &quot;Elaboração: Alisson Soares&quot;) 14.9 Extração de palavras chave Keywords - colocação (collocation) O termo “colocação” refere-se a uma sequência de palavras que ocorrem juntas mais frequentemente que separadas ou pelo acaso. Usamos esta análise para encontrar termos chamados “multiwords”, isto é, várias palavras que se comportam como se fossem uma, como por exemplo “Nova Yorque”, “Ministério Público”, etc. A função keywords_collocation() do pacote Udpipe faz este cálculo, aceitando como argumentos: input: data frames, onde cada linha é um termo e estes estão na ordem que aparecem no texto term: indicação da coluna com termos group: indicação de id (identificação) do documento Além destes, há os argumentos opcionais: ngram_max: integral, indicando o tamanho máximo das colocações. O padrão é 2. n_min: integral indicando o número mínimo das colocações. O padrão 2. sep: separador das colocações. O padrão é ” “, isto é, espaço. Mas pode ser útil usar o símbolo”_” para tornar estes termos um único termo. Esta função nos retorna um data frame com várias colunas, como veremos mais abaixo. Ela calcula: PMI (pointwise mutual information): log2(P(w1w2) / P(w1) P(w2)) MD (mutual dependency): log2(P(w1w2)^2 / P(w1) P(w2)) LFMD (log-frequency biased mutual dependency): MD + log2(P(w1w2)) PMI &lt;- NotasTaq2 |&gt; tidytext::unnest_tokens(word, fala, to_lower = FALSE) |&gt; keywords_collocation(term = &quot;word&quot;, group = &quot;nome&quot;, ngram_max = 4, sep = &quot;_&quot;) # vendo o df gerado head(PMI, 20) ## keyword ngram left right freq freq_left freq_right ## 1 Los_Angeles 2 Los Angeles 3 3 3 ## 2 BCI_Balpex 2 BCI Balpex 3 3 3 ## 3 Evelyn_Beatrice 2 Evelyn Beatrice 3 3 3 ## 4 Wesley_Cota 2 Wesley Cota 3 3 3 ## 5 Christian_Doutorsten 2 Christian Doutorsten 3 3 3 ## 6 Peace_Federation 2 Peace Federation 3 3 3 ## 7 Jandira_Feghali 2 Jandira Feghali 3 3 3 ## 8 Thermo_Fisher 2 Thermo Fisher 3 3 3 ## 9 Fla_Flu 2 Fla Flu 3 3 3 ## 10 Vossan_Holleben 2 Vossan Holleben 3 3 3 ## 11 Apoorv_Kumar 2 Apoorv Kumar 3 3 3 ## 12 Raman_Neves 2 Raman Neves 3 3 3 ## 13 Goldman_Sachs 2 Goldman Sachs 3 3 3 ## 14 Vossack_VossapoRub 2 Vossack VossapoRub 3 3 3 ## 15 Yang_Wanming 2 Yang Wanming 3 3 3 ## 16 bebidas_alcoólicas 2 bebidas alcoólicas 3 3 3 ## 17 these_are 2 these are 3 3 3 ## 18 shelf_company 2 shelf company 3 3 3 ## 19 I&#39;m_here 2 I&#39;m here 3 3 3 ## 20 pó_liofilizado 2 pó liofilizado 3 3 3 ## pmi md lfmd ## 1 19.97731 0 -19.97731 ## 2 19.97731 0 -19.97731 ## 3 19.97731 0 -19.97731 ## 4 19.97731 0 -19.97731 ## 5 19.97731 0 -19.97731 ## 6 19.97731 0 -19.97731 ## 7 19.97731 0 -19.97731 ## 8 19.97731 0 -19.97731 ## 9 19.97731 0 -19.97731 ## 10 19.97731 0 -19.97731 ## 11 19.97731 0 -19.97731 ## 12 19.97731 0 -19.97731 ## 13 19.97731 0 -19.97731 ## 14 19.97731 0 -19.97731 ## 15 19.97731 0 -19.97731 ## 16 19.97731 0 -19.97731 ## 17 19.97731 0 -19.97731 ## 18 19.97731 0 -19.97731 ## 19 19.97731 0 -19.97731 ## 20 19.97731 0 -19.97731 Após alguns testes, de ordenar pela frequência, pelo pmi ou lfmd, cheguei a esta configuração: # o 4 grams PMI |&gt; filter(ngram == 4)|&gt; filter(freq &gt; 200) |&gt; arrange(-pmi) |&gt; select(keyword, freq, pmi, md, lfmd ) |&gt; filter(stringr::str_detect(keyword, &quot;[:upper:][:lower:]+.*_[:upper:]&quot;)) |&gt; head(20) ## keyword freq pmi md lfmd ## 1 do_Rio_de_Janeiro 207 13.016019 -0.85262461 -14.72127 ## 2 Rio_Grande_do_Sul 221 12.539650 -1.23457823 -15.00881 ## 3 Conselho_Federal_de_Medicina 212 12.430687 -1.40352401 -15.23773 ## 4 de_Contas_da_União 257 12.224509 -1.33199735 -14.88850 ## 5 do_Estado_do_Amazonas 201 10.844267 -3.06681226 -16.97789 ## 6 Comissão_Parlamentar_de_Inquérito 2078 10.485215 -0.05593594 -10.59709 ## 7 do_Presidente_da_República 613 10.002059 -2.30032848 -14.60272 ## 8 o_Presidente_da_República 832 9.858514 -2.00317690 -13.86487 ## 9 a_esta_Comissão_Parlamentar 229 9.521124 -4.20180370 -17.92473 ## 10 no_Ministério_da_Saúde 706 8.840751 -3.25785565 -15.35646 ## 11 ao_Ministério_da_Saúde 467 8.811546 -3.88330632 -16.57816 ## 12 pelo_Ministério_da_Saúde 269 8.806116 -4.68455289 -18.17522 ## 13 do_Ministério_da_Saúde 1640 8.785348 -2.09730346 -12.97995 ## 14 o_Ministério_da_Saúde 1229 8.771502 -2.52735998 -13.82622 ## 15 Muito_obrigado_Senhor_Presidente 257 7.314028 -6.24247838 -19.79898 ## 16 Senhor_Presidente_Senhor_Presidente 494 6.801091 -5.81267250 -18.42644 ## 17 Tribunal_de_Contas_da 257 5.439656 -8.11685027 -21.67336 ## 18 esta_Comissão_Parlamentar_de 586 4.904648 -7.46272665 -19.83010 ## 19 nesta_Comissão_Parlamentar_de 236 4.896238 -8.78324993 -22.46274 ## 20 desta_Comissão_Parlamentar_de 462 4.895846 -7.81453585 -20.52492 # trigramas PMI |&gt; filter(ngram == 3)|&gt; filter(freq &gt; 200) |&gt; arrange(-pmi) |&gt; select(keyword, freq, pmi, md, lfmd ) |&gt; filter(stringr::str_detect(keyword, &quot;[:upper:][:lower:]+.*_[:upper:]&quot;)) |&gt; head(20) ## keyword freq pmi md lfmd ## 1 Rio_de_Janeiro 370 12.99218 -0.03863831 -13.06946 ## 2 Senador_Fernando_Bezerra 320 12.62575 -0.61452582 -13.85480 ## 3 Tribunal_de_Contas 429 12.58938 -0.22798867 -13.04536 ## 4 Grande_do_Sul 225 12.54521 -1.20321276 -14.95163 ## 5 Luis_Carlos_Heinze 229 12.38224 -1.34075961 -15.06376 ## 6 Contas_da_União 258 12.22458 -1.32639465 -14.87737 ## 7 Senador_Alessandro_Vossaeira 239 12.13850 -1.52283372 -15.18417 ## 8 Deputado_Ricardo_Barros 231 12.12336 -1.58709681 -15.29755 ## 9 Senador_Rogério_Carvalho 282 11.94951 -1.47314072 -14.89579 ## 10 nos_Estados_Unidos 244 11.80522 -1.82624272 -15.45771 ## 11 da_Prevent_Senior 310 11.75958 -1.52649619 -14.81257 ## 12 Federal_de_Medicina 214 11.72838 -2.09235413 -15.91309 ## 13 Senador_Luis_Carlos 218 11.70040 -2.09361400 -15.88763 ## 14 Roberto_Ferreira_Dias 318 11.61064 -1.63867669 -14.88800 ## 15 Senador_Omar_Aziz 267 11.53790 -1.96360333 -15.46511 ## 16 Senador_Eduardo_Braga 276 11.45507 -1.99860731 -15.45229 ## 17 de_São_Paulo 430 11.34557 -1.46843459 -14.28244 ## 18 Estados_e_Municípios 602 11.26635 -1.06222933 -13.39081 ## 19 Senador_Humberto_Costa 329 11.22966 -1.97059957 -15.17086 ## 20 Senador_Randolfe_Rodrigues 292 10.88069 -2.49169065 -15.86407 # bigramas vamos ordenar pelo pmi # (rodei antes, Por frequência não trazia bons resultados) PMI |&gt; filter(ngram == 2)|&gt; arrange(-pmi) |&gt; head(20) |&gt; select(keyword, freq, pmi, md, lfmd ) ## keyword freq pmi md lfmd ## 1 Los_Angeles 3 19.97731 0 -19.97731 ## 2 BCI_Balpex 3 19.97731 0 -19.97731 ## 3 Evelyn_Beatrice 3 19.97731 0 -19.97731 ## 4 Wesley_Cota 3 19.97731 0 -19.97731 ## 5 Christian_Doutorsten 3 19.97731 0 -19.97731 ## 6 Peace_Federation 3 19.97731 0 -19.97731 ## 7 Jandira_Feghali 3 19.97731 0 -19.97731 ## 8 Thermo_Fisher 3 19.97731 0 -19.97731 ## 9 Fla_Flu 3 19.97731 0 -19.97731 ## 10 Vossan_Holleben 3 19.97731 0 -19.97731 ## 11 Apoorv_Kumar 3 19.97731 0 -19.97731 ## 12 Raman_Neves 3 19.97731 0 -19.97731 ## 13 Goldman_Sachs 3 19.97731 0 -19.97731 ## 14 Vossack_VossapoRub 3 19.97731 0 -19.97731 ## 15 Yang_Wanming 3 19.97731 0 -19.97731 ## 16 bebidas_alcoólicas 3 19.97731 0 -19.97731 ## 17 these_are 3 19.97731 0 -19.97731 ## 18 shelf_company 3 19.97731 0 -19.97731 ## 19 I&#39;m_here 3 19.97731 0 -19.97731 ## 20 pó_liofilizado 3 19.97731 0 -19.97731 PMI |&gt; filter(ngram == 2) |&gt; arrange(-freq) |&gt; #arrange(-lfmd) |&gt; #arrange(-pmi) |&gt; head(20) |&gt; select(keyword, freq, pmi, md, lfmd ) |&gt; # pegando palavras que comecem com maiúsculo filter(stringr::str_detect(keyword, &quot;[:upper:][:lower:]+_[:upper:]&quot;)) |&gt; head(20) ## keyword freq pmi md lfmd ## 1 Vossa_Excelência 7662 8.432056 -0.2267121 -8.88548 ## 2 Senhor_Presidente 5655 6.202402 -2.8945600 -11.99152 ## 3 Comissão_Parlamentar 2110 9.436541 -1.0827057 -11.60195 ## 4 Senador_Renan 1875 7.130525 -3.5590733 -14.24867 ## 5 Governo_Federal 1472 7.775341 -3.2633704 -14.30208 ## 6 Senador_Marcos 1221 6.883017 -4.4254088 -15.73383 ## 7 Senador_Randolfe 1211 7.099158 -4.2211325 -15.54142 ## 8 Senhor_Relator 1198 6.583876 -4.7519853 -16.08785 ## 9 Marcos_Rogério 1104 10.221220 -1.2325290 -12.68628 ## 10 Presidente_Senhor 951 3.631954 -8.0370182 -19.70599 ## 11 Senador_Humberto 895 7.181998 -4.5745321 -16.33106 ## 12 Prevent_Senior 791 11.756530 -0.1782100 -12.11295 ## 13 Senador_Girão 785 6.708305 -5.2374201 -17.18314 ## 14 São_Paulo 739 10.399803 -1.6330400 -13.66588 ## 15 Senador_Eduardo 730 6.671064 -5.3794571 -17.42998 ## 16 que_Vossassa_Senhoria 726 2.134724 -9.9237241 -21.98217 ## 17 Renan_Calheiros 722 10.433340 -1.6330783 -13.69950 ## 18 Senhor_Senadores 712 6.213147 -5.8733933 -17.95993 ## 19 Senhor_Senador 672 3.070192 -9.0997638 -21.26972 ## 20 Polícia_Federal 655 9.244953 -2.9619696 -15.16889 14.10 TF-IDF dos Senadores # Observando novamente a estrutura do df que criamos só com os senadores str(senadores_falasJuntas) ## tibble [37 × 3] (S3: tbl_df/tbl/data.frame) ## $ nome : chr [1:37] &quot;Omar Aziz&quot; &quot;Renan Calheiros&quot; &quot;Randolfe Rodrigues&quot; &quot;Humberto Costa&quot; ... ## $ N_palavras: int [1:37] 413495 349545 341685 138419 125102 124592 114902 89020 76785 76585 ... ## $ falas : chr [1:37] &quot;Como é que é? Peço só um minutinho, só um minutinho! Senhor Presidente... Eu acho que Vossa Excelência.. Nós &quot;| __truncated__ &quot;Quer dizer que há outros impedimentos a serem... Acredito não ser o caso de VossaExcelência, mas o Estado de A&quot;| __truncated__ &quot;Presidente... Presidente, qual a ordem? Presidente, só para declinar a ordem, quem são? Agora é a Eliziane? &quot;| __truncated__ &quot;Senhor Presidente, eu queria, inicialmente, aqui, corrigir algumas colocações que foram feitas. O Presidente Ro&quot;| __truncated__ ... str(NT_falasJuntasCount) ## tibble [149 × 3] (S3: tbl_df/tbl/data.frame) ## $ nome : chr [1:149] &quot;Omar Aziz&quot; &quot;Renan Calheiros&quot; &quot;Randolfe Rodrigues&quot; &quot;Marcos Rogério&quot; ... ## $ N_palavras: int [1:149] 413495 349545 341685 196415 146087 138419 125102 124592 120133 114902 ... ## $ falas : chr [1:149] &quot;Como é que é? Peço só um minutinho, só um minutinho! Senhor Presidente... Eu acho que Vossa Excelência.. Nós &quot;| __truncated__ &quot;Quer dizer que há outros impedimentos a serem... Acredito não ser o caso de VossaExcelência, mas o Estado de A&quot;| __truncated__ &quot;Presidente... Presidente, qual a ordem? Presidente, só para declinar a ordem, quem são? Agora é a Eliziane? &quot;| __truncated__ &quot;Senhor Presidente, Senhora. e Senhor. Senadores, faço a presente questão de ordem, Senhor Presidente, desde log&quot;| __truncated__ ... Vemos que temos 42 senadores que falaram na CPI, mas alguns deles falaram pouco (a Senadora Daniella Ribeiro, por exemplo, disse duas palavras). Vamos restringir este número sen.tfidf &lt;- senadores_falasJuntas |&gt; arrange(desc(N_palavras)) |&gt; head(25) |&gt; # arrange(desc(N_palavras)) |&gt; head(20) tidytext::unnest_tokens( output = &#39;word&#39;, token = &#39;words&#39;, input = falas, strip_punct = T ) |&gt; dplyr::count(nome, word, sort = TRUE) |&gt; # contando os termos tidytext::bind_tf_idf(word, nome, n) |&gt; arrange(desc(tf_idf)) # Se quiser conferir antes os nomes dos senadores selecionados senadorTfIdf &lt;- sen.tfidf$nome |&gt; unique() senadorTfIdf ## [1] &quot;Styvenson Valentim&quot; &quot;Reguffe&quot; ## [3] &quot;Angelo Coronel&quot; &quot;Leila Barros&quot; ## [5] &quot;Izalci Lucas&quot; &quot;Jorge Kajuru&quot; ## [7] &quot;Roberto Rocha&quot; &quot;Soraya Thronicke&quot; ## [9] &quot;Zenaide Maia&quot; &quot;Luis Carlos Heinze&quot; ## [11] &quot;Eliziane Gama&quot; &quot;Omar Aziz&quot; ## [13] &quot;Simone Tebet&quot; &quot;Jean Paul Prates&quot; ## [15] &quot;Fabiano Contarato&quot; &quot;Otto Alencar&quot; ## [17] &quot;Eduardo Braga&quot; &quot;Marcos do Val&quot; ## [19] &quot;Randolfe Rodrigues&quot; &quot;Renan Calheiros&quot; ## [21] &quot;Jorginho Mello&quot; &quot;Alessandro Vieira&quot; ## [23] &quot;Tasso Jereissati&quot; &quot;Fernando Bezerra Coelho&quot; ## [25] &quot;Humberto Costa&quot; # Apenas as palavras de maior tf_idf de cada senador # lembrando que já organizamos antes pelo tf-idf decrescente sen.tfidf.top &lt;- sen.tfidf |&gt; group_by(nome) |&gt; slice_head(n = 7) Para não bagunçar o gráfico, vamos colocar um conjunto de nomes de cada vez # Nomes de 1 a 9. nomes &lt;- senadorTfIdf[1:9] # gerando o gráfico sen.tfidf.top |&gt; filter(nome %in% nomes ) |&gt; ggplot(aes(reorder(word, tf_idf), tf_idf, fill = nome)) + geom_bar(stat = &quot;identity&quot;, alpha = .8, show.legend = FALSE) + labs(title = &quot;Palavras peculiares utilizadas&quot;, subtitle = &quot;tf-idf da fala dos senadores&quot;, x = NULL, y = &quot;tf-idf&quot;, caption = &quot;Fonte: Notas Taquigráficas CPI da Pandemia\\nElaboração: Alisson Soares&quot;) + facet_wrap(~nome, ncol = 3, # as escalas nos diferentes gráficos não terão a mesma proporção scales = &quot;free&quot;) + coord_flip() 14.11 Dicionário A conversão em matrizes e dessa para DTM é um passo intermediário crucial em diversas abordagens de análise textual, como TF-IDF e Topic Modeling. Vamos gerar uma métrica de conceitos a partir de de dicionários de termos. Para cada conceito, escolhi um conjunto de palavras. As categorias como as palavras destas foram escolhidas de modo rápido, não sistemático. O vetor de vacinas foi comentado pois rodei previamente, os termos relacionados a vacina predominavam em todos os casos. library(quanteda) cpi.corpus &lt;- corpus(NT_falasJuntasCount, docid_field = &quot;nome&quot;, text_field = &quot;falas&quot;) # Criando um Document Term Matrix # o processo abaixo demora um pouco cpi.dfm &lt;- tokens(cpi.corpus, remove_punct = TRUE) %&gt;% dfm() %&gt;% dfm_remove(pattern = SW ) # utilizando a lista de stopwords que criamos anteriormente #tokens_select(pattern = SW, selection = &quot;remove&quot;) dict &lt;- dictionary(list( tratPre = c(&quot;cloroquina&quot;, &quot;ivermectina&quot;, &quot;azitromicina&quot;, &quot;Kit&quot;, &quot;precoce&quot;, &quot;ozônio&quot;), tratPre.defensores = c(&quot;Raoult&quot;, &quot;Zelenko&quot;, &quot;Yamagushi&quot;, &quot;Zebalos&quot;, &quot;Wong&quot;, &quot;Zanotto&quot;), # vacinas = c(&quot;vacinas?&quot;, &quot;CoronaVac&quot;, &quot;Sinopharm&quot;, &quot;CanSino&quot;, &quot;Butantan&quot;, &quot;AstraZeneca&quot;, &quot;Oxford&quot;, &quot;Comirnaty&quot;, &quot;Bharat&quot;, &quot;BioTech&quot;, &quot;BioNTech&quot;, &quot;Pfizer&quot;, &quot;Janss?en&quot;, &quot;Johnson&quot;, &quot;Spikevax&quot;, &quot;Moderna&quot;, &quot;Sputnik&quot;, &quot;Gamaleya&quot;), corrupcao = c(&quot;corrup.*&quot;, &quot;propin.*&quot;, &quot;superfatur.*&quot;, &quot;prevaric.*&quot;))) # rodando nosso dicionário dict_dtm &lt;- dfm_lookup(cpi.dfm, dictionary = dict, valuetype = &quot;regex&quot;, nomatch = &quot;_unmatched&quot;) # nomatch = &quot;semCorrespondencia&quot;) # supondo que nosso dicionário seja minimamente bom # vendo o quanto senadores abordaram certos temas dict_dtm[5:10,] ## Document-feature matrix of: 6 documents, 4 features (8.33% sparse) and 1 docvar. ## features ## docs tratPre tratPre.defensores corrupcao _unmatched ## Eduardo Girão 165 0 159 61492 ## Humberto Costa 272 11 60 56666 ## Simone Tebet 54 0 125 51126 ## Eliziane Gama 141 6 45 49696 ## Rogério Carvalho 188 7 95 50018 ## Luis Carlos Heinze 357 62 33 49758 Gerando um gráfico com o ggplot # vamos renomear os labels com os seguintes rótulos, a ser usado no ggplot rotulos &lt;- c( &quot;Corrupção&quot;, &quot;Trat. Precoce&quot;, &quot;Defensores trat. precoce&quot;) # retirar a ultima coluna que não nos é útil dict.df &lt;- quanteda::convert(dict_dtm[1:15,-5], to = &quot;data.frame&quot;) %&gt;% # dict.df &lt;- as.dataframe(dict_dtm[1:15,-5]) %&gt;% tidyr::pivot_longer(., cols = names(dict), values_to = &quot;Valores&quot;) ggplot(dict.df, aes(x = name, y = Valores, fill = name ) ) + geom_col() + labs(title = &quot;Dicionário/conceito&quot;, x = NULL, y = NULL, caption = &quot;Elaboração: Alisson Soares Observação: Cada pessoa está em uma escala diferente. O gráfico não se trata de análise empírica, mas de demonstração das ferramentas de análise&quot;, fill = &quot;Temáticas:&quot; ) + theme(legend.position=&quot;bottom&quot;, text = element_text(&quot;Temáticas:&quot;)) + # Mudando os nomes das variáveis na legenda: scale_fill_discrete(labels = rotulos) + # Retirar os rótulos: scale_x_discrete(labels = NULL) + facet_wrap(~doc_id, ncol = 3, scales = &quot;free&quot;) + # rotacionando o gráfico coord_flip() Vamos analisar com dicionário, mas agora longitudinalmente. NT.longitudinal &lt;- NotasTaq2 %&gt;% group_by(reuniao, data,) %&gt;% summarise(falas = paste(fala, collapse = &quot; &quot;)) ## `summarise()` has grouped output by &#39;reuniao&#39;. You can override using the ## `.groups` argument. head(NT.longitudinal) ## # A tibble: 6 × 3 ## # Groups: reuniao [6] ## reuniao data falas ## &lt;dbl&gt; &lt;date&gt; &lt;chr&gt; ## 1 1 2021-04-27 &quot;Invocando a proteção de Deus, declaro aberta a sessão par… ## 2 2 2021-04-29 &quot;Havendo número regimental, declaro aberta a 2ª Reunião da… ## 3 3 2021-05-04 &quot;Bom dia. Havendo número regimental, declaro aberta a 3ª R… ## 4 4 2021-05-05 &quot;Bom dia. Havendo número regimental, declaro aberta a 4ª R… ## 5 5 2021-05-06 &quot;Bom dia! Havendo número regimental, declaro aberta a 5ª R… ## 6 6 2021-05-11 &quot;Havendo número regimental, declaro aberta a 6ª Reunião da… # criando corpus corpus.longi &lt;- corpus(NT.longitudinal, docid_field = &quot;data&quot;, text_field = &quot;falas&quot;) # Criando um Document Term Matrix # o processo abaixo demora um pouco dfm.longi &lt;- tokens(corpus.longi , remove_punct = TRUE) %&gt;% dfm() %&gt;% # retirando stopwords que criamos anteriormente dfm_remove(pattern = SW ) dict &lt;- dictionary(list( tratPre = c(&quot;cloroquina&quot;, &quot;ivermectina&quot;, &quot;azitromicina&quot;, &quot;Kit&quot;, &quot;precoce&quot;, &quot;ozônio&quot;), tratPre.defensores = c(&quot;Raoult&quot;, &quot;Zelen[ck]o&quot;, &quot;Yamagushi&quot;, &quot;Zebalos&quot;, &quot;Wong&quot;, &quot;Zanotto&quot;), vacinas = c(&quot;vacinas?&quot;, &quot;CoronaVac&quot;, &quot;Butantan&quot;, &quot;AstraZeneca&quot;, &quot;Oxford&quot;, &quot;Comirnaty&quot;, &quot;BioNTech&quot;, &quot;Pfizer&quot;, &quot;Janss?en&quot;, &quot;Johnson&quot;, &quot;Spikevax&quot;, &quot;Moderna&quot;, &quot;Sputnik&quot;, &quot;Gamaleya&quot;), corrupcao = c(&quot;corrup.*&quot;, &quot;propin.*&quot;, &quot;superfatur.*&quot;, &quot;prevaric.*&quot;, &quot;crim[ei].*&quot;, &quot;fraud.*&quot;, &quot;lava[ng].*&quot;))) # rodando nosso dicionário dict_dtm &lt;- dfm_lookup(dfm.longi, dictionary = dict, valuetype = &quot;regex&quot;, #nomatch = &quot;_unmatched&quot;) nomatch = &quot;_semCorrespondencia&quot;) # supondo que nosso dicionário seja minimamente bom, vejamos: dict_dtm[5:10,] ## Document-feature matrix of: 6 documents, 5 features (6.67% sparse) and 1 docvar. ## features ## docs tratPre tratPre.defensores vacinas corrupcao _semCorrespondencia ## 2021-05-06 210 1 450 24 34576 ## 2021-05-11 85 0 500 19 24044 ## 2021-05-12 43 0 501 26 33213 ## 2021-05-13 13 1 695 17 22543 ## 2021-05-18 70 6 382 60 29226 ## 2021-05-19 100 1 158 14 23438 Criando um ggplot dict.df &lt;- as.data.frame(dict_dtm) %&gt;% tidyr::pivot_longer(., cols = names(dict), values_to = &quot;Valores&quot;) ## Warning: &#39;as.data.frame.dfm&#39; é obsoleto. ## Use &#39;convert(x, to = &quot;data.frame&quot;)&#39; em seu lugar ## Veja help(&quot;Deprecated&quot;) ggplot(dict.df, aes(x = doc_id, y = Valores, fill = name ) ) + geom_col(position = &quot;dodge&quot;) + labs(title = &quot;Dicionário/conceito&quot;, x = NULL, y = NULL, caption = &quot;Elaboração: Alisson Soares\\nO gráfico não se trata de análise empírica, mas de demonstração das ferramentas de análise&quot;) + theme(legend.position=&quot;bottom&quot;, axis.text.x = element_text(angle = 90)) E os mesmos dados, mas em gráfico de linhas: ggplot(dict.df, aes(x = doc_id, y = Valores) ) + geom_line(aes(group=name, color=as.factor(name), )) + geom_point(aes(group=name, color=as.factor(name))) + labs(title = &quot;Dicionário/conceito&quot;, subtitle = &quot;Frequência absoluta de termos relacionados às categorias&quot;, x = NULL, y = NULL, caption = &quot;Elaboração: Alisson Soares\\nO gráfico não se trata de análise empírica, mas de demonstração das ferramentas de análise&quot;, fill=&quot;Conceito&quot;) + theme(legend.position=&quot;bottom&quot;, axis.text.x = element_text(angle = 90)) 14.12 Análise de coocorrência Ao utilizar dicionário, sempre há o perigo de não usarmos os termos mais adequados. Podemos incorporar termos pouco relevantes e deixar de lado termos importantes. Um modo de lidar com este problema se dá com análise de coocorrência de termos. Com a função widyr::pairwise_count vamos contar o número de vezes que cada par de termos aparecem por pessoa. A análise de coocorrência pode multiplicar nossos problemas, uma vez que pode gerar tabela gigantesca e com isto o processamento pode ficar extremamente lento. Para se ter uma ideia, fazer análise de coocorrência de um senador de nosso dataframe pode gerar um tibble de quase 200 milhões de coocorrências. Teremos de criar um pipe de comandos especial para este tipo de análise # Tokenizando por sentença senadores_tokens &lt;- senadores_falasJuntas |&gt; select(nome, falas) |&gt; tidytext::unnest_tokens(palavras, falas, token = &quot;sentences&quot;) # Filtrando sentenças com termos que nos interessam # termos &lt;- &quot;vacin|corrup&quot; termos &lt;- &quot;corrup&quot; senadores_tokens2 &lt;- senadores_tokens %&gt;% filter(grepl(termos, .$palavras)) # tokenizar novamente, desta vez por palavra senadores_tokens3 &lt;- senadores_tokens2 |&gt; tidytext::unnest_tokens(words, palavras) |&gt; anti_join(SW.df) # retirar stopwords ## Joining, by = &quot;words&quot; cooc_pessoa &lt;- senadores_tokens3 |&gt; widyr::pairwise_count(words, nome, sort = TRUE) |&gt; arrange(-n) ## Warning: `distinct_()` was deprecated in dplyr 0.7.0. ## Please use `distinct()` instead. ## See vignette(&#39;programming&#39;) for more help ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. # arrange(cooc_pessoa, -n) filtrado &lt;- cooc_pessoa %&gt;% filter(grepl(termos, .$item1)) # termos buscados e respectiva frequência termos.encontrados &lt;- filtrado$item1 %&gt;% plyr::count() |&gt; arrange(-freq) termos.encontrados ## x freq ## 1 corrupção 2564 ## 2 corrupto 1604 ## 3 corruptos 731 ## 4 corruptas 604 ## 5 corruptores 588 ## 6 anticorrupção 344 ## 7 corrupta 281 # examinando os termos mais frequentes correlacionados aos termos buscado filtrado$item2[1:150] ## [1] &quot;presidente&quot; &quot;senhor&quot; &quot;governo&quot; ## [4] &quot;cpi&quot; &quot;gente&quot; &quot;ministério&quot; ## [7] &quot;federal&quot; &quot;pessoas&quot; &quot;brasil&quot; ## [10] &quot;todos&quot; &quot;dinheiro&quot; &quot;senador&quot; ## [13] &quot;bolsonaro&quot; &quot;saúde&quot; &quot;pode&quot; ## [16] &quot;hoje&quot; &quot;vacina&quot; &quot;pandemia&quot; ## [19] &quot;sobre&quot; &quot;disse&quot; &quot;aí&quot; ## [22] &quot;ter&quot; &quot;agora&quot; &quot;ainda&quot; ## [25] &quot;momento&quot; &quot;brasileiros&quot; &quot;crime&quot; ## [28] &quot;ser&quot; &quot;esquema&quot; &quot;combate&quot; ## [31] &quot;público&quot; &quot;caso&quot; &quot;todo&quot; ## [34] &quot;contra&quot; &quot;passiva&quot; &quot;forma&quot; ## [37] &quot;ativa&quot; &quot;qualquer&quot; &quot;corrupção&quot; ## [40] &quot;coisa&quot; &quot;recursos&quot; &quot;dentro&quot; ## [43] &quot;fazer&quot; &quot;corrupto&quot; &quot;dizer&quot; ## [46] &quot;dessas&quot; &quot;responsabilidade&quot; &quot;gente&quot; ## [49] &quot;indícios&quot; &quot;presidente&quot; &quot;brasileiro&quot; ## [52] &quot;desta&quot; &quot;meio&quot; &quot;neste&quot; ## [55] &quot;vacinas&quot; &quot;tudo&quot; &quot;senadores&quot; ## [58] &quot;falar&quot; &quot;todas&quot; &quot;investigação&quot; ## [61] &quot;desvio&quot; &quot;senhor&quot; &quot;vossa&quot; ## [64] &quot;excelência&quot; &quot;fazendo&quot; &quot;desse&quot; ## [67] &quot;governo&quot; &quot;anos&quot; &quot;administrativa&quot; ## [70] &quot;vamos&quot; &quot;casos&quot; &quot;lá&quot; ## [73] &quot;verdade&quot; &quot;toda&quot; &quot;denúncia&quot; ## [76] &quot;infelizmente&quot; &quot;precisa&quot; &quot;comissão&quot; ## [79] &quot;república&quot; &quot;ministro&quot; &quot;tentativa&quot; ## [82] &quot;acho&quot; &quot;partigo&quot; &quot;dessa&quot; ## [85] &quot;relação&quot; &quot;empresa&quot; &quot;nenhuma&quot; ## [88] &quot;crimes&quot; &quot;maior&quot; &quot;públicos&quot; ## [91] &quot;outro&quot; &quot;processo&quot; &quot;senador&quot; ## [94] &quot;vezes&quot; &quot;girão&quot; &quot;disse&quot; ## [97] &quot;deputado&quot; &quot;nome&quot; &quot;tipo&quot; ## [100] &quot;pode&quot; &quot;inclusive&quot; &quot;faz&quot; ## [103] &quot;relator&quot; &quot;renan&quot; &quot;estado&quot; ## [106] &quot;hoje&quot; &quot;brasileira&quot; &quot;lá&quot; ## [109] &quot;ministério&quot; &quot;incompetência&quot; &quot;ainda&quot; ## [112] &quot;lavagem&quot; &quot;importante&quot; &quot;cada&quot; ## [115] &quot;dias&quot; &quot;fatos&quot; &quot;menos&quot; ## [118] &quot;covaxin&quot; &quot;nesse&quot; &quot;alguém&quot; ## [121] &quot;miranda&quot; &quot;ver&quot; &quot;compra&quot; ## [124] &quot;vossassa_senhoria&quot; &quot;esquema&quot; &quot;país&quot; ## [127] &quot;exemplo&quot; &quot;polícia&quot; &quot;milhões&quot; ## [130] &quot;grande&quot; &quot;área&quot; &quot;us&quot; ## [133] &quot;além&quot; &quot;nessa&quot; &quot;tempo&quot; ## [136] &quot;questão&quot; &quot;algo&quot; &quot;claro&quot; ## [139] &quot;fraude&quot; &quot;apenas&quot; &quot;assim&quot; ## [142] &quot;seguinte&quot; &quot;investigação&quot; &quot;fazer&quot; ## [145] &quot;pessoas&quot; &quot;vossa&quot; &quot;excelência&quot; ## [148] &quot;brasil&quot; &quot;dizer&quot; &quot;bem&quot; Podemos utilizar esta busca de termos que co ocorreram com termos relacionados à corrupção (ou outros termos de preferência), para com isto encontrar termos novos relacionados a esta temática, que não constam no dicionário que usamos anteriormente. library(udpipe) valor_skipgram &lt;- 5 cooc_pessoas &lt;- udpipe::cooccurrence(senadores_tokens3, # group: nome da coluna com &quot;id&quot; group = &quot;nome&quot;, # term: coluna com palavras a serem contadas term = &quot;words&quot;, skipgram = valor_skipgram ) cooc.corrup &lt;- cooc_pessoas %&gt;% filter(grepl(&quot;corrup&quot;, .$term1)) cooc.corrup[1:30,] |&gt; ggplot( aes(x=item2, y=n))+ geom_col() + # rotacionando o gráfico coord_flip() labs(title = &#39;Palavras que coocorreram com termo &quot;corrupção&quot;&#39;) ggraph::ggraph(cooc.corrup[1:70,] , layout = &quot;fr&quot;) + geom_edge_link(aes(width = n, edge_alpha = n), edge_colour = &quot;lightskyblue&quot;) + geom_node_text(aes(label = name), col = &quot;darkgreen&quot;, size = 4) 14.13 Modelagem de tópicos A modelagem de tópicos pode demorar, dependendo das configurações de sua máquina e do tamanho do seu corpus a ser processado. A função pryr::object_size(par_dtm) nos retornou que nosso objeto possui 7,6 Mb de tamanho, ou 7.3 Mb de tamanho sem as stopwords. Usando a função system.time(funcao) é possível medir o tempo gasto por determinada tarefa. Assim, um computador i5 com 8Gb de Ram demorou 662.695 segundos - cerca de 11 minutos - para rodar esta modelagem de tópicos de um arquivo de 7,6 Mb. library(topicmodels) texts &lt;- corpus_reshape(cpi.corpus, to = &quot;paragraphs&quot;) #par_dtm &lt;- dfm(texts, stem = TRUE, # create a document-term matrix # remove_punct = TRUE, # remove = stopwords(&quot;english&quot;) par_dtm &lt;- dfm_trim(cpi.dfm, min_count = 5) # remove rare terms par_dtm &lt;- convert(par_dtm, to = &quot;topicmodels&quot;) # convert to topicmodels format set.seed(1) valor_k &lt;- 7 lda_model &lt;- topicmodels::LDA(par_dtm, method = &quot;Gibbs&quot;, k = valor_k) # vendo nossa modelagem de tópicos, 5 primeiras linhas terms(lda_model, 5) ## Topic 1 Topic 2 Topic 3 Topic 4 Topic 5 ## [1,] &quot;brasil&quot; &quot;gente&quot; &quot;senador&quot; &quot;senhor&quot; &quot;senhor&quot; ## [2,] &quot;senador&quot; &quot;pessoas&quot; &quot;senhor&quot; &quot;presidente&quot; &quot;presidente&quot; ## [3,] &quot;tratamento&quot; &quot;pra&quot; &quot;aí&quot; &quot;cpi&quot; &quot;sa&quot; ## [4,] &quot;milhões&quot; &quot;ter&quot; &quot;vossa_excelência&quot; &quot;governo&quot; &quot;v&quot; ## [5,] &quot;vacinas&quot; &quot;ser&quot; &quot;gente&quot; &quot;vossa_excelência&quot; &quot;comissão&quot; ## Topic 6 Topic 7 ## [1,] &quot;saúde&quot; &quot;senhor&quot; ## [2,] &quot;senador&quot; &quot;sim&quot; ## [3,] &quot;ministério&quot; &quot;senador&quot; ## [4,] &quot;vacina&quot; &quot;empresa&quot; ## [5,] &quot;relação&quot; &quot;dia&quot; Vamos visualizar o topic modeling com o ggplot: # convertendo para o formato tidy topicos &lt;- tidytext::tidy(lda_model, matrix = &quot;beta&quot;) termos_p_topico &lt;- 10 top_termos &lt;- topicos %&gt;% group_by(topic) %&gt;% top_n(termos_p_topico, beta) %&gt;% ungroup() %&gt;% arrange(topic, -beta) # top_n() doesn&#39;t handle ties -__- so just take top 10 manually top_termos &lt;- top_termos %&gt;% group_by(topic) %&gt;% slice(1:termos_p_topico) %&gt;% ungroup() top_termos$topic &lt;- factor(top_termos$topic) top_termos %&gt;% mutate(term = reorder(term, beta)) %&gt;% ggplot(aes(term, beta)) + geom_bar(stat = &quot;identity&quot;) + facet_wrap(~ topic, scales = &quot;free&quot;) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + coord_flip() + labs(title = &quot;Modelagem tópicos - Topic Modeling&quot;, subtitle = paste0(&quot;k = &quot;, valor_k, &quot;, feito a partir das falas dos senadores&quot;), caption = &quot;Elaboração: Alisson Soares&quot;) "],["datasets.html", "15 Apêndice: Datasets 15.1 Pacotes R com datasets 15.2 Datasets em sites diversosi: 15.3 Datasets no Kaggle:", " 15 Apêndice: Datasets Alguns datasets/databases para análise textual 15.1 Pacotes R com datasets Alguns pacotes R com datasets inclusos em pacotes que podem ser interessantes para análise textual: pacote corpus: Text Corpus Analysis. Possui estas base de dados: federalist com os 85 textos dos federalistas, um texto por linha. gutenberg_corpus corpus de textos do projeto Gutenberg, criando um texto por linha. O pacote Harry Potter de Bradley Boehmke (não confundir com o pacote homônimo no CRAN) possui o texto completo de sete primeiros livros da série Harry Potter. Para instalar, utilize o comando devtools::install_github(\"bradleyboehmke/harrypotter\") e para carregá-lo library(harrypotter). Requer o pacote devtools instalado. Hathi Trust hathiTools, disponível no Github. “This package allows you to interact with various free data resources made available by the Hathi Trust digital library, including the Hathi Trust Bookworm, a tool similar to the Google ngram viewer and the Hathi Trust Workset Builder 2.0. It also allows you to download and process the Hathi Trust Extracted Features files, which contain per-page word counts and part-of-speech information for over 15 million digitised volumes, including many of those originally digitised by Google for its Google Books project.” datasets-package The corpus data_corpus_udhr contains the Universal Declaration of Human Rights in over 400 languages. data_corpus_udhr[c(\"eng\", \"deu_1996\", \"arb\", \"heb\", \"cmn_hans\", \"jpn\")] Mídia de massa data('acq', package = \"tm\") dataset com 50 artigos da Reuters em um objeto tipo VCorpus. “acq” do pacote tm contém: 50 artigos da Reuters e suas meta-informações, referentes à aquisições corporativas; “crude” do pacote tm com 20 artigos, também da Reuters, que versam sobre “crude oil”. Filmes: GitHub - beanumber/imdb: “R package to load the IMDB into a database.” A função corpus::loadImdb() retorna uma lista com resenhas de filmes do IMDbd Música: Pacote vagalumeR, API que pega dados do site Vagalume. “Vagalume API via R. You can get information about specific artists/bands, as their related artists, and information about the lyrics, as the top lyrics of an artist/band and the text of a song itself.” chorrrds. “is a package for R that scrapes the Cifraclub website to download and organize music chords. It can be considered a package for MIR (Music Information Retrieval), a broad area of computational music which extracts and processes music data, from the unstructured ones, as sound waves, to structured, like sheet music or chords.” Rspotify. “This package allows you to connect R to Spotify’s API and get information about Songs, Albums, Artists and Users.” Dataset billboard do pacote tidyr: “Song rankings for Billboard top 100 in the year 2000” Política: Pacote speechbr raspa discursos e Notas Taquigráficas da Câmara dos Deputados do Brasil. data(presidential_debates_2012) do pacote textstem, um dataset com versão limpa de três debates presidenciais dos EUA da eleição de 2012. Pacote sotu que contém o texto dos discursos inaugurais de todos os presidentes dos EUA. Constitucionalmente, o presidente que toma posse deve fazer um discurso onde aponta os principais desafios que a nação irá enfrentar. Estes discursos inaugurais dos presidentes dos EUA também se encontram na base data_corpus_inaugural do pacote Quanteda. O pacote corpus possui datasets como The Federalist Papers. Pacote DemocracyData com diversos datasets de medidas de democracia no mundo. Aqui um vignette de uso do pacote. 15.2 Datasets em sites diversosi: Site do Projeto Gutenberg possui diversos livros gratuitos em formato texto puro e gratuito. O pacote gutenbergr pode auxiliar neste processo. Tweets de Trump no The Trump Archive ou já no formato R com trump_tweets: Trump Tweets from2009 to 2017. fivethyrteight russian troll tweets: 2,973,371 tweets da “fábrica de trolls” da Agência de Pesquisa da Internet da Rússia, que foram usados para tentar influenciar as eleições nos EUA. Mais detalhes em Why We’re Sharing 3 Million Russian Troll Tweets que virou um working paper Troll Factories: The Internet Research Agency and State-Sponsored Agenda Building. slaves voyage database com registros de navios negreiros que saíram da África, com local de partida e chegada, ano, quantos escravizados chegaram, nomes do barco e do capitão. R datasets. Lista com cerca de 1700 datasets em csv. Esta lista também está disponível em csv e html. Na versão html, há busca inclusa e ao clicar em “doc”, aparece uma descrição um pouco mais detalhada do dataset. Notícias verdadeiras e falsas em português no Fake.Br Corpus Site Brasil.io possui datasets como: Boletins informativos e casos de Coronavirus por município no Brasil; Cursos e notas de corte do Prouni 2018; Dados das eleições brasileiras desde 1996; gastos dos deputados e magistrados; Sócios das Empresas no Brasil; Classificação de gênero em nomes brasileiros, com base nos dados do censo 2010. Site Base dos dados possui diversos datasets prontos para uso: ParlSpeech “O ParlSpeech V2 contém vetores com texto completo de mais de 6,3 milhões de discursos parlamentares nas principais câmaras legislativas da Áustria, República Tcheca, Alemanha, Dinamarca, Holanda, Nova Zelândia, Espanha, Suécia e Reino Unido, cobrindo períodos entre 21 e 32 anos” entre os anos de 2000 e 2019. Religião: Diversas versões da bíblia, em diferentes traduções, em línguas variadas (inglês, português, grego, hebreu, etc.) e em diferentes formatos (Json, csv, sql, xml, etc.) no site hackathon.bible 15.3 Datasets no Kaggle: Star Trek Scripts. “Raw text scripts and processed lines of all Star Trek series scripts”. (Necessita de conta no Kaggle). Lista com 31 datasetes para datascience, contendo, por exemplo: dataset com 300k de artigos da CNN, outro com o Wiki how to coleção de papers do Arxiv, outro com texto completo de artigos relacionados à Covid-19, coleção sobre filmes do Neflix, dentre outros. Lista no Google Drive com vários datasets estruturados, vários podem ser usados em análise textual, entre eles: 1,8 bilhões de páginas de livros do Halthi Trust, dados sobre julgamento por bruxaria na Escócia do século XVII, bases de dados sobre golpes de estado no mundo, dados etnográficos sobre caçadores-coletores, etc. Outros sites para buscar datasets: https://data.world/ https://dados.gov.br/ https://data.nasdaq.com https://archive.ics.uci.edu/ Machine Learning Repository https://fivethiryeight.com/ https://github.com/BuzzFeedNews "]]
