[["análise-textual-text-mining.html", "Capítulo 8 Análise Textual (text mining) 8.1 Introdução 8.2 Abordagens: saco de palavras (bag of words) e análise semântica (semantic parsing) 8.3 Abordagem Bag of words 8.4 Nuvem de palavras 8.5 Remoção de palavra vazia (stopwords) 8.6 Estemização (stemming) e lematização 8.7 Palavras em contexto (keyword-in-context KWIC) 8.8 Key term extraction", " Capítulo 8 Análise Textual (text mining) CAPÍTULO AINDA EM CONSTRUÇÃO Conteúdo planejado: Introdução à análise textual via computador Tipos de abordagens: bag of words, semantic parsing. Frequência de termos (bag of words, n-grams, skipgrams, TF-IDF) nuvem de palavras (wordclouds), Polarized tag cloud, pyramid plot (em expansão). Correlação de palavras, tipos de distâncias, dendogramas parts-of-speech tagging keyword extraction (em expansão) redes de palavras (word networks) Inteligência Artificial: clusterização; topic modelling Análise de sentimentos. Orientações para ler este capítulo: Em vários dos códigos aqui presentes, usaremos a notação pacote::função, que dispensa carregar o pacote previamente. Apesar de desnecessária caso o pacote seja carregado anteriormente, ele facilitar saber qual função de que pacote está sendo usada, além de desambiguar, uma vez que há funções de nome idêntico em pacotes diferentes. Assim, sempre confira se o pacote utilizado no exemplo já está instalado em sua máquina. No Rstudio, basta ir à aba “packages” e fazer a busca na lupa para conferir. Se preferir usar o console do R, use o comando installed.packages()[,1] para listar todos os pacotes instalados, e grep(\"dplyr\", installed.packages()[,1], value=T) para checar se um pacote (no caso, o dplyr) está instalado. Há também uma listagem com datasets para usar na análise textual, algumas inclusive já no formato R. 8.1 Introdução A análise computacional de textos é praticamente um sinônimo de Mineração de Texto (text mining) e tem muito em comum com o campo de Processamento de Língua Natural ou Processamento de Linguagem Natural, mas não são exatamente a mesma coisa. Como vimos no capítulo sobre história da análise textual, esta existia antes antes da inteligência artifical e mesmo dos computadores. Há diversas funções nativas do R que usamos na mineração de texto/análise textual, mas também há diversas suítes de pacotes (pacotes com vários pacotes, com várias funções) focados em análise textual com diversas ferramentas, como o tidytext, quanteda (QUantitative ANalysis TExtual DAta), OpenNLP, Rweka, languageR, koRpus, RcmdrPlugin.temis, RKEA (R Keyphrase Extraction Algorithm), tm (Text Mining Package) e qdap (Quantitative Discourse Analysis Package). Estas são algumas das mais famosas suítes de pacotes, com diversas ferramentas, mas há alguns outros pacotes focados em funções mais específicas, como o pacote wordcloud, ggwordcloud (nuvem de palavras para o ggplot2, com mais opções), por exemplo. Há redundância entre estes pacotes, isto é, eles tem funções próprias muito semalhantes às funções de outros pacotes, o que não quer dizer que não existam diferenças significativas. Há também pacotes em R para análise textual em modo gráfico. Um software bem conhecido de análise textual e que possui interface gráfica é o iramuteq (Interface de R pour les Analyses Multidimensionnelles de Textes et de Questionnaires), criado em 2009 por Pierre Ratinaud. Apesar de ainda ser bastante utilizado, o Iramuteq tem diversas limitações. Vimos um pouco sobre o RCommander. Há um plugin para ele dedicado à análise textual, o RcmdrPlugin.temis. Porém, sua última atualização ocorreu em 2018. Dicas lista com diversos pacotes R, relacionados à Processamento de Linguagem Natural. Lista extensa, porém desatualizada. 8.2 Abordagens: saco de palavras (bag of words) e análise semântica (semantic parsing) Na análise textual podemos analisar levando ou não em consideração a ordem das palavras ou sua função gramatical. Se o ordenamento ou a função das palavras não é importante, e queremos saber, por exemplo, apenas a frequência de termos, então faremos uma abordagem tipo “saco de palavras” (bag of words). Se precisamos saber as classes gramaticais, então a ordem das palavras é importante. Vamos começar os exemplos com um pacote que pega dados do Google Ngram e nos retorna frequência de termos longitudinalmente, com base de dados do Google Books. 8.3 Abordagem Bag of words Na abordagem de “saco de palavras” (bag of words) a ordem dos termos não importa, bem como geralmente não importa a sua classe gramatical. 8.3.1 Frequência de palavras/termos e Ngrams Numa abordagem do tipo saco-de-palavras, a abordagem mais simples, mas sempre útil, é verificar a frequência de certos termos. Apesar de simples, análises mais sofisticadas podem começar com a análise de frequência e partir para abordagens mais sofisticadas. aprendizado instrumental de uma língua, ao identificar as palavras mais frequentes em certa área do conhecimento. detecção de língua detecção de termos mais frequentes em uma busca identificação de palavras compostas 8.3.1.1 n-gram: explicando o conceito Se partirmos do exemplo da frase “Ivo viu a uva” teremos unigram N=1 “a” “viu” “Ivo” “uva” bigrams N=2 “a uva” “Ivo viu” “viu a” trigrams N=3 “Ivo viu a” “viu a uva” ngram=4 N=4 “Ivo viu a uva” … … … Quando o N passa de 3, chamamos de ngram e seu valor. Agora um exemplo com o R. Primeiro veremos exemplos com o Google Ngram, que é mais simples, e depois montaremos nosso próprio ngram. 8.3.1.2 GoogleNgrams A Google pegou sua enorme base de dados dos milhares de livros do Google Books e extraiu os termos mais frequentes, e os colocou disponível para consulta no site Goolge Books Ngram Viewer. O Google Ngrams facilitou a busca por ngrams nesta base de dados, naquilo que chamavam de “culturonomics”. O nome não pegou, a ferramenta tem suas limitações, mas ainda assim pode ser bem útil. A base de dados possui 5.2 milhões de livros, cerca de 4% de todos os livros já publicados. Para mais informações sobre a base de dados e sobre o GoogleNgram no site. Tanto o Python (com o get-ngrams) como o R (ngramr) possuem pacotes que usam os dados do Google Ngram. Instalando o pacote ngramr install.packages(&#39;ngramr&#39;) Carregando os pacote library(ngramr) E um exemplo de uso ng &lt;- ngramr::ngram(c(&quot;Max Weber&quot;, &quot;Émile Durkheim&quot;), year_start = 1890) ggplot2::ggplot(ng, aes(x=Year, y=Frequency, colour=Phrase)) + geom_line() Um exemplo da página do ngramr no Github com mais opções, usando a função ggram() no ngramr, que pega dados do GoogleNgram e plota os dados com o ggplot2: ggram(c(&quot;monarchy&quot;, &quot;democracy&quot;), year_start = 1500, year_end = 2000, corpus = &quot;eng_gb_2012&quot;, ignore_case = TRUE, geom = &quot;area&quot;, geom_options = list(position = &quot;stack&quot;)) + labs(y = NULL) É possível mudar entre diferentes corpus, que neste caso representam as diferentes línguas, como “eng_us_2019”, “eng_gb_2019”, “chi_sim_2019”, “fre_2019”, “ger_2019”, “heb_2019”, “ger_2012”, “spa_2012”, “rus_2012”, “ita_2012”. Para ver todos os corpus disponíveis veja no site busque a sessão “Corpora”. Infelizmente, não há corpus em português no Google Ngram. classicos = c(&quot;Max Weber&quot;, &quot;Émile Durkheim&quot;, &quot;Karl Marx&quot;, &quot;Gabriel Tarde&quot;, &quot;Georg Simmel&quot;) ggram(classicos, year_start = 1980, year_end = 2000, # Para mudar lingua, mude o corpus # ignore case: se diferencia maiúsculo de minúsculo corpus = &quot;fre_2019&quot;, ignore_case = TRUE, # tipo de grafico em geom geom = &quot;line&quot;, geom_options = list()) + # labs: label do eixo y labs(y = NULL) Dicas Ngramr: Site do Books Ngram Viewer explicando seus parâmetros. PDF com a documentação do ngramr Instalação/Primeiros passos com o Ngramr na página do Github do ngramr Um projeto similar ao Google Ngram - inclusive usando parte do mesmo pessoal - é o bookworm:HalthiTrust do projeto Halthi Trust-Digital Livrary, com muito mais línguas, inclusive o português e mais opções de busca. 8.3.1.3 N-grams no R library(ngram) ## ## Attaching package: &#39;ngram&#39; ## The following object is masked from &#39;package:ngramr&#39;: ## ## ngram Vamos pegar um trecho de Alfred Shutz. txt=&quot;A Fenomenologia busca o início real de todo pensamento filosófico... Seu lugar é além - ou melhor, antes - de todas as distinções entre realismo e idealismo.&quot; Vamos quebrar o texto em ngrams. Geralmente usa-se valores entre 1 e 3. ng &lt;- ngram::ngram(txt, # n = valor do ngram n=3) # imprimindo o objeto que criamos, que mostra o total de ngrams ng ## An ngram object with 25 3-grams # imprimindo os ngrams gerados. ngram::get.ngrams(ng) ## [1] &quot;além - ou&quot; &quot;início real de&quot; ## [3] &quot;- de todas&quot; &quot;antes - de&quot; ## [5] &quot;busca o início&quot; &quot;entre realismo e&quot; ## [7] &quot;o início real&quot; &quot;A Fenomenologia busca&quot; ## [9] &quot;distinções entre realismo&quot; &quot;lugar é além&quot; ## [11] &quot;as distinções entre&quot; &quot;de todo pensamento&quot; ## [13] &quot;filosófico... Seu lugar&quot; &quot;é além -&quot; ## [15] &quot;- ou melhor,&quot; &quot;de todas as&quot; ## [17] &quot;Fenomenologia busca o&quot; &quot;melhor, antes -&quot; ## [19] &quot;todo pensamento filosófico...&quot; &quot;todas as distinções&quot; ## [21] &quot;ou melhor, antes&quot; &quot;realismo e idealismo.&quot; ## [23] &quot;real de todo&quot; &quot;Seu lugar é&quot; ## [25] &quot;pensamento filosófico... Seu&quot; Diversos outros pacotes::funções fazem a quebra em ngrams, como RWeka::NGramTokenizer ou o quanteda. A função de ngram do quanteda tem a vantagem de poder definir um escopo de valores de ngram de uma vez, podendo gerar unigramas, bigramas e trigramas com um só comando. Em outros pacotes isto é possível apenas com pós processamento. A quebra do texto em ngrams faz mais sentido quando, com eles, observamos os termos mais repetidos. Isso é o que vamos fazer a seguir. Num exemplo mais prático, fomos até o site gutenberg (site com vários livros gratuitos) e pegamos o link para o txt do livro “O Príncipe” de Maquiavel, em inglês. # link para o livro &quot;The Prince&quot; de Maquiavel. url.prince = &quot;https://www.gutenberg.org/files/1232/1232-0.txt&quot; # carregando o url num objeto R maquiavel &lt;- readLines(url(url.prince)) Observando a estrutura do objeto “maquiavel” que acabamos de criar: # Como o objeto importado está como um vetor com vários elementos: str(maquiavel) ## chr [1:5188] &quot;The Project Gutenberg eBook of The Prince, by Nicolo Machiavelli&quot; ... # vemos que é um vetor com 5.188 itens. Precisamos transformar estes vários vetores em um só elemento com o comando # paste0(var, collapse = &quot; &quot;) maquiavel2 &lt;- paste(maquiavel, collapse = &quot; &quot;) Por hora, usaremos o pacote tradicional ngram, escrito em C, e por isso, rápido. Podemos fazer a sumarização (summarizing) obtendo a frequência de vezes que um ngram apareceu no texto, bem como também a frequência relativa (proporcional) com a função ngram:: get.phrasetable, que retorna um data frame. Rode o ngram com diferentes valores para ver qual deles ertorna resultados mais informativos do conteúdo. prince_ngrams &lt;- ngram::get.phrasetable(ngram::ngram(maquiavel2, n = 3)) # restringindo aos trigramas mais frequentes prince_ngrams[1:16,] ## ngrams freq prop ## 1 he did not 20 0.0003770526 ## 2 it is necessary 20 0.0003770526 ## 3 in order to 19 0.0003582000 ## 4 the King of 19 0.0003582000 ## 5 Project Gutenberg-tm electronic 18 0.0003393473 ## 6 ought to be 18 0.0003393473 ## 7 in such a 18 0.0003393473 ## 8 prince ought to 18 0.0003393473 ## 9 so as to 17 0.0003204947 ## 10 that it was 15 0.0002827894 ## 11 those who have 15 0.0002827894 ## 12 if he had 15 0.0002827894 ## 13 for him to 14 0.0002639368 ## 14 the Project Gutenberg 14 0.0002639368 ## 15 such a way 14 0.0002639368 ## 16 the death of 13 0.0002450842 Dicas Sugestão de leitura JURAFSKY, Dan.; MARTIN, James H. cap.3 N-gram Language Models Manual do pacote ngram 8.4 Nuvem de palavras Vários pacotes fazem as chamadas nuvem de palavras no R. Um deles é o wordcloud, que além de fazer nuvens de palavras, também é capaz de fazê-lo comparando documentos, Em sua forma mais simples texto_tocqueville &lt;- &quot;Em nosso tempo, a liberdade de associação tornou-se uma garantia necessária contra a tirania da maioria. Nos Estados Unidos, quando uma vez um partido se toma dominante. todo o poder público passa para as suas mãos; seus amigos particulares ocupam todos os empregos e dispõem de todas as forças organizadas. Como os homens mais distintos do partido contrário não podem atravessar a barreira que os separa do poder, é preciso que possam se estabelecer fora; é preciso que a minoria oponha sua força moral inteira ao poderio material que a oprime. Opõe-se, pois, um perigo a um perigo mais temível. A onipotência da maioria parece-me um risco tão grande para as repúblicas americanas que o meio perigoso que se usa para limitá-la parece-me, ainda assim, um bem. Exprimirei aqui um pensamento que lembrará o que disse em outra parte a respeito das liberdades comunais: não há país em que as associações sejam mais necessárias, para impedir o despotismo dos partidos ou a arbitrariedade do príncipe, do que aquele em que o estado social é democrático. Nas nações aristocráticas, os corpos secundários formam associações naturais que detêm os abusos de poder. Nos países em que semelhantes associações não existem, se os particulares não podem criar artificial e momentaneamente alguma coisa que se lhes assemelhe, não percebo mais nenhum dique contra nenhuma sorte de tirania, e um grande povo pode ser oprimido impunemente por um punhado de facciosos ou por um homem. ... Não podemos dissimular que a liberdade ilimitada de associação, em matéria política, é, de todas as liberdades, a última que um povo pode suportar. Se ela não o faz cair na anarquia, o faz tocá-la por assim dizer a cada instante. Essa liberdade, tão perigosa, oferece porém num ponto algumas garantias: nos países em que as associações são livres, as sociedades secretas são desconhecidas. Na América, há facciosos, mas não conspiradores.&quot; E para criar uma nuvem de palavras simples, basta usar o comando: wordcloud::wordcloud(texto_tocqueville) ## Warning in tm_map.SimpleCorpus(corpus, tm::removePunctuation): transformation ## drops documents ## Warning in tm_map.SimpleCorpus(corpus, function(x) tm::removeWords(x, ## tm::stopwords())): transformation drops documents wordcloud::wordcloud(texto_tocqueville, # número mínimo de repetições que uma palavra tem de ter para entrar no gráfico min.freq = 2, # cores, do menos frequente ao mais frequente colors = c(&quot;royalblue&quot;,&quot;blue&quot;, &quot;darkblue&quot;)) ## Warning in tm_map.SimpleCorpus(corpus, tm::removePunctuation): transformation ## drops documents ## Warning in tm_map.SimpleCorpus(corpus, function(x) tm::removeWords(x, ## tm::stopwords())): transformation drops documents Vemos que algumas palavras frequentes não nos dizem muita coisa, como “que”, “nos”, “para”. Como queremos apreender algo do sentido do texto com a nuvem de palavras, seria interessante remover tais termos pouco significativos, as chamadas “palavras vazias” ou “stopwords”. 8.5 Remoção de palavra vazia (stopwords) Ao analisarmos texto, o mais frequente são palavras bem pouco informativas, como artigos “o”, “a” “os”, “as”. Para termos uma noção melhor removemos as chamadas “stopwords”. manifesto &lt;- &quot;A História de toda a sociedade até hoje é a história da luta de classes.&quot; # criamos uma pequena lista de stopwords minhas_sw &lt;- c(&quot;a&quot;,&quot;o&quot;, &quot;e&quot;, &quot;da&quot;, &quot;de&quot;, &quot;do&quot;) # transformando o texto em vetor manif_vetor &lt;- manifesto %&gt;% # convertendo o texto todo para minúsculo tolower %&gt;% # quebrando o texto em vetores strsplit(., &quot; &quot;) %&gt;% # o comando strplit retorna lista. Vamos forçar para retornar como vetor char unlist manif_vetor ## [1] &quot;a&quot; &quot;história&quot; &quot;de&quot; &quot;toda&quot; &quot;a&quot; &quot;sociedade&quot; ## [7] &quot;até&quot; &quot;hoje&quot; &quot;é&quot; &quot;a&quot; &quot;história&quot; &quot;da&quot; ## [13] &quot;luta&quot; &quot;de&quot; &quot;classes.&quot; Por se tratar de vetor, podemos usar comando tradicionais, com os operadores ! que indica negação, e %in% que checa se algo está contido em um vetor. manif_vetor[!(manif_vetor) %in% minhas_sw] ## [1] &quot;história&quot; &quot;toda&quot; &quot;sociedade&quot; &quot;até&quot; &quot;hoje&quot; &quot;é&quot; ## [7] &quot;história&quot; &quot;luta&quot; &quot;classes.&quot; Explicando: (manif_vetor) %in% minhas_sw checa se itens de “manif_vetor” estão contidos em “minhas_sw”. Retorna um booleanoo de “TRUE” e “FALSE”. ! inverte o comando anterior, checando agora quais itens de “manif_vetor” não estão contidos em “minhas_sw”, também retornando um vetor com booleanos de “TRUE” e “FALSE”. Para obter os valores (as palavras), jogamos esta fórmula anterior dentro de “manif_vetor[fórmula_anterior]”. As stopwords costumam ser as mesmas. E se já houvesse uma lista pronta? Existe. É possível encontrar listas prontas na internet, mas diversas funções no R já incluem em si tais listas. Para ver a lista padrão no R, use: library(tm, quietly = T) # pegando apenas as primeiras 20 stopwrods em inglês tm::stopwords(&quot;en&quot;) %&gt;% head(.,20) ## [1] &quot;i&quot; &quot;me&quot; &quot;my&quot; &quot;myself&quot; &quot;we&quot; ## [6] &quot;our&quot; &quot;ours&quot; &quot;ourselves&quot; &quot;you&quot; &quot;your&quot; ## [11] &quot;yours&quot; &quot;yourself&quot; &quot;yourselves&quot; &quot;he&quot; &quot;him&quot; ## [16] &quot;his&quot; &quot;himself&quot; &quot;she&quot; &quot;her&quot; &quot;hers&quot; # vendo a lista em português tm::stopwords(&quot;pt&quot;) ## [1] &quot;de&quot; &quot;a&quot; &quot;o&quot; &quot;que&quot; &quot;e&quot; ## [6] &quot;do&quot; &quot;da&quot; &quot;em&quot; &quot;um&quot; &quot;para&quot; ## [11] &quot;com&quot; &quot;não&quot; &quot;uma&quot; &quot;os&quot; &quot;no&quot; ## [16] &quot;se&quot; &quot;na&quot; &quot;por&quot; &quot;mais&quot; &quot;as&quot; ## [21] &quot;dos&quot; &quot;como&quot; &quot;mas&quot; &quot;ao&quot; &quot;ele&quot; ## [26] &quot;das&quot; &quot;à&quot; &quot;seu&quot; &quot;sua&quot; &quot;ou&quot; ## [31] &quot;quando&quot; &quot;muito&quot; &quot;nos&quot; &quot;já&quot; &quot;eu&quot; ## [36] &quot;também&quot; &quot;só&quot; &quot;pelo&quot; &quot;pela&quot; &quot;até&quot; ## [41] &quot;isso&quot; &quot;ela&quot; &quot;entre&quot; &quot;depois&quot; &quot;sem&quot; ## [46] &quot;mesmo&quot; &quot;aos&quot; &quot;seus&quot; &quot;quem&quot; &quot;nas&quot; ## [51] &quot;me&quot; &quot;esse&quot; &quot;eles&quot; &quot;você&quot; &quot;essa&quot; ## [56] &quot;num&quot; &quot;nem&quot; &quot;suas&quot; &quot;meu&quot; &quot;às&quot; ## [61] &quot;minha&quot; &quot;numa&quot; &quot;pelos&quot; &quot;elas&quot; &quot;qual&quot; ## [66] &quot;nós&quot; &quot;lhe&quot; &quot;deles&quot; &quot;essas&quot; &quot;esses&quot; ## [71] &quot;pelas&quot; &quot;este&quot; &quot;dele&quot; &quot;tu&quot; &quot;te&quot; ## [76] &quot;vocês&quot; &quot;vos&quot; &quot;lhes&quot; &quot;meus&quot; &quot;minhas&quot; ## [81] &quot;teu&quot; &quot;tua&quot; &quot;teus&quot; &quot;tuas&quot; &quot;nosso&quot; ## [86] &quot;nossa&quot; &quot;nossos&quot; &quot;nossas&quot; &quot;dela&quot; &quot;delas&quot; ## [91] &quot;esta&quot; &quot;estes&quot; &quot;estas&quot; &quot;aquele&quot; &quot;aquela&quot; ## [96] &quot;aqueles&quot; &quot;aquelas&quot; &quot;isto&quot; &quot;aquilo&quot; &quot;estou&quot; ## [101] &quot;está&quot; &quot;estamos&quot; &quot;estão&quot; &quot;estive&quot; &quot;esteve&quot; ## [106] &quot;estivemos&quot; &quot;estiveram&quot; &quot;estava&quot; &quot;estávamos&quot; &quot;estavam&quot; ## [111] &quot;estivera&quot; &quot;estivéramos&quot; &quot;esteja&quot; &quot;estejamos&quot; &quot;estejam&quot; ## [116] &quot;estivesse&quot; &quot;estivéssemos&quot; &quot;estivessem&quot; &quot;estiver&quot; &quot;estivermos&quot; ## [121] &quot;estiverem&quot; &quot;hei&quot; &quot;há&quot; &quot;havemos&quot; &quot;hão&quot; ## [126] &quot;houve&quot; &quot;houvemos&quot; &quot;houveram&quot; &quot;houvera&quot; &quot;houvéramos&quot; ## [131] &quot;haja&quot; &quot;hajamos&quot; &quot;hajam&quot; &quot;houvesse&quot; &quot;houvéssemos&quot; ## [136] &quot;houvessem&quot; &quot;houver&quot; &quot;houvermos&quot; &quot;houverem&quot; &quot;houverei&quot; ## [141] &quot;houverá&quot; &quot;houveremos&quot; &quot;houverão&quot; &quot;houveria&quot; &quot;houveríamos&quot; ## [146] &quot;houveriam&quot; &quot;sou&quot; &quot;somos&quot; &quot;são&quot; &quot;era&quot; ## [151] &quot;éramos&quot; &quot;eram&quot; &quot;fui&quot; &quot;foi&quot; &quot;fomos&quot; ## [156] &quot;foram&quot; &quot;fora&quot; &quot;fôramos&quot; &quot;seja&quot; &quot;sejamos&quot; ## [161] &quot;sejam&quot; &quot;fosse&quot; &quot;fôssemos&quot; &quot;fossem&quot; &quot;for&quot; ## [166] &quot;formos&quot; &quot;forem&quot; &quot;serei&quot; &quot;será&quot; &quot;seremos&quot; ## [171] &quot;serão&quot; &quot;seria&quot; &quot;seríamos&quot; &quot;seriam&quot; &quot;tenho&quot; ## [176] &quot;tem&quot; &quot;temos&quot; &quot;tém&quot; &quot;tinha&quot; &quot;tínhamos&quot; ## [181] &quot;tinham&quot; &quot;tive&quot; &quot;teve&quot; &quot;tivemos&quot; &quot;tiveram&quot; ## [186] &quot;tivera&quot; &quot;tivéramos&quot; &quot;tenha&quot; &quot;tenhamos&quot; &quot;tenham&quot; ## [191] &quot;tivesse&quot; &quot;tivéssemos&quot; &quot;tivessem&quot; &quot;tiver&quot; &quot;tivermos&quot; ## [196] &quot;tiverem&quot; &quot;terei&quot; &quot;terá&quot; &quot;teremos&quot; &quot;terão&quot; ## [201] &quot;teria&quot; &quot;teríamos&quot; &quot;teriam&quot; Há também o pacote stopwords, que no momento possui, para o português, de fontes como snowball, nltk e stopwords-iso. Para instalar, basta rodar o já conhecido install.packages(\"stopwords\"). # vendo as linguagens disponiveis stopwords::stopwords_getlanguages(&quot;snowball&quot;) ## [1] &quot;da&quot; &quot;de&quot; &quot;en&quot; &quot;es&quot; &quot;fi&quot; &quot;fr&quot; &quot;hu&quot; &quot;ir&quot; &quot;it&quot; &quot;nl&quot; &quot;no&quot; &quot;pt&quot; &quot;ro&quot; &quot;ru&quot; &quot;sv&quot; # vendo as fontes de stopwords disponíveis stopwords::stopwords_getsources() ## [1] &quot;snowball&quot; &quot;stopwords-iso&quot; &quot;misc&quot; &quot;smart&quot; ## [5] &quot;marimo&quot; &quot;ancient&quot; &quot;nltk&quot; &quot;perseus&quot; # vendo um extrato das stopwords em português, fonte snowball head(stopwords::stopwords(&quot;pt&quot;, source = &quot;snowball&quot;), 20) ## [1] &quot;de&quot; &quot;a&quot; &quot;o&quot; &quot;que&quot; &quot;e&quot; &quot;do&quot; &quot;da&quot; &quot;em&quot; &quot;um&quot; &quot;para&quot; ## [11] &quot;com&quot; &quot;não&quot; &quot;uma&quot; &quot;os&quot; &quot;no&quot; &quot;se&quot; &quot;na&quot; &quot;por&quot; &quot;mais&quot; &quot;as&quot; # vendo um extrato das stopwords em português, fonte stopwords-iso head(stopwords::stopwords(&quot;pt&quot;, source = &quot;stopwords-iso&quot;), 20) ## [1] &quot;a&quot; &quot;acerca&quot; &quot;adeus&quot; &quot;agora&quot; &quot;ainda&quot; &quot;alem&quot; &quot;algmas&quot; ## [8] &quot;algo&quot; &quot;algumas&quot; &quot;alguns&quot; &quot;ali&quot; &quot;além&quot; &quot;ambas&quot; &quot;ambos&quot; ## [15] &quot;ano&quot; &quot;anos&quot; &quot;antes&quot; &quot;ao&quot; &quot;aonde&quot; &quot;aos&quot; E comparando o número de elementos das diferentes fontes de stopwords stopwords::stopwords(&quot;pt&quot;, source = &quot;stopwords-iso&quot;) %&gt;% length() ## [1] 560 stopwords::stopwords(&quot;pt&quot;, source = &quot;snowball&quot;) %&gt;% length() ## [1] 203 stopwords::stopwords(&quot;pt&quot;, source = &quot;nltk&quot;) %&gt;% length() ## [1] 204 Para aplicar esta função no nosso texto, podemos usar removeWords(texto, stopwords(\"pt\")). Diferentes pacotes de análise textual possuem diferentes formas de retirar as stopwords. Para adicionar novas palavras à lista de stopwords que vamos usar no momento, cria-se um novo vetor - chamamos aqui de “novas_stopwords” - com as novas palavras a serem retiradas, e em seguida o stopwords() # checando o tamanho do vetor stopwords disponível length(tm::stopwords(&quot;pt&quot;)) ## [1] 203 # criando novo vetor com mais palavras novas_stopwords &lt;- c(&quot;então&quot;, &quot;portanto&quot;, tm::stopwords(&quot;pt&quot;)) # checando se nossos termos foram incluídos length(novas_stopwords) ## [1] 205 Ou para facilitar a inclusão de novos termos, podemos fazer do seguinte modo: # Separamos nossos termos por espaço novas &lt;- &quot;então portanto&quot; # quebrando o char em vetor de termos # ao invés de usarmos unlist, podemos usar [[1]] strsplit(novas, &quot; &quot;)[[1]] ## [1] &quot;então&quot; &quot;portanto&quot; # colocando os novos termos em um novo vetor novas_stopwords &lt;- c(strsplit(novas, &quot; &quot;)[[1]], tm::stopwords(&quot;pt&quot;)) length(novas_stopwords) ## [1] 205 Deste modo, podemos ir acrescentando mais facilmente novos termos à nossa lista de stopwords. Há outras funções com listas de stopwords, como qdap::stopwords. Para remover stopwords, temos diferentes pacotes com diferentes funções, como dplyr::anti_join(), tm::tm_map(corpus, removeWords, stopwords(\"english\") e qdap::rm_stopwords(). O dplyr possui ainda a função semi_join que mostra termos em comum, que se repetem em x e y. Já anti_join faz o oposto, mostra todas linhas de ‘x’ sem match em ‘y’, e é com ela que retiramos as stopwords. 8.6 Estemização (stemming) e lematização Imagine que tenha palavras como “escrever, escrevi, escreveu” e você está interessado nos verbos mais frequentes. É útil considerar estas variações do verbo como uma palavra só. Isso pode ser obtido de dois modos, através da stemização e por lematização. Em ambos o objetivo é o mesmo, reduzir a flexão a uma base comum ou raiz. A estemização funciona cortando um pedaço do final da palavra, ao passo que lematização reduz as variações à raiz, podendo inclusive pegar verbos irregulares. Por que então usar estemização? A construção de lematizadores é mais complicada, além de ser um processo mais demorado e que consome mais recursos. 8.6.1 Estemização A estemização pode ser feita com o pacote SnowballC,que é baseado no snowball, que continua sendo desenvolvido no GitHub do projeto. Desenvolvida originalmente por Martin Porter, seu nome é um tributo ao SNOBOL, uma linguagem dos anos 1960 que lidava com strings. Para entender o algoritmo de estemização em português e alguns exemplos, veja aqui. Carregando o pacote SnowballC library(SnowballC) Exemplo de estemização # Vendo as línguas disponíveis SnowballC::getStemLanguages() ## [1] &quot;arabic&quot; &quot;basque&quot; &quot;catalan&quot; &quot;danish&quot; &quot;dutch&quot; ## [6] &quot;english&quot; &quot;finnish&quot; &quot;french&quot; &quot;german&quot; &quot;greek&quot; ## [11] &quot;hindi&quot; &quot;hungarian&quot; &quot;indonesian&quot; &quot;irish&quot; &quot;italian&quot; ## [16] &quot;lithuanian&quot; &quot;nepali&quot; &quot;norwegian&quot; &quot;porter&quot; &quot;portuguese&quot; ## [21] &quot;romanian&quot; &quot;russian&quot; &quot;spanish&quot; &quot;swedish&quot; &quot;tamil&quot; ## [26] &quot;turkish&quot; # criando um vetor de palavras palavras= c(&quot;plantar&quot;, &quot;plantei&quot;, &quot;ajudou&quot;, &quot;ajudarás&quot;, &quot;comer&quot;, &quot;comendo&quot;) # testando a stemização SnowballC::wordStem(palavras, language = &quot;portuguese&quot;) ## [1] &quot;plant&quot; &quot;plant&quot; &quot;ajud&quot; &quot;ajud&quot; &quot;com&quot; &quot;com&quot; Vamos testar com outras palavras: palavras= c(&quot;estou&quot;, &quot;está&quot;, &quot;estamos&quot;, &quot;sou&quot;, &quot;és&quot;) SnowballC::wordStem(palavras, language = &quot;portuguese&quot;) ## [1] &quot;estou&quot; &quot;está&quot; &quot;estam&quot; &quot;sou&quot; &quot;és&quot; Repare que verbos irregulares como “ser” e “estar” não funcionaram muito bem. Uma alternativa é usar a lematização ao invés da stemização. 8.6.2 Lematização EM CONSTRUÇÃO A lematização reduz variações/inflexões de uma palavra, de modo que sejam analisados como um termo único. A lematização chega à forma raiz da palavra, ainda que sejam verbos irregulares. Funções como textstem::lemmatize_words() , koRpus::treetag, SnowballC::wordStem e udpipe fazem este trabalho de lematização. goffman_stigma &lt;- c(&quot;The central feature of the stigmatized individual&#39;s situation in life can now be stated.&quot;, &quot;It is a question of what is often, if vaguely, called `acceptance&#39;.&quot;, &quot;How does the stigmatized person respond to his situation?&quot;) ## Default lexicon::hash_lemmas dictionary textstem::lemmatize_strings(goffman_stigma) ## [1] &quot;The central feature of the stigmatize individual&#39;s situation in life can now be state.&quot; ## [2] &quot;It be a question of what be often, if vague, call `acceptance&#39;.&quot; ## [3] &quot;How do the stigmatize person respond to his situation?&quot; 8.7 Palavras em contexto (keyword-in-context KWIC) Podemos ver como certas palavras são usadas em diversas frases no texto para ter uma ideia melhor do contexto em que aparecem. No quanteda, usamos a função kwic(Dados, pattern = \"padrão\"), após o texto ter sido tokenizado. Se ainda não tiver carregado o pacote Quanteda: library(quanteda) ## Package version: 3.1.0 ## Unicode version: 13.0 ## ICU version: 69.1 ## Parallel computing: 4 of 4 threads used. ## See https://quanteda.io for tutorials and examples. Vamos para um exemplo do texto “Ciência como vocação” de Max Weber: texto = &quot;Por fim, é da sabedoria quotidiana que algo pode ser verdadeiro, embora não seja nem belo, nem sagrado, nem bom. Mas estes são apenas os casos mais elementares da luta que entre si travam os deuses dos ordenamentos e valores singulares. Como será possível pretender decidir &#39;cientificamente&#39; entre o valor da cultura francesa e o da alemã é coisa que não enxergo. Também aqui diferentes deuses lutam entre si, e para sempre. Acontece, embora noutro sentido, o mesmo que ocorria no mundo antigo, quando ainda se não tinha desencantado dos seus deuses e demónios: tal como os Gregos ofereciam sacrifícios, umas vezes, a Afrodite, outras a Apolo e, sobretudo, aos deuses da sua cidade, assim acontece ainda hoje, embora o culto se tenha desmistificado e careça da plástica mítica, mas intimamente verdadeira, daquela conduta. Sobre estes deuses e a sua eterna luta decide o destino, não decerto uma &#39;ciência&#39;. Apenas se pode compreender o que seja o divino para uma e outra ordem ou numa e noutra ordem&quot; # termos a serem buscados termos.vetor= c(&quot;deus*&quot;, &quot;divin*&quot;, &quot;luta&quot;) texto %&gt;% # precisamos primeiro tokenizar tokens%&gt;% # rodando a função de palavras chave em contexto kwic(., # termos a serem buscados. Pode ser um termo ou um vetor termos.vetor, # quantas palavras devem ser mostradas ao redor 4, # Para pegar tanto palavras minúculas como as em maiúsculo. case_insensitive = TRUE) ## Keyword-in-context with 8 matches. ## [text1, 35] casos mais elementares da | luta | que entre si travam ## [text1, 41] entre si travam os | deuses | dos ordenamentos e valores ## [text1, 75] . Também aqui diferentes | deuses | lutam entre si, ## [text1, 106] tinha desencantado dos seus | deuses | e demónios: tal ## [text1, 131] , sobretudo, aos | deuses | da sua cidade, ## [text1, 162] conduta. Sobre estes | deuses | e a sua eterna ## [text1, 167] e a sua eterna | luta | decide o destino, ## [text1, 187] o que seja o | divino | para uma e outra No KWIC é possível ainda: usar regex como padrão de busca, através do parâmetro valuetype = \"regex em buscar por duas ou mais palavras em contexto com pattern = phrase( texto %&gt;% # precisamos primeiro tokenizar tokens%&gt;% # rodando a função de palavras chave em contexto kwic(., # termos a serem buscados. Pode ser um termo ou um vetor pattern = phrase(&quot;eterna luta&quot;), # quantas palavras devem ser mostradas ao redor 7, # Para pegar tanto palavras minúculas como as em maiúsculo. case_insensitive = TRUE) ## Keyword-in-context with 1 match. ## [text1, 166:167] . Sobre estes deuses e a sua | eterna luta | ## ## decide o destino, não decerto uma 8.8 Key term extraction Dado um ou mais documentos, a extração automática de termos que descrevem estes documentos, dá se o nome de keyword extraction, podendo ser extraídas palavras, frases ou segmentos. Exemplos de key word extraction são TF-IDF, Parts of Speech tagging (por exemplo, pegando os substantivos mais frequentes), colocação e Coocorrências, algoritmos textrank, RAKE (Rapid Automatic Keyword Extraction), YAKE! (Yet Another Keyword Extractor). 8.8.1 TF-IDF: Term-Frequency Inverse Document Frequency A “frequência do termo–inverso da frequência nos documentos”, do inglês “Term-Frequency Inverse Document Frequency”, ou “TF-IDF” é utilizado para medir a relevância de palavras em uma série de documentos. Para funcionar, requer que existam vários documentos, ou textos, ou capítulos, etc. Neste algoritmo, as palavras que aparecem em todos ou em muitos documentos - como as stopwords - serão “penalizadas” e terão pontuação baixa. Agora, se uma palavra aparece bastante em um documento, mas não em outros, terá pontuação alta, e isto pode indicar que seja relevante, significativa para entender a peculiaridade daquele documento/texto. TF-IDF é útil num processo chamado de “keyword extraction” ou “extração de palavras chave”. Vamos para um exemplo: Documento 1: “Eu quero abacaxi” Documento 2: “Eu? Eu quero banana” Frequência de termos, ou TF, representa a proporção que uma palavra tem no documento em questão. Esta frequência pode ser apresentada no formato de matriz. Frequência Doc 1 Doc2 Eu 1 2 quero 1 1 abacaxi 1 0 banana 0 1 total palavras 3 4 8.8.1.1 TF: Frequência de termos (Term Frequency) O TF de um termo que ocorre em um documento é calculado da seguinte maneira: \\(tf(t,d)\\) : contagem de t(termo) em d(documento) / número de palavras no documento O documento 1 possui 3 palavras, o documento 2 possui 4 palavras, portanto, a frequência de termos (TF) fica assim: TF Doc 1 Doc2 Eu 1 / 3 = 0,33333 2 / 4 = 0,5 quero 1 / 3 = 0,33333 1 / 4 = 0,25 abacaxi 1 / 3 = 0,33333 0 banana 0 1 / 4 = 0,25 Total 1 1 No Documento 1, temos 3 palavras no total, cada uma, por ser única no documento, possui TF de 1/3, ou 0,33333. O Documento 2 possui 4 palavras no total. “quero” e “banana” possui um TF de 1/4 cada, ou 0,25, enquanto “eu”, que apareceu duas vezes, possui TF de 2/4 ou 0,5. TF me diz o quão frequente é uma palavra/termo em um documento. Isto pode ser feito em números absolutos bem como em termos proporcionais (bom olhar as documentações dos pacotes para entender qual o padrão utilizado) 8.8.1.2 IDF: Inverse Document Frequency IDF, inverse document frequency, mostra o peso de um termo em relação à coleção total de documentos/textos, dando um valor baixo para termos frequentes em todos os documentos e que por isso são pouco informativos sobre as peculiaridades daquele documento, bem como privilegia termos frequentes em poucos documentos. Assim, no nosso exemplo, as palavras “eu” e “quero” estão presentes em dois documentos de um total de dois documentos, tendo o IDF de 0. Já “abacaxi” e “banana”, termos que aparecem uma vez e somente em um documento cada, possuem IDF de 0,30102. O cálculo é feito da seguinte forma: Número de documentos no corpus (no caso acima, dois), dividido pelo número de documentos onde o termo aparece. Se o termo aparece uma vez somente ou 50 vezes em um documento, em ambos os casos será computado como um. O resultado disto é posto num logaritmo. IDF Eu Log(2/2) = 0 quero Log(2/2) = 0 abacaxi Log(2/1) = 0,30102 banana Log(2/1) = 0,30102 Com o IDF sabemos quais termos ocorrem em vários documentos e os que ocorrem em poucos. Para saber o peso de cada termo em cada documento, usamos então o TF-IDF. 8.8.1.3 Calculando TF-IDF TF-IDF é a multiplicação dos dois termos, TF * IDF. Ao multiplicar TF por IDF, obtemos o score da palavra no documento. TF Doc 1 TF Doc2 IDF TF-IDF Doc1 TF-IDF Doc2 Eu 1 / 3 = 0,33333 2 / 4 = 0,5 Log(2/2) = 0 0,33333 * 0 = 0 0,5 * 0 = 0 quero 1 / 3 = 0,33333 1 / 4 = 0,25 Log(2/2) = 0 0,33333 * 0 = 0 0,5 * 0 = 0 abacaxi 1 / 3 = 0,33333 0 Log(2/1) = 0,301 0,3 * 0,3 = 0,1003 0 * 0,3 = 0 banana 0 1 / 4 = 0,25 Log(2/1) = 0,301 0 * 0,3 = 0 0,25 * 0,3 = 0,0752 Matemática Esta é a fórmula do tf-idf, e nos retorna o índice tf-idf para cada palavra em cada documento. \\(W_{ij} = tf_{i,j} \\times \\log(\\frac{N}{df_i})\\) Destrinchando a fórmula: Fórmula Descrição \\(W_{ij}\\) um termo \\(i\\) num documento \\(j\\), para o qual vamos calcular o tf-idf \\(tf_{i,j}\\) frequência do termo \\(i\\), no documento \\(j\\) \\(df_{ij}\\) Número de documentos que contenham o termo \\(i\\). Pouco importa se aparece apenas uma vez ou se 500 vezes num mesmo documento, seu valor em cada documento, se presente, é 1 \\(N\\) Número total de documentos \\(df_i\\) frequência de documentos que contenham o termo \\(i\\) Se o resultado encontrado se aproximar de “0”, então a palavra se encontra presente em vários documentos. Caso contrário, quanto mais se aproxima de “1”, mais rara é esta palavra em outros documentos e mais concentrada em poucos documentos. Vale ressaltar que o cálculo TF-IDF pode ser feito tanto com a frequência absoluta ou como com a relativa. Vimos exemplo de TF-IDF com apenas dois “documentos”. Vamos usar mais documentos para entender melhor o TF-IDF. Usamos um pacote de R nos bastidores para gerar a tabela à seguir, mas veremos o código que o gerou mais à frente. doc1 &lt;- &quot;Eu quero abacaxi!&quot; doc2 &lt;- &quot;Eu quero açaí!&quot; doc3 &lt;- &quot;Eu quero manga ou açaí! Eu quero manga! Manga!&quot; O exemplo acima possui diferentes configurações de palavras para observarmos o TF-IDF: “eu” e “quero” em todos os docs “abacaxi”, “manga” e “ou” que ocorrem uma vez. “açaí” que ocorre uma vez em dois documentos diferentes. “manga” que aparece várias vezes em um documento somente. ## Document-feature matrix of: 3 documents, 6 features (38.89% sparse) and 0 docvars. ## features ## docs eu quero abacaxi açaí manga ou ## text1 0 0 0.1590404 0 0 0 ## text2 0 0 0 0.05869709 0 0 ## text3 0 0 0 0.01956570 0.1590404 0.05301347 “eu” e “quero” ocorrem em todos os docs e possuem TF-IDF de 0 em todos os casos. “abacaxi” aparece somente na primeira frase e tem TF-IDF de 0.1590404 “açaí” ocorre uma vez em dois documentos possui TF-IDF em um doc com menos palavras no total. “ou” e “manga” só aparecem na frase 3, e manga aparece 3 vezes e tem TF-IDF maior que a palavra “ou”, que só aparece uma vez. No doc3, “ou” (que só aparece uma vez em um doc) possui TF-IDF maior que “açaí”, que aparece em mais de um doc. Podemos realizar o TF-IDF no R calculando manualmente, como neste exemplo em video ou neste tutorial, ou podemos usar alguns dos vários pacotes que tem já implementadas a função, como o Tidytext, Quanteda e TM, que veremos a seguir. Vale atentar que cálculos feitos com diferentes pacotes podem não bater entre si. Se for este o caso, atente para se usam frequência absoluta ou relativa, e qual a base do Logaritmo utilizado (se de base 2 ou 10). Em um exemplo real de uso de tf-idf, este tutorial usou tf-idf entre diferentes livros do Harry Potter: Fonte: Text Mining: Term vs. Document Frequency do AFIT Data Science Lab R Programming Guide 8.8.1.4 TF-IDF no R: Tidyverse Podemos realizar o TF-IDF no R com o tidytext com a função tidytext::bind_tf_idf. doc1 &lt;- &quot;Eu quero abacaxi&quot; doc2 &lt;- &quot;Eu? Eu quero banana&quot; # criando o data frame, onde cada linha é um documento. df &lt;- data.frame(&quot;texto&quot; = c(doc1,doc2), # ID de &quot;identificação&quot; &quot;ID&quot; = c(1,2), stringsAsFactors = F) df %&gt;% # quebrando o texto em tokens tidytext::unnest_tokens(output = &#39;word&#39;, token = &#39;words&#39;, # input = nome da coluna do dataframe input = texto) %&gt;% # contando os termos dplyr::count(ID, word, sort = TRUE) %&gt;% # TF-IDF tidytext::bind_tf_idf(word, ID, n) ## ID word n tf idf tf_idf ## 1 2 eu 2 0.5000000 0.0000000 0.0000000 ## 2 1 abacaxi 1 0.3333333 0.6931472 0.2310491 ## 3 1 eu 1 0.3333333 0.0000000 0.0000000 ## 4 1 quero 1 0.3333333 0.0000000 0.0000000 ## 5 2 banana 1 0.2500000 0.6931472 0.1732868 ## 6 2 quero 1 0.2500000 0.0000000 0.0000000 Em bind_tf_idf sendo: word a coluna contendo termos, ID a coluna contendo os IDs dos docs, n a contagem de palavras produzido por count(). Dicas Exemplo/tutorial de TF-IDF com o tidyverse de Julia Silge e David Robinson. 8.8.1.5 TF-IDF no R: Quanteda TF-IDF com o pacote Quanteda é usado com a função quanteda::dfm_tfidf doc1 &lt;- &quot;Eu quero Elias&quot; doc2 &lt;- &quot;Eu? Eu quero Durkheim&quot; # criando um vetor com documentos para transformar em um corpus no quanteda meuvetor &lt;- c(doc1,doc2) # criando um objeto tipo corpus a partir do vetor meucorpus &lt;- quanteda::corpus(meuvetor) # criando uma matriz de frequência meudfm &lt;- meucorpus %&gt;% # quebrando em tokens quanteda::tokens( # removendo a pontuação remove_punct = TRUE) %&gt;% # transformando em um document frame matrix quanteda::dfm() meudfm ## Document-feature matrix of: 2 documents, 4 features (25.00% sparse) and 0 docvars. ## features ## docs eu quero elias durkheim ## text1 1 1 1 0 ## text2 2 1 0 1 # gerando o tf-idf (frequencia absoluta) quanteda::dfm_tfidf(meudfm) ## Document-feature matrix of: 2 documents, 4 features (25.00% sparse) and 0 docvars. ## features ## docs eu quero elias durkheim ## text1 0 0 0.30103 0 ## text2 0 0 0 0.30103 # TF-IDF usando frequência relativa, proporcional quanteda::dfm_tfidf(meudfm, scheme_tf = &quot;prop&quot;) ## Document-feature matrix of: 2 documents, 4 features (25.00% sparse) and 0 docvars. ## features ## docs eu quero elias durkheim ## text1 0 0 0.1003433 0 ## text2 0 0 0 0.0752575 quanteda::dfm_tfidf(x, scheme_tf = “count”, scheme_df = “inverse”, base = 2) Parâmetro Descrição x objeto de entrada, devendo ser um document-feature matrix scheme_tf esquema para dfm_weight(); sendo o padrão “count”. Para usar a frequência relativa, usa-se o “prop” scheme_df esquema para docfreq(); sendo o padrão “inverse”. base A base para logaritmo no dfm_weight() e docfreq(), sendo 10 o valor padrão. Outro valor comum é 2 8.8.1.6 TF-IDF no R: TM Vamos usar agora o pacote TM. doc1 &lt;- &quot;Eu quero Durkheim&quot; doc2 &lt;- &quot;Elias! Eu quero Elias&quot; vetor_vetores &lt;- c(doc1,doc2) # criando o objeto tipo corpus para o TM meu_corpus &lt;- tm::Corpus(tm::VectorSource(vetor_vetores)) # observando a estrutura do objeto criado, que é uma lista str(meu_corpus) ## Classes &#39;SimpleCorpus&#39;, &#39;Corpus&#39; hidden list of 3 ## $ content: chr [1:2] &quot;Eu quero Durkheim&quot; &quot;Elias! Eu quero Elias&quot; ## $ meta :List of 1 ## ..$ language: chr &quot;en&quot; ## ..- attr(*, &quot;class&quot;)= chr &quot;CorpusMeta&quot; ## $ dmeta :&#39;data.frame&#39;: 2 obs. of 0 variables Vamos a um pré-processamento do pacote tm com a função tm_map() e tm::removePunctuation. meu_corpus2 &lt;- # passando tudo para minúsculo tm::tm_map(meu_corpus, tolower) %&gt;% # removendo pontuações tm::tm_map(., tm::removePunctuation) ## Warning in tm_map.SimpleCorpus(meu_corpus, tolower): transformation drops ## documents ## Warning in tm_map.SimpleCorpus(., tm::removePunctuation): transformation drops ## documents meu_corpus2 ## &lt;&lt;SimpleCorpus&gt;&gt; ## Metadata: corpus specific: 1, document level (indexed): 0 ## Content: documents: 2 A matriz de termo por documento (document-term-matrix) computa quantas vezes um termo aparece por documento, que no nosso caso foi uma frase simples. “Durkheim” aparece uma vez no documento 1, “quero” aparece uma vez em cada documento e “Elias” aparece duas vezes no docuemnto 2. Opção 1, mais simples dtm.tfidf &lt;- tm::DocumentTermMatrix(meu_corpus2, control = list(weighting = tm::weightTfIdf)) ## Warning in TermDocumentMatrix.SimpleCorpus(x, control): custom functions are ## ignored # vendo a estrutura dtm.tfidf ## &lt;&lt;DocumentTermMatrix (documents: 2, terms: 3)&gt;&gt; ## Non-/sparse entries: 2/4 ## Sparsity : 67% ## Maximal term length: 8 ## Weighting : term frequency - inverse document frequency (normalized) (tf-idf) # Para visualizar, transformamos nosso objeto em matriz as.matrix(dtm.tfidf) ## Terms ## Docs durkheim quero elias ## 1 0.5 0 0.0000000 ## 2 0.0 0 0.6666667 Opção 2: com normalização e retirada de stopwords doc1 &lt;- &quot;Eu quero Durkheim&quot; doc2 &lt;- &quot;Elias! Eu quero Elias&quot; vetor.docs &lt;- c(doc1,doc2) # criando o objeto tipo corpus para o TM # como nossa fonte são vetores, usamos VectorSource meu_corpus &lt;- tm::Corpus(tm::VectorSource(vetor.docs)) tm::DocumentTermMatrix(meu_corpus, # para concatenar várias transformacoes, vamos usar function control = list(weighting = function(x) tm::weightTfIdf(x, normalize = T), removePunctuation = TRUE, stopwords = TRUE)) %&gt;% as.matrix ## Warning in TermDocumentMatrix.SimpleCorpus(x, control): custom functions are ## ignored ## Terms ## Docs durkheim quero elias ## 1 0.5 0 0.0000000 ## 2 0.0 0 0.6666667 Dicas TF-IDF JONES, Karen Spärck. A statistical interpretation of term specificity and its application in retrieval. Journal of Documentation. (1972) Cap. 6.5 TF-IDF: Weighing terms in the vector in JURAFSKI,D; MARTIN,J. Speech and Language Processing. Gerard Salton and Christopher Buckley (1988). Term-weighting approaches in automatic text retrieval. Information Processing and Management, 24/5, 513-523. Tutorial vignette (exemplo) de TF-IDF com o tidytext Análise tf-idf com livros do Harry Potter usando o Tidyverse. Vídeos: Video em português Aula 5.4: Problema com Matrizes de Frequência e TF-IDF | Processamento de Língua Natural. do curso de “Processamento de Língua Natural” (LIG948B), ministrado na Faculdade de Letras da Universidade Federal de Minas Gerais (FALE-UFMG). Video tutorial de tf-idf no R (em inglês) “TF-IDF | Introduction to Text Analytics with R Part 5” do Data Science Dojo, fazendo o cálculo sem pacotes. "],["text-mining-semantic-parsing.html", "Capítulo 9 Text mining: Semantic Parsing 9.1 POS - Part-of-speech tagging 9.2 Pacote UDPipe", " Capítulo 9 Text mining: Semantic Parsing Até agora vimos abordagem onde a ordem das palavras, sua função gramatical não importava na análise. Vamos agora para análise onde isso se faz importante. 9.1 POS - Part-of-speech tagging É possível identificar a classe gramatical das palavras em frases. Vários pacotes no R fazem isso, como o openNLP, coreNLP (Wrapper ao redor do Stanford CoreNLP Tools) e a função qdap::pos() (requerem rJava, que requer JDE e JRE do Java instalados), RcppMeCab que é um wrapper da biblioteca “mecab’, spacyr, um wrapper ao redor do spaCy do Python (Requer o Python e a biblioteca Anaconda instalados), UDPipe em C++ que possui modelos pré-treinados em 65 liguagens. No caso de pacotes que requerem o Rjava, uma dica é, ao menos no Linux, antes de instalar o Rjava, rodar o seguinte comando no terminal (não no console do R!). O comando a seguir faz com que o Rjava consiga encontrar o Java, e assim evita alguns tipos de erro na instalação. sudo R CMD javareconf 9.2 Pacote UDPipe Para o P.O.S (part-of-speech tagging), vamos usar o pacote UDPipe, já que ele não requer Python ou Java, sendo um wrapper do UDPipe C++, evitando boa parte das complicações de instalação, como em outros pacotes. O projeto é da Institute of Formal and Applied Linguistics, Faculty of Mathematics and Physics, Charles University da Reṕublica Tcheca. A função de POS é baseada no Google universal part-of-speech tags. UD vem de universal dependencies, um framework aberto para anotações gramaticais com 200 treebanks em amsi de 100 linguas. Milan Straka, Jan Hajiˇc, Jana Strakov. ́UDPipe: Trainable Pipeline for Processing CoNLL-U Files Performing Tokenization, Morphological Analysis, POS Tagging and Parsing Modelos de línguas no github de jwijffels Após instalar, carregando o pacote: Vamos instalar um modelo de lingua não inglesa. Para ver a lista de linguagens disponíveis, rode o comando ?udpipe_download_model. Mas atente que nem todas as línguas estão disponíveis na última versão, sendo necessário a sua especificação. Para saber sobre os modelos em portuges utilizados no UD, veja aqui. Vamos utilisar a versão bosque, que funciona para PT-PT e PT-Br. Tal como está, o download ocorrerá no diretório de trabalho atual (digite getwd() para saber) # opção 1 dl &lt;- udpipe_download_model(language = &quot;portuguese-br&quot;, udpipe_model_repo = &#39;jwijffels/udpipe.models.ud.2.0&#39;) # Opção 2. &quot;bosque&quot; é a mais atual e mais utilisada dl &lt;- udpipe_download_model(language = &quot;portuguese-bosque&quot;) str(dl) Uma vez que o modelo foi baixado, vamos carregá-lo udmodel_ptBosque &lt;- udpipe_load_model(file = dl$file_model) Ou, caso tenha baixado em outro local: udmodel_ptBosque &lt;- udpipe_load_model(file = &quot;~/Documentos/R/portuguese-bosque-ud-2.5-191206.udpipe&quot;) E vamos aos testes: library(dplyr) texto &lt;- &quot;O rato Rogério roeu rapidamente a roupa roxa do rei Roberto de Roma.&quot; txt.anotado &lt;- udpipe::udpipe_annotate(udmodel_ptBosque, x = texto) %&gt;% as.data.frame() ## This looks like you restarted your R session which has invalidated the model object, trying now to reload the model again from the file at /home/alisson/Documentos/Programação/R/analise_textual_sociologia/portuguese-bosque-ud-2.5-191206.udpipe in order to do the annotation. str(txt.anotado) ## &#39;data.frame&#39;: 16 obs. of 14 variables: ## $ doc_id : chr &quot;doc1&quot; &quot;doc1&quot; &quot;doc1&quot; &quot;doc1&quot; ... ## $ paragraph_id : int 1 1 1 1 1 1 1 1 1 1 ... ## $ sentence_id : int 1 1 1 1 1 1 1 1 1 1 ... ## $ sentence : chr &quot;O rato Rogério roeu rapidamente a roupa roxa do rei Roberto de Roma.&quot; &quot;O rato Rogério roeu rapidamente a roupa roxa do rei Roberto de Roma.&quot; &quot;O rato Rogério roeu rapidamente a roupa roxa do rei Roberto de Roma.&quot; &quot;O rato Rogério roeu rapidamente a roupa roxa do rei Roberto de Roma.&quot; ... ## $ token_id : chr &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## $ token : chr &quot;O&quot; &quot;rato&quot; &quot;Rogério&quot; &quot;roeu&quot; ... ## $ lemma : chr &quot;o&quot; &quot;rato&quot; &quot;Rogério&quot; &quot;roer&quot; ... ## $ upos : chr &quot;DET&quot; &quot;NOUN&quot; &quot;PROPN&quot; &quot;VERB&quot; ... ## $ xpos : chr NA NA NA NA ... ## $ feats : chr &quot;Definite=Def|Gender=Masc|Number=Sing|PronType=Art&quot; &quot;Gender=Masc|Number=Sing&quot; &quot;Gender=Masc|Number=Sing&quot; &quot;Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin&quot; ... ## $ head_token_id: chr &quot;2&quot; &quot;4&quot; &quot;2&quot; &quot;0&quot; ... ## $ dep_rel : chr &quot;det&quot; &quot;nsubj&quot; &quot;appos&quot; &quot;root&quot; ... ## $ deps : chr NA NA NA NA ... ## $ misc : chr NA NA NA NA ... # Para vermos apenas as classes gramaticais, apenas filtramos pela coluna: txt.anotado$upos ## [1] &quot;DET&quot; &quot;NOUN&quot; &quot;PROPN&quot; &quot;VERB&quot; &quot;ADV&quot; &quot;DET&quot; &quot;NOUN&quot; &quot;ADJ&quot; NA ## [10] &quot;ADP&quot; &quot;DET&quot; &quot;NOUN&quot; &quot;PROPN&quot; &quot;ADP&quot; &quot;PROPN&quot; &quot;PUNCT&quot; Na coluna “upos”: - Roberto e Rogério são “PROPN”, ou seja, “proper names”, ou “nomes próprios” - rato aparece como “NOUN”, isto é, substantivo. - “VERB é verbo, ADJ é adjetivo, ADV é advérbio. Se quiser fazer só o POS Tagging, sem fazer lematização, e com isto ganhar tempo, pode-se restringir com o argumento tagger que pode ser um vetor: texto2 &lt;- &quot;O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa&quot; txt.anotado2 &lt;- udpipe_annotate(udmodel_ptBosque, x = texto2, tagger = &quot;default&quot; , parser = &quot;none&quot;) %&gt;% as.data.frame() txt.anotado2 ## doc_id paragraph_id sentence_id ## 1 doc1 1 1 ## 2 doc1 1 1 ## 3 doc1 1 1 ## 4 doc1 1 1 ## 5 doc1 1 1 ## 6 doc1 1 1 ## 7 doc1 1 1 ## 8 doc1 1 1 ## 9 doc1 1 1 ## 10 doc1 1 1 ## 11 doc1 1 1 ## 12 doc1 1 1 ## 13 doc1 1 1 ## 14 doc1 1 1 ## 15 doc1 1 1 ## 16 doc1 1 1 ## 17 doc1 1 1 ## 18 doc1 1 1 ## 19 doc1 1 1 ## 20 doc1 1 1 ## 21 doc1 1 1 ## 22 doc1 1 1 ## 23 doc1 1 1 ## 24 doc1 1 1 ## 25 doc1 1 1 ## 26 doc1 1 1 ## 27 doc1 1 1 ## sentence ## 1 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 2 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 3 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 4 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 5 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 6 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 7 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 8 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 9 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 10 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 11 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 12 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 13 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 14 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 15 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 16 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 17 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 18 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 19 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 20 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 21 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 22 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 23 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 24 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 25 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 26 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## 27 O fundamento psicológico sobre o qual se eleva o tipo das individualidades da cidade grande é a intensificação da vida nervosa ## token_id token lemma upos xpos ## 1 1 O o DET &lt;NA&gt; ## 2 2 fundamento fundamento NOUN &lt;NA&gt; ## 3 3 psicológico psicológico ADJ &lt;NA&gt; ## 4 4 sobre sobre ADP &lt;NA&gt; ## 5 5 o o DET &lt;NA&gt; ## 6 6 qual qual PRON &lt;NA&gt; ## 7 7 se se PRON &lt;NA&gt; ## 8 8 eleva elevar VERB &lt;NA&gt; ## 9 9 o o DET &lt;NA&gt; ## 10 10 tipo tipo NOUN &lt;NA&gt; ## 11 11-12 das &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 12 11 de de ADP &lt;NA&gt; ## 13 12 as o DET &lt;NA&gt; ## 14 13 individualidades individualidade NOUN &lt;NA&gt; ## 15 14-15 da &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 16 14 de de ADP &lt;NA&gt; ## 17 15 a o DET &lt;NA&gt; ## 18 16 cidade cidade NOUN &lt;NA&gt; ## 19 17 grande grande ADJ &lt;NA&gt; ## 20 18 é ser AUX &lt;NA&gt; ## 21 19 a o DET &lt;NA&gt; ## 22 20 intensificação intensificação NOUN &lt;NA&gt; ## 23 21-22 da &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 24 21 de de ADP &lt;NA&gt; ## 25 22 a o DET &lt;NA&gt; ## 26 23 vida vida NOUN &lt;NA&gt; ## 27 24 nervosa nervoso ADJ &lt;NA&gt; ## feats head_token_id dep_rel ## 1 Definite=Def|Gender=Masc|Number=Sing|PronType=Art &lt;NA&gt; &lt;NA&gt; ## 2 Gender=Masc|Number=Sing &lt;NA&gt; &lt;NA&gt; ## 3 Gender=Masc|Number=Sing &lt;NA&gt; &lt;NA&gt; ## 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 5 Gender=Masc|Number=Sing|PronType=Art &lt;NA&gt; &lt;NA&gt; ## 6 Gender=Masc|Number=Sing|PronType=Rel &lt;NA&gt; &lt;NA&gt; ## 7 Case=Acc|Gender=Masc|Number=Sing|Person=3|PronType=Prs &lt;NA&gt; &lt;NA&gt; ## 8 Mood=Ind|Number=Sing|Person=3|Tense=Imp|VerbForm=Fin &lt;NA&gt; &lt;NA&gt; ## 9 Definite=Def|Gender=Masc|Number=Sing|PronType=Art &lt;NA&gt; &lt;NA&gt; ## 10 Gender=Masc|Number=Sing &lt;NA&gt; &lt;NA&gt; ## 11 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 12 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 13 Definite=Def|Gender=Fem|Number=Plur|PronType=Art &lt;NA&gt; &lt;NA&gt; ## 14 Gender=Fem|Number=Plur &lt;NA&gt; &lt;NA&gt; ## 15 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 16 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 17 Definite=Def|Gender=Fem|Number=Sing|PronType=Art &lt;NA&gt; &lt;NA&gt; ## 18 Gender=Fem|Number=Sing &lt;NA&gt; &lt;NA&gt; ## 19 Gender=Fem|Number=Sing &lt;NA&gt; &lt;NA&gt; ## 20 Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin &lt;NA&gt; &lt;NA&gt; ## 21 Definite=Def|Gender=Fem|Number=Sing|PronType=Art &lt;NA&gt; &lt;NA&gt; ## 22 Gender=Fem|Number=Sing &lt;NA&gt; &lt;NA&gt; ## 23 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 24 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 25 Definite=Def|Gender=Fem|Number=Sing|PronType=Art &lt;NA&gt; &lt;NA&gt; ## 26 Gender=Fem|Number=Sing &lt;NA&gt; &lt;NA&gt; ## 27 Gender=Fem|Number=Sing &lt;NA&gt; &lt;NA&gt; ## deps misc ## 1 &lt;NA&gt; &lt;NA&gt; ## 2 &lt;NA&gt; &lt;NA&gt; ## 3 &lt;NA&gt; &lt;NA&gt; ## 4 &lt;NA&gt; &lt;NA&gt; ## 5 &lt;NA&gt; &lt;NA&gt; ## 6 &lt;NA&gt; &lt;NA&gt; ## 7 &lt;NA&gt; &lt;NA&gt; ## 8 &lt;NA&gt; &lt;NA&gt; ## 9 &lt;NA&gt; &lt;NA&gt; ## 10 &lt;NA&gt; &lt;NA&gt; ## 11 &lt;NA&gt; &lt;NA&gt; ## 12 &lt;NA&gt; &lt;NA&gt; ## 13 &lt;NA&gt; &lt;NA&gt; ## 14 &lt;NA&gt; &lt;NA&gt; ## 15 &lt;NA&gt; &lt;NA&gt; ## 16 &lt;NA&gt; &lt;NA&gt; ## 17 &lt;NA&gt; &lt;NA&gt; ## 18 &lt;NA&gt; &lt;NA&gt; ## 19 &lt;NA&gt; &lt;NA&gt; ## 20 &lt;NA&gt; &lt;NA&gt; ## 21 &lt;NA&gt; &lt;NA&gt; ## 22 &lt;NA&gt; &lt;NA&gt; ## 23 &lt;NA&gt; &lt;NA&gt; ## 24 &lt;NA&gt; &lt;NA&gt; ## 25 &lt;NA&gt; &lt;NA&gt; ## 26 &lt;NA&gt; &lt;NA&gt; ## 27 &lt;NA&gt; SpacesAfter=\\\\n Dicas POST - Part-of-Speech Tagging An introduction to part-of-speech tagging and the Hidden Markov Model do free code camp. Vignette do Udpipe no CRan. Para ir além do básico com o Udpipe, ver Jan Wijffels. UDPipe Natural Language Processing - Basic Analytical Use Cases. 2021. 9.2.1 Coocorrência de palavras A coocorrência de palavras pode nos auxiliar a pegar o sentido de uma grande quantidade de frases, nos mostrando palavras usadas na mesma sentença ou nas proximidades umas das outras. Por exemplo, podemos ver quantas vezes substantivos (nouns) aparecem junto a adjetivos (adj) na mesma sentença, ou junto a verbos (VERB), ou até alguma distância determinada. Vários pacotes fazem este trabalho, como o widyr::pairwise_count. Usaremos o pacote do UDpipe. O comando udpipe::cooccurrence() aceita vetores e dataframes como input (dê uma olhada no help do termo). Num exemplo simples: coocor &lt;- udpipe::cooccurrence(c(&quot;Abacate&quot;, &quot;Banana&quot;, &quot;Abacate&quot;, &quot;Abacate&quot;, &quot;Banana&quot;, &quot;carambola&quot;, &quot;Banana&quot;, &quot;uva&quot;)) coocor ## term1 term2 cooc ## 1 Abacate Banana 2 ## 2 Banana Abacate 1 ## 3 Abacate Abacate 1 ## 4 Banana carambola 1 ## 5 carambola Banana 1 ## 6 Banana uva 1 Podemos fazer a rede de palavras disso. Primeiro carregamos os pacotes library(igraph) library(ggraph) Rodando: rede.palavras &lt;- igraph::graph_from_data_frame(coocor) ggraph::ggraph(rede.palavras, layout = &quot;fr&quot;) + geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = &quot;lightskyblue&quot;) + geom_node_text(aes(label = name), col = &quot;darkgreen&quot;, size = 4) + theme_graph(base_family = &quot;Arial Narrow&quot;) + theme(legend.position = &quot;none&quot;) + labs(title = &quot;Coocorrência&quot;) var1 &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;c&quot;) udpipe::cooccurrence(var1, skipgram = 0) ## term1 term2 cooc ## 1 A B 2 ## 2 B A 1 ## 3 A A 1 ## 4 B c 1 udpipe::cooccurrence(var1, skipgram = 1) ## term1 term2 cooc ## 1 A B 3 ## 2 B A 2 ## 3 A A 2 ## 4 B c 1 ## 5 A c 1 Vamos usar uma bse de dados mais extensa, as Notas Taquigráficas da CPI da Pandemia. Elas foram estruturadas em dataframes em csv e Rds. O modo mais fácil e indicado é importar o .Rds, que já está no formato do R. As notas foram separadas por pessoa, partido, bloco parlamentar, função na CPI, e estado de origem. NotasTaq &lt;- readRDS(url(&quot;https://github.com/SoaresAlisson/NotasTaquigraficas/raw/master/rds/NT_30-Oitiva-Luiz_Paulo_Dominguetti_Pereira.Rds&quot;)) # vamos transformar nosso dataframe em tibble NotasTaq = dplyr::as_tibble(NotasTaq) # pegando apenas as falas e transformando em um vetor de um elemento, para poder usar no udpipe falas &lt;- NotasTaq %&gt;% #filter(nome == &quot;Omar Aziz&quot;) %&gt;% select(fala) %&gt;% filter(nome == unique(NotasTaq$nome)[2]) %&gt;% select(fala) %&gt;% paste0() NT.ud &lt;- udpipe_annotate(udmodel_ptBosque, x = falas) %&gt;% as.data.frame() Podemos fazer uma frequência básica NT.ud.lemma &lt;- NT.ud %&gt;% filter(upos == &quot;ADJ&quot;) %&gt;% select(lemma) # pegando apenas substantivos (noun) da coluna upos estatisticasTexto &lt;- subset(NT.ud, upos %in% c(&quot;NOUN&quot;)) # txt_freq retorna um df com 3 colunas: 1) termo (key); 2) frequência (freq) e 3) frequencia percentual (freq_pct) estatisticasTexto &lt;- txt_freq(estatisticasTexto$token) estatisticasTexto$key &lt;- factor(estatisticasTexto$key, levels = rev(estatisticasTexto$key)) lattice::barchart(key ~ freq, # pegando os 25 casos mais frequentes data = head(estatisticasTexto, 25), col = &quot;indianred&quot;, main = &quot;Substantivos mais frequentes do depoente&quot;, xlab = &quot;Frequência&quot;) Perceba que há imprecisões: “Sr.” é pronome de tratamento, mas figura como substantivo. 9.2.2 Rede de palavras (wordnet) Montando a rede de palavras com os dados acima. coocorrencias &lt;- cooccurrence(x = subset(NT.ud, upos %in% c(&quot;NOUN&quot;, &quot;ADJ&quot;, &quot;VERB&quot;)), term = &quot;lemma&quot;, group = c(&quot;doc_id&quot;, &quot;paragraph_id&quot;, &quot;sentence_id&quot;)) wordnetwork &lt;- head(coocorrencias, 60) wordnetwork &lt;- igraph::graph_from_data_frame(wordnetwork) ggraph::ggraph(wordnetwork, layout = &quot;fr&quot;) + ggraph::geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = &quot;lightskyblue3&quot;) + geom_node_text(aes(label = name), col = &quot;darkgreen&quot;, size = 4) + theme_graph(base_family = &quot;Arial&quot;) + theme(legend.position = &quot;none&quot;) + labs(title = &quot;Coocorrência dentro da sentença&quot;, subtitle = &quot;60 substantivos, adjetivos e verbos no depoimento \\nde Dominguetti na CPI da Pandemia&quot;) É possível ainda fazer rede de palavras com a função qdap::word_associate que ao invés de mostrar ligações mais frequentes, destaca as palavras mais frequentes através de seu tamanho, como numa nuvem de palavras. 9.2.3 Análise de semelhanças 9.2.3.1 Algoritmo Smith_Waterman Se busca regiões similares entre dois textos, um modo de detectar tais semelhança é usando o algoritmo Smith-Waterman, desenvolvido inicialmente na Biologia para identificar sequências de moléculas no artigo: SMITH T.F.Smith, WATERMAN, M. S. Identification of common molecular subsequences. Journal of Molecular Biology. Volume 147, Issue 1, 25 March 1981, Pages 195-197 Dadas duas sequências de letras, o algoritmo encontra o local ótimo de alinhamento. O pacote text.alignment do UDPipe aplica o algoritmo a palavras e letras, tentando identificar regiões similares entre duas strings. Você pode conferir um vignette do pacote (no linguajar do R, “vignette” é um guia rápido, com exemplo passo a passo) ou o manual. Com este pacote é possível: - encontrar palavras em documentos mesmo com grafia incorreta - Encontrar sequências de um texto em outros. Bom para comparar traduções ou identificar plágios. # traducao Nelson Jahr Garcia txt1 = &quot;E esqueceu-se de acrescentar: a primeira vez como tragédia, a segunda como farsa.&quot; # traducao Boitempo txt2 = &quot;Ele se esqueceu de acrescentar: a primeira vez como tragédia, a segunda como farsa.&quot; text.alignment::smith_waterman(txt1, txt2) ## Swith Waterman local alignment score: 153 ## ---------- ## Document a ## ---------- ## E ###esqueceu-se de acrescentar: a primeira vez como tragédia, a ## segunda como farsa. ## ---------- ## Document b ## ---------- ## e se esqueceu### de acrescentar: a primeira vez como tragédia, a ## segunda como farsa. Repare que em ambas as frases, preencheu-se com o sinal de tralha (#) até os textos ficarem alinhados. O modo padrão da função é buscar por caracteres, mas podemos mudar o padrão para palavras, o que pode ser mais adequado para nosso caso. text.alignment::smith_waterman(txt1, txt2, type = &quot;words&quot;) ## Swith Waterman local alignment score: 23 ## ---------- ## Document a ## ---------- ## se ######## de acrescentar a primeira vez como tragédia a segunda como ## farsa ## ---------- ## Document b ## ---------- ## se esqueceu de acrescentar a primeira vez como tragédia a segunda como ## farsa Dicas Analise Textual Julia Silge Learn tidytext with my new learnr course. Um curso interativo do pacote Tidytext. Textos sobre análise textual CASTELFRANCHI, Yurij. A análise de textos auxiliada pelo computador: um laboratório a céu aberto para as ciências sociais. Journal of Science Communication 16(02)(2017)C04 GRIMMER, Justin.STEWART, Brandom. Text as Data: The Promise and Pitfalls of Automatic ContentAnalysis Methods for Political Texts. Political Analysis(2013) pp. 1–31. doi:10.1093/pan/mps028. TREADWELL, Donald. Content Analysis: Understanding Text and Image in Numbers. Understanding Text and Image in Numbers. In __ Introducing Communication Research: paths of Inquiry. Sage. 2014. (Capítulo sobre análise de conteúdo) Link para diversos artigos de Gary King sobre Automated Text Analysis. Vídeos BROWN, Taylor W. Workshop on automated text analysis no Summer Institute in Computational Social Science na Universidade de Oxford em 2019. Em inglês, sem legendas, usando o pacote Quanteda. Parte 1 e Parte 2. O material da aula no Google Drive. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
