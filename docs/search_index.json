[["objetivos-deste-manual.html", "Introdução à Análise Textual aplicada à Sociologia Objetivos deste manual Plano do livro", " Introdução à Análise Textual aplicada à Sociologia Alisson Soares Primeira versão em 22 de abril de 2021. Última atualização em 04 julho de 2021 Objetivos deste manual Sou Alisson Soares, sociólogo, e comecei há pouco tempo no campo das humanidades digitais. Achei por bem montar este manual como uma forma de compartilhar o que venho aprendendo. Algumas ferramentas, softwares serão de modo gráfico, mas por vezes vamos recorrer à programação. A linguagem aqui será o R, uma linguagem de programação gratuita, livre e de código aberto, bastante utilizada no mundo acadêmico. O objetivo deste manual é: Ser um livro em progresso constante. Ser introdutório, mas também sempre indicar bons materiais para maior aprofundamento dos temas tratados. Sempre que possível, oferecer material em português, apesar de nem sempre isto ser possível. Oferecer uma introdução às ferramentas computacionais utilizadas em pesquisa das ciências sociais. Usar preferencialmente ferramentas gratuitas e de código aberto. Sempre que possível, apresentar exemplos de pesquisa e ciência social utilizando estas ferramentas. Ao final de cada capítulo sessão apresentamos dicas de links externos. Há também um capítulo dedicado a links de temas correlatos, que podem ajudar a vida dos humanistas digitais. Plano do livro Os seguintes capítulos serão incorporados futuramente e estão em elaboração: Rápida introdução à programação (conceitos básicos como variável, loops, condicionais if-else, função, etc.) Normalização e expressões regulares. Rápida introdução ao R (tipos de dados, estrutura de dados, importação de dados, ambientes virtuais, suíte tidyverse) Introdução à análise textual via computador Frequência de termos (bag of words, n-grams, skipgrams, TF-IDF) dendogramas Correlação, tipos de distâncias parts-of-speech keyword extraction Introdução à análise de redes. Inteligência Artificial: clusterização; topic modelling Análise de sentimentos. Glossário de termos "],["intro.html", "Introdução O termo “humanidade digitais”", " Introdução “the best digital theory-building of the past decade stems from social and computational origins. As such, it is increasingly apparent that digital sociologists need to develop a computational as well as a sociological imagination.” (Neil Selwyn) Social researchers of the future will need to write code, wrangle data, and think computationally as well as sociologically. (EVANS &amp; FOSTER) O termo “humanidade digitais” Segundo um estudo do LinkedIn em 15 países, prevendo 150 milhões de vagas nos próximos 5 anos, das áreas de trabalhos digitais em alta e 2021, duas se relacionam a Humanidades Digitais: Big data Analytics (segundo lugar) e “Text, image and voice processing” (6º lugar). Profissionalmente, pode ser interessante entrar nesta área. Existem diversos termos correlatos a “Humanidades digitais” (“Digital humanities”), como “E-humanities,”Cultural Analytics”, “ciências sociais computacionais”, “sociologia digital”, “história digital”, “etnografia digital”, “etnografia quantitativa”, “Análise cultural quantitativa” (quantitative cultural analysis) “cultural analytics”, “humanities data science”, “humanities data analysis” “humanities computing”, “distant reading”, “computational social science”, etc. Embora estes termos não sejam todos sinônimos entre si, há diversos pontos de contato. Em quase todos eles há a junção de algo de humanidades e algo de digital, demonstrando seu caráter transdisciplinar. A área possui contribuições de diversas áreas, como das ciências da informação, linguística, ciências da computação, história, sociologia, comunicação, etc. O termo é controverso mesmo entre os participantes, uma vez que é amplo demais. Uma definição ampla define o termo como intersecção entre humanidades e computação. Porém, uma busca no Google por um texto já configura uso de ferramenta digital. Outra definição mais restrita envolveria aprender a pensar como o computador e a partir disto extrair diversas novas possibilidades. Uma introdução aos diferentes significados do termo pode ser vista no artigo “Humanidades digitais” do grupo de pesquisas da USP. Em termos mais práticos, Humanidades digitais inclui a análise de: sons/música, imagens. textos. Nosso objetivo aqui Há todo um repertório de ferramentas comuns utilizados tanto por cientistas de dados como por humanistas digitais, com diferentes fins. Nosso foco será aprender diversas destas ferramentas de uso comum, mas focando em seu uso na sociologia. "],["história-da-análise-textual.html", "Capítulo 1 História da Análise Textual 1.1 Linha do tempo da história da Análise Textual", " Capítulo 1 História da Análise Textual “May we hope that when things come to such a crisis, human labor of the literary sort may be in part superseded by machinery? Machinery has done wonders, and when we think of what literature is becoming, it is certainly to be wished that we could read it by machinery, and by machinery digest it” (Andrew Stauffer In London’s Daily News. 15 de Setembro de 1869 apud Catherine DeRose.) A análise textual abarca campos do conhecimento bem variados, como psicologia, ciências da computação, ciências da informação, linguística, ciência política, sociologia, etc. Apresentamos aqui uma linha do tempo com alguns dos principais eventos relacionados à análise textual, bem como ao seu uso com computadores. De modo resumido, podemos pensar nos primeiros desenvolvimentos ao final do século XIX, a introdução do computador e mais recentemente, a introdução da inteligência artificial como pontos marcantes nesta cronologia. Fizemos aqui uma breve cronologia, que não pretende ser extensiva, com todos eventos importantes, mas apenas demarcar alguns pontos interessantes, para ajudar a dar alguma ideia àqueles que iniciam no campo das humanidades digitais e na sociologia digital. 1.1 Linha do tempo da história da Análise Textual Séc. XVII - Igreja Católica analisa a proporção de textos impressos de conteúdo não religioso 1887 Medenhall. Analisa o comprimento de palavras: MENDENHALL, T. C. . The characteristic curves of composition. Science.Vol ns-9, Issue 214S. 11 March 1887. baixar pdf Wincenty Lutoslawski. Análise de palavras raras na obra de Platão. Cunhou o termo “estilometria” Lutoslawski’s Origin and Growth of Plato’s Logic - The Origin and Growth of Plato’s Logic, 1888 Benjamin Bourdon (1860-1943, psicólogo e professor da Université de Rennes): Ao pesquisar sobre a expressão de emoções através de palavras, analisou o livro “Exodus” da Bíblia e calculou frequências, classificou e eliminou as stopwords. “In 1888, in a research on the expression of emotions through words, Benjamin Bourdon analysed the Exodus of the Bible and calculated the frequencies by rearranging and classifying them, eliminating the stop words” fonte. 1888 Friedrich Kaeding (1855 - 1929), cria índices de frequência para estruturação de sistemas estenográficos (sistema de escrita por abreviações para que a escrita seja tão rápida como a fala). 1934 Harold Laswell (1902-1978, cientista político) produz a primeira contagem de palavras chave. 1934 Vygostky produz a primeira análise quantitativa de narrativa 1949 Robert Busa (padre jesuíta) junto à IBM com o projeto Index Thomisticus, que levou 34 anos, envolveu cerca de 70 pessoas, sobre as obras de São Tomás de Aquino. “The IBM… considered this first enterprise of using a computer for linguistic and lexicographic goals as a pilot-project” fonte, indexando mais de 10 milhões de palavras. Busa, R. (1980). “The Annals of Humanities Computing: The Index Thomisticus”. Computers and the Humanities. 14 (2): 83–90. doi:10.1007/BF02403798. ISSN 0010-4817. ROCKWELL, Geoffrey; PASSAROTTI, Marco (2019-05-27). “The Index Thomisticus as a Big Data Project”. Umanistica Digitale (5). doi:10.6092/issn.2532-8816/8575. 1950 Gottschalk usa Content Analysis para rastrear temas freudianos. GOTTSCHALK, Louis A. The Measurement of Psychological States Through the Content Analysis of Verbal Behavior. University of California Press. 1969. 317p. Gottschalk-Gleser Content Analysis Method of Measuring the Magnitude of Psychological Dimensions 1950 Alan Turing aplica Inteligência Artificial a textos. 1952 Bereleson publica o primeiro manual de análise de conteúdo. BERELSON, B. (1952). Content analysis in communication research. New York: Hafner. 1954 Primeira tradução automática de texto (Georgetown–IBM experiment) do russo para o inglês. HUTCHINS, John. The first public demonstration of machine translation: the Georgetown-IBM system, 7th January 1954. 2006. Press Release da IBM 1963 Mosteller e Wallace analisam a autoria dos Federalist Papers. MOSTELLER, F., WALLACE, D. L.. 1963. Inference in an authorship problem. Journal of the American Statistical Association 58:275–309. 1965 Tomashevsky formaliza análise quantitativa de narrativa. TOMASHEVSKY, B. (1965). Thematics. In L T. Lemon &amp; M. I. Reis (Eds. &amp; Trans.), Russian formalist criticism: Four essays (pp. 61-95). Lincoln: University of Nebraska Press. (Original de 1925) 1966 Stones e Bales usam computador para medir propriedades psicométricas de textos na RAND. 1980 Declínio do formalismo chomskyano; nascimento do Processamento de Linguagem Natural (PLN). 1980 Aplicação de Aprendizado de máquinas (Machine Learning) ao Processamento de Linguagem Natural 1981 Walter Weintraub e a contagem de parts-of-speech. WEINTRAUB, Walter. Verbal Behavior: adaptation and psychopathology. Springer:NY. 1981. SOBEL, Dava. Language patterns reveal problems in personality. NYT. Oct. 27,1981. 1985 Schrodt introduz codificação automática de eventos (Automated Event Coding). SCHRODT, Philip A. Automated Coding Of International Event Data Using Sparse Parsing Techniques. 2000. 1986 James W. Pennebaker desenvolve LIWC (Linguistic Inquiry and Word Count). 1989 Roberto Franzosi (perfil no Research Gate) (sociólogo) traz a análise quantitativa de narrativa (quantitative Narrative Analysis) para as Ciências Sociais. 1998 Primeiro desenvolvimento de Topic Models. 1998 John W Mohr conduz a primeira análise quantitativa de visões de mundo. 1999 Peter Bearman (sociólogo) et al. aplicam métodos de rede a narrativas “Narrative network”. 2001 David M. Blei et al desenvolvem a LDA (Latent Dirichlet Allocation). David M. Blei, Andrew Y. Ng, Michael I. Jordan. Latent Dirichlet Allocation. Journal of Machine Learning Research 3 (2003) 993-1022. Github da Blei Lab. 2003 MALLET (MAchine Learning for LanguagE Toolkit), um dos primeiros sistemas de topic models, é criado. “MALLET is a Java-based package for statistical natural language processing, document classification, clustering, topic modeling, information extraction, and other machine learning applications to text. MALLET includes sophisticated tools for document classification: efficient routines for converting text to”features”, a wide variety of algorithms (including Naïve Bayes, Maximum Entropy, and Decision Trees), and code for evaluating classifier performance using several commonly used metrics.” 2005 Quin et al analisam discursos políticos usando topic models. Kevin M Quinn, Burt L Monroe, Michael Colaresi,Michael H Crespin, and Dragomir R Radev. 2010. How to analyze political attention with minimal assumptions and costs. American Journal of Political Science54(1):209–228 2010 Gary King e Daniel Hopkins trazem Topic Models ao mainstream. Hopkins, Daniel, and Gary King. 2010. “A Method of Auto-mated Nonparametric Content Analysis for Social Science.”American Journal of Political Science54(1): 229–47. 2014 Margaret Roberts, et al. desenvolvem “Structural Topic Models”. 2014 - Primeiro Workshop sobre Argument mining ou “argumentation mining” Proceedings das edições anteriores aqui e aqui (na Sessão “Past Workshops” é possível acessar papers de edições anteriores - desde 2014 - do evento). Fonte: Versão ampliada, baseado parcialmente em: “SICSS 2018 - History of Quantitative Text Analysis” slides, video e BROWN, Taylor W. Workshop on automated text analysis no Summer Institute in Computational Social Science na Universidade de Oxford em 2019. Em inglês, sem legendas. Parte 1. Pretende-se posteriormente expandir esta seção, explicando em mais detalhes alguns dos exemplos acima "],["exemplos-de-pesquisas-em-humanidades-digitais.html", "Capítulo 2 Exemplos de Pesquisas em Humanidades Digitais 2.1 Bibliometria / cientometria / cienciometria 2.2 Exemplo: Google Trends como Proxy para epidemias 2.3 Exemplo: Como as fake news sobre a pandemia de Covid-19 se assemelham/divergem entre os países? 2.4 Exemplo: Mudança de significado de palavras 2.5 Exemplo: Análise de complexidade musical 2.6 Exemplo: Polarização 2.7 Exemplo de integração quali-quanti: Complementando dados qualitativos 2.8 Exemplo de integração quali-quanti: Depurando dados quantitativos", " Capítulo 2 Exemplos de Pesquisas em Humanidades Digitais Para entender os potenciais das humanidades digitais para pesquisa, nada melhor que observarmos exemplos de pesquisas. Aqui seguem alguns de exemplos, selecionados pelo potencial de integração entre humanidades e métodos digitais, mais que quanto ao possível mérito/demérito científico. 2.1 Bibliometria / cientometria / cienciometria O estudo das citações, das rede de citações em artigos científicos foi talvez um dos pioneiros no uso de algumas das técnicas aqui descritas, existindo já há décadas. Chamada de “bibliometria”, “cientometria” ou “cienciometria”, ela conta citações de determinados autores em artigos científicos e tenta avaliar o quão influentes estes são. 2.1.1 Exemplo: Filósofos da ciência na Sociologia Por exemplo, HEDSTRÖM et al (1998), buscando saber a influência dos principais filósofos da ciência (Hempel, Kuhn, Popper e Wittgenstein) na sociologia em diferentes países e regiões (países nórdicos, EUA, Grã Bretanha, Alemanha e França), analisou o número de artigos nas principais revistas sociológicas que os citaram. (Fonte: HEDSTRÖM et al. 1998. p. 343) Pelos dados ali apresentados, Popper seria o filósofo mais influente na Europa, principalmente nos países de língua alemã, ao passo que Kuhn seria mais dominante nos EUA. HEDSTRÖM, Peter; SWEDBERG; and UDÉHN, Lars. Popper’s Situational Analysis and Contemporary Sociology. Philosophy of the Social Sciences 1998; 28; 339-64] 2.1.2 Exemplo: A teoria dos sistemas sociais de Niklas Luhmann Stephen Roth analisou a chamada diferenciação funcional dos subsistemas da sociedade mundial, isto é, como os subssitemas como política, economia, religião, ciência, direito, meios de comunicação de massa, etc. se autonomizam em relação aos outros, entre os anos de 1800 e 2000, e para tal utilizou dados do Google Ngram viewer, que por sua vez se baseia no Google Books (mais detalhes sobre estas ferramentes na seção sobre frequência de palavras). Ele encontrou, por exemplo, declínio da presença relativa (isto é, proporcional a cada ano) da palavra “Deus” (god) nos livros em inglês ao longo do tempo. Fonte: Roth (2014, p.46). E se examinarmos os termos relacionados aos Meios de Comunicação de Massa, podemos ver a importância relativa do termo “imprensa” (press) aumentando ao longo do tempo. Fonte: Roth(2014, p.47). Roth, Steffen. 2014. “Fashionable Functions: A Google Ngram View of Trends in Functional Differentiation (1800-2000)” International Journal of Technology and Human Interaction, 34–58. Uma versão posterior, um pouco mais elaborada em: Steffen Roth, Carlton Clark, Nikolay Trofimov, Artur Mkrtichyan, Markus Heidingsfelder, Laura Appignanesi, Miguel Pérez-Valls, Jan Berkel, Jari Kaivo-oja. Futures of a distributed memory. A global brain wave measurement (1800–2000). Technological Forecasting &amp; Social Change 118 (2017) 307–323 Dicas Há o pacote no Python Get Ngrams e o R possui o pacote ngramr que pega os dados no site Google Ngram, os coloca no formato de dataframe do R, bem como plota o gráfico no R usando o pacote ggplot Veja a sessão dedicada ao pacote ngramr neste manual 2.1.3 Exemplo: Tendência de termos chave da Sociologia Além de contar citação nas referências, pode-se contar as palavras mais frequentes no corpo do texto e compará-las. Bernau (2018), por exemplo, coletou dados do JSTOR’s Data for Research e plotou um gráfico longitudinal (ao longo do tempo) de frequência de palavas com termos chaves da sociologia, como “classe”, “raça” e “gênero” da revista American Sociological Review. Ele também disponibilizou o script em R que desenvolveu para esta análise. BERNAU, John A. Text Analysis with JSTOR Archives. November 12, 2018. https://doi.org/10.1177/2378023118809264 2.1.4 Exemplo: Tendências da filosofia Inspirado neste trabalho, Brian Weatherson usou também o pacote jstor_dfr, baixou dados da filosofia, e os clusterizou através de topic modeling. Podemos ver as tendências gerais de vários ramos da filosofia ao longo do tempo: Tendencias na história da filosofia O resultado e mais informações estão em seu livro online: WEATHERSON, Brian . A History of Philosophy Journals. Volume 1: Evidence from Topic Modeling, 1876-2013) 2.2 Exemplo: Google Trends como Proxy para epidemias A busca no Google por certos sintomas de doenças, ou melhor, a variação na busca por certas doenças e sintomas correlatos pode indicar que variação real da doença. Isso acontece, por exemplo, com sintomas de gripe. Um pico no aumento das buscas pelos sintomas indica um prenúncio do aumento das infecções, a ser checado/validado posteriormente. No artigo Google Trends: A Web-Based Tool for Real-Time Surveillance of Disease Outbreaks., os autores explicam que a ferramenta lançada em 2018: &gt; “Google Flu Trends can detect regional outbreaks of influenza 7–10 days before conventional Centers for Disease Control and Prevention surveillance systems” Para funcionar, há certas pré-condições sociais. Vale para gripe, e a ferramenta também prevê aumentos da Covid-19. Na reportagem da Piaui “No carnaval, buscas por “sintomas covid” voltaram a subir; sete dias depois, número de novos casos bateu recorde” de 09 de março de 2021 compara as buscas no google com casos reais. Gráfico: buscas por “sintomas Covid no Google” versus casos reais ver também “Sintomas Covid” en Google trends:.Un indicador alternativo para el seguimiento de la incidencia de casos. com exemplos da Espanha, México, Chile e Argentina. No entanto, a ferramenta que parecia promissora falhou em prever o pico de gripe de 2013, sobrestimando por 140%. A empresa achou melhor terminar o projeto, conforme um artigo da Wired de 2015. Mais detalhes podem lidos no artigo The Parable of Google Flu: Traps in Big Data Analysis. O Instituto de Ciências Cognitvas de Osnabrück leva a ideia adiante, com modelo mais complexo, utilizando dados de redes sociais como Twitter e através do Watson da IBM.(site do projeto). 2.3 Exemplo: Como as fake news sobre a pandemia de Covid-19 se assemelham/divergem entre os países? O relatório de 23/11/2020 sobre isolamento científico, intitulado “scientific [self] isolation” do Laut, Centro de Análise da Liberdade e do Autoritarismo, cruzou as checagens de fake news de 129 países diferentes (há uma plataforma que traduz as reportagens de fact checking de todo mundo para o inglês) e investigou a distribuição de notícias falsas sobre os tratamentos da Covid19. Os pesquisadores procederam então uma distribuição num plano das discussões nos países conforme sua semelhança. Quanto mais próximos, mais semelhantes os debates ao redor do tema. Encontraram então que o Brasil é o país mais isolado em sua discussão envolvendo certos medicamentos, no canto superior direito. grafico 2.4 Exemplo: Mudança de significado de palavras Kulkarni et al (2015) mostraram como através de ferramentas computacionais é possível identificar a mudança de significado de termos, seja ao longo de um século (com dados do Google NGram), seja em dinâmicas mais rápidas, como no twitter. Um dos termos analisado foi o “gay”: Linguistic Change da palavra “gay” Várias outras palavras foram analisadas, como “tape” que significava “fita adesiva”, mas passou a significar também “fita cassete” nos anos 1970; ou “apple” e “windows” que ganharam novo significado com a indústria da computação. KULKARNI, V., Al-Rfou, R; PEROZZI, B e SKIENA, S. Statistically Significant Detection of Linguistic Change. WWW 2015, May 18–22, 2015. http://dx.doi.org/10.1145/2736277.2741627 . 2.5 Exemplo: Análise de complexidade musical A reportagem da Folha de São Paulo Música brasileira foi simplificada ao longo das décadas, diz pesquisa cita o trabalho do cientista de dados Leonardo Sales (blog do autor), que analisou os acordes e vocabulário das letras, com base em 44 mil cifras e 102 mil letras raspadas de sites como cifras.com.br e letras.com.br em uma série de postagens: parte 1, parte 2, parte 3 sobre as letras, parte 4. Os códigos para raspagem de dados destes sites estão disponíveis em Python. grafico Como quase toda notação, há vantagens e desvantagens. Uma desvantagem, que levantou críticas, é que a análise se baseia em cifra, muito utilizada em músicas mais populares, mas inadequada para estilos mais complexos, como jazz. 2.6 Exemplo: Polarização Uma boa parte de pesquisas em política com métodos digitais se dedicou a analisar o fenômeno da polarização política. Christopher A. Bail, Lisa P. Argyle, Taylor W. Brown, John P. Bumpus, Haohan Chen, M. B. Fallin Hunzaker, Jaemin Lee, Marcus Mann, Friedolin Merhout, and Alexander Volfovsky. 2018. Exposure to opposing views on social media can increase political polarization. Proceedings of the National Academy of Sciences 115, 37 (Sept. 2018), 9216–9221. https://doi.org/10.1073/pnas.1804840115 Publisher: National Academy of Sciences Section: Social Sciences. Parte do trabalho de Franzosi descrevemos na seção sobre dados estruturados. Roberto P. Franzosi. Sociology, narrative, and the quality versus quantity debate (Goethe versus Newton): Can computer-assisted story grammars help us understand the rise of Italian fascism (1919–1922)? . Theor Soc (2010) 39:593–629 DOI 10.1007/s11186-010-9131-3 2.7 Exemplo de integração quali-quanti: Complementando dados qualitativos Uma dica de integração quali-quanti (qualitativo e quantitativo) usando análise textual vem do PEW Research. Aqui explicam como a partir de uma análise de grupos focais feita em 2019 com grupos dos EUA e da Grã Bretanha sobre atitudes frente a globalização/nacionalismo, complementaram com pesquisa quantitativa de análise textual, analisando as diferenças entre os grupos de cada país. Usaram técnicas como frequência de palavras, correlação de palavras e Topic modelling. Através destas análises, encontraram tópicos que se mostraram relevantes a serem incorporados em surveys futuros. DEVLIN, Kat.“How quantitative methods can supplement a qualitative approach when working with focus groups”. medium. Dec 18, 2020. 2.8 Exemplo de integração quali-quanti: Depurando dados quantitativos Nem sempre uma tabela de dados estruturados tem tudo estruturado, do modo que a sua pergunta de pesquisa necessite. Dentro de um campo específico de uma tabela pode-se precisar de desmembrar ainda mais os dados. Eis aqui um exemplo. Os pesquisadores do Dadoscope queriam investigar se houve aumento na abertura de igrejas evangélicas durante os anos Lula e Dilma. Os pesquisadores baixaram dados da Receita Federal referente ao Cadastro Nacional de Pessoa Jurídica e filtraram por “94.91–0–00 — Atividades de organizações religiosas ou filosóficas” no campo “Classificação Nacional de Atividades Econômicas”. O problema é que isto agrega não só igrejas evangélicas, como também católicas e de outras religiões e até agremiações filosóficas e institutos de psicanálise. Aqui entra a integração: Para tentarmos realizar a classificação das 150 mil igrejas evangélicas de maneira semi-supervisionada nós usamos Snorkel, uma biblioteca escrita em Python… foi preciso treinar um algoritmo de classificação usando uma amostra dos dados. De forma sucinta, os dados são separados em amostras que são usadas para treino, teste e validação da classificação. Para classificar os mais de 150 mil nomes únicos presentes na amostra de treino, criamos funções que classificam de forma grosseira as igrejas (e.g., se a palavra “assembleia” estiver presente, classificar a igreja como “evangélica”). Depois de escrever dezenas dessas funções, comparamos sua acurácia com uma amostra de teste de 5% dos nomes únicos, manualmente classificados por dois pesquisadores. Feito isso, usamos uma rede neural que combina em camadas estas funções e voilá: 91.8% de acurácia. Sabemos que este resultado não é perfeito, mas ele torna o trabalho de classificação viável. O artigo completo “Exclusivo: Igrejas evangélicas pentecostais tiveram boom de crescimento nos governos Lula e Dilma” pode ser lido aqui: artigo na Forum e o mesmo artigo na Medium. Caderno de notas no dadoscope para entender melhor como o processo foi realizado. Sobre a ferramenta utilizada, a Snorkel, ver a página do Github, introdução (em inglês) ao Snorkel. Por fim, outra dica para pensar a integração de dados quantitativos e qualitativos é a palestra de Dr. Christof Schöch: The Convergence of Quantitative and Qualitative Approaches, ocorrida no 1st Summerschool of Digital Humanities: Distant Reading - Potentials and Applications, em inglês. "],["estrutura-de-dados-e-tipos-de-formatos.html", "Capítulo 3 Estrutura de dados e tipos de formatos 3.1 Dados estruturados 3.2 Dados não estruturados 3.3 Dados semi-estruturados 3.4 Observações finais", " Capítulo 3 Estrutura de dados e tipos de formatos Objetivos do capítulo: Apresentar a distinção entre dados estruturados, dados não estruturados e dados semi estruturados Apresentar alguns formatos de arquivos frequentes na análise textual e humanidades digitais, como csv e tsv, Json, markdown, yaml, LaTex, BibTex, xml e html. Podemos pensar a organização de dados quanto à sua estrutura de três formas: dados estruturados, dados semi estruturados e dados não estruturados. 3.1 Dados estruturados Formatos de arquivos estruturados são csv,xml, json, xls, xlsx, etc. Muitos destes possuem formato de tabela, o que torna bastante fácil encontrar a informação buscada. 3.1.1 Os formatos csv (comma separeted values) e tsv. O formato csv (comma separeted values ou “valores separados por vírgula”) é um dos mais simples, consiste de arquivo de texto simples, com valores separados por um caractere (ou conjunto de caracteres) que separam os valores em cada linha, sendo geralmente vírgula ou ponto e vírgula ou tabulação (tecla tab). Qualquer caractere ou conjunto de caracteres pode ser usado como separador de campos. Na imensa maioria dos casos cada linha é separada pela quebra de linha. Por exemplo, a seguinte tabela: Estado sigla capital região Acre AC Rio Branco Norte Alagoas AL Maceió Nordeste Amapá AP Macapá Norte Amazonas AM Manaus Norte Bahia BA Salvador Nordeste Ceará CE Fortaleza Nordeste Em abrirmos o csv no bloco de notas (notepad): Estado;sigla;capital;região; Acre;AC;Rio Branco;Norte; Alagoas;AL;Maceió;Nordeste; Amapá;AP;Macapá;Norte; Amazonas;AM;Manaus;Norte; Bahia;BA;Salvador;Nordeste; Ceará;CE;Fortaleza;Nordeste; O separador de campo neste arquivo CSV é o ponto e vírgula ;. Ao pedirmos ao computador para localizar qual a designação da sigla “AP”, ele saberá buscar facilmente esta informação. No caso ali, a vírgula é o separador de campos, mas qualquer outro caractere pode ser usado como separador. O formato .tsv, por exemplo, é separado por tabulação - ou o símbolo \\t. Mas é possível encontrar arquivo csv, porém com separador tipo “ ou”;“. 3.1.2 O formato Json O Json (“JavaScript Object Notation”, isto é “Notação de Objetos JavaScript”), é organizado no esquema de pares nome/valor. Por exemplo, ao separarmos primeiro nome firstName de sobrenome lastName no Json: {&quot;employees&quot;:[ { &quot;firstName&quot;:&quot;João&quot;, &quot;lastName&quot;:&quot;da Silva&quot; }, { &quot;firstName&quot;:&quot;Ana&quot;, &quot;lastName&quot;:&quot;Maria&quot; }, { &quot;firstName&quot;:&quot;Joaquim&quot;, &quot;lastName&quot;:&quot;Xavier&quot; } ]} O arquivo json inicia e termina com colchetes [] Todo Json é delimitado por chaves {}, Os dados são representados no esquema nome/valor `\"nome\": \"valor\". Estes são separados por vírgula. O Json tem sido muito usado nas ciência de dados como um modo leve e fácil de armazenamento de dados. É possível que ao requisitar dados em um site, ele venha em Json. DICA: Caso queira mais detalhes sobre o formato Json: Video introdutório sobre o formato Json do canal Código Fonte TV JSON // Dicionário do Programador. Video introdutório, porém mais prático, focado na estrutura do mesmo: JSON em 6 minutos do canal “Canal TI”. Para ver as regras de sintaxe do Json. 3.2 Dados não estruturados Os dados não estruturados são a forma como encontramos em livros impressos, artigos, jornais, revistas, etc. São a forma de texto que nós humanos lemos normalmente. Por exemplo: “Algum tempo hesitei se devia abrir estas memorias pelo principio ou pelo fim, isto é, se poria em primeiro logar o meu nascimento ou a minha morte. Supposto o uso vulgar seja começar pelo nascimento, duas considerações me levaram a adoptar differente methodo: a primeira é que eu não sou propriamente um autor defunto, mas um defunto autor, para quem a campa foi outro berço; a segunda é que o escripto ficaria assim mais galante e mais novo. Moysés, que tambem contou a sua morte, não a poz no introito, mas no cabo: differença radical entre este livro e o Pentateuco….” Este tipo de texto, não estruturado, é alvo do Processamento de linguagem natural (PLN)/ Natural Language Process (NLP). 3.3 Dados semi-estruturados Dados semi-estruturados são um meio termo entre os estruturados e os semi estruturados. Por vezes são chamados de “auto-descritivos”. Vejamos exemplos destes. 3.3.1 Exemplos de dados semi-estruturados 3.3.1.1 Markup Códigos especiais, ou linguagem “markup” é uma notação de documento que tem duas apresentações, uma simplificada como texto normal para humanos, e outra com os “markup” para que o computador entenda. 3.3.2 O formato Markdown Um exemplo bem simples de markup é o Mardown, usado na escrita rápida de textos. Exemplo de markdown 3.3.3 O formato YAML O YAML (“YAML Ain’t Markup Language”) é um padrão de serialização de dados que prima por ser “human friendly”, isto é, de fácil leittura também para humanos. Em arquivos markdown tem-se usado o yaml como cabeçalho, com informações para a renderização do pdf, como título, subtítulo, resumo, palavras chave, etc. Ao converter markdown para o formato final, o computador irá interpretar estas informações. Um exemplo de yaml no arquivo markdown: --- title: &quot;Título do meu pdf&quot; subtitle: subtitulo qualquer author: Fulano de Tal # comentário qualquer fontsize: 12pt urlcolor: blue geometry: margin=2.5cm abstract: &gt; meu resumo bla bla bla bla --- # Titulo Texto texto texto texto texto texto texto ## Subtitulo Texto texto texto texto texto texto texto O cabeçalho em yaml é delimitado no seu início e fim por três traços consecutivos ---. Repare que o símbolo tralha # dentro do yml é interepretado como comentário, já no markdown, indica capítulo. DICA: Um modo prático de trabalhar na elaboração de textos - principalmente acadêmicos - com markdown e yaml é renderizá-lo com o pandoc, que é um canivete suíço na transformação de formatos de texto. Com ele, pode-se criar pdfs, html, doc, docs, odt, etc. a partir de seu arquivo markdown. Pandoc funciona via linha de comando. 3.3.4 O Formato LaTex O LaTex é uma linguagem usada na confecção, principalmente de textos (livros, artigos) acadêmicos, bem como apresentações. O formato LaTex permite grande flexibilidade, e é muito usado para escrever fórmulas matemáticas e gerar as referências bibliográficas automaticamente. Por isso, o LaTex é muito usado no contexto acadêmico. O seu formato mínimo pode ser visto assim: \\documentclass{article} \\begin{document} Olá Mundo \\end{document} Exemplo simples de texto em LaTex e sua renderização Ou em um exemplo um pouco mais elaborado: Exemplo de LaTex com o software Gummi no Linux Perceba que antes de \\begin{document}, isto é, no cabeçalho do documento temos várias informações, entre elas o título do artigo na linha 5 em title{}, e em \\author{}, nas linhas de 6 a 8, temos os autores. Temos também delimitados os capítulos ou seções, no caso ali em section{}. DICA: Para renderizar textos .tex em pdfs deve-se usar um interpretador. O pandoc é uma opção. Embora seja possível usar apenas o interpretador/conversor e um bloco de notas, o mais comum em Tex e LaTex é usar algum programa focado. O TeXstudio é uma boa opção. Caso use Linux e queria a renderização à medida que edita o texto, olhe o Gummi. Caso queira fazer os documentos em LaTex sem ter de “programar”, dê uma olhada no LyX. Há também editores de LaTex online, como o overleaf, que possibilita trabalhar em equipe, observando as alterações feitas por cada pessoa 3.3.5 O formato BibTex Um formato “irmão” do LaTex e markdown é o BibTex, um formato estruturado, com dados bibliográficos usado como fonte para gerar automaticamente a bibliografia ao final do texto renderizado em formatos como Tex, LaTex e markdown. As referências nesse formato ficam salvos num grande arquivo .bib. Um exemplo de citação dentro do bib: @book{Coleman:IntroMathSociology, address = {New York}, pages = {570}, publisher = {The Free Press of Glencoe Collie, Macmillan Limited}, title = {Introduction to Mathematical Sociology}, year = {1964} } O @book indica o tipo, podendo ser também, por exemplo, @article para artigos, @inbook para parte de um livro, @phdthesis para tese de phd (há mais opções). Para uma lista completa, ver The 14 BibTeX entry types. O Coleman:IntroMathSociology é o ID, a identificação única, que é também usado na citação do LaTex (Por exemplo, usando \\cite{Coleman:IntroMathSociology} dentro do Tex) ou do Markdown (usando [@IntroMathSociology] dentro do texto) para que o compilador saiba qual texto está sendo citado no texto. Podemos usar o texto que quisermos ali, desde que sem espaço. Exemplo de citação usando bibtex no LaTex Este arquivo bib que contém as referências bibliográficas, como é texto puro, pode ser editado num editor de texto comum, como o notepad, Gedit, etc. Mas o mais indicado é usar um software gestor de bibliografia, como o JabRef ou o KBibTex. O KBibTex possui menos recursos que o JabRef mas dá plenamente conta do recado, sendo inclusive o gerenciador que utilizo. Além de ser usado para gerar pdfs com as referências, o formato também pode ser usado em pesquisas bibliométricas. 3.3.6 Os formatos xml e html No caso, nome seria “FirstName” e seu valor seria “João”, nome seria “lastName” e seu valor “da Silva” E esses mesmos dados no formato xml: &lt;employees&gt; &lt;employee&gt; &lt;firstName&gt;João&lt;/firstName&gt; &lt;lastName&gt;da Silva&lt;/lastName&gt; &lt;/employee&gt; &lt;employee&gt; &lt;firstName&gt;Ana&lt;/firstName&gt; &lt;lastName&gt;Maria&lt;/lastName&gt; &lt;/employee&gt; &lt;employee&gt; &lt;firstName&gt;Joaquim&lt;/firstName&gt; &lt;lastName&gt;Xavier&lt;/lastName&gt; &lt;/employee&gt; &lt;/employees&gt; Algumas linguagens usadas em texto são chamadas de markup, onde o que é mostrado na tela, para humanos lerem, difere do que o computador “entende”. Exemplo é o xml acima, o html ou ainda linguagens como markdown e LaTex. O html tem por base o xml. O html possui basicamente a seguinte estrutura &lt;!doctype html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Titulo da pagina&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;Título do capítulo&lt;/h1&gt; &lt;p&gt;Texto texto texto&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; Se você copiar o conteúdo acima e salvar num arquivo com o nome, digamos teste.html e abrí-lo com seu navegador de internet (Firefox, Chrome, Opera, etc.), verá como funciona esta ideia de markup. No caso do html, os valores são delimitados por tags, como: &lt;ALGO&gt;conteúdo&lt;/ALGO&gt;, onde &lt;/ indica que estamos fechando a tag. Assim, em &lt;h1&gt;Título do capítulo&lt;/h1&gt;, o texto “Título do capítulo” está entre a tag “h1”. Os arquivos html, assim como o LaTex, possui duas partes principais. O cabeçalho (head) e o corpo (body). Figure 3.1: Tatuagem head/body. Autor desconhecido Entre &lt;head&gt; e seu fechamento, &lt;/head&gt; ficam os metadados, como título, data, etc. Entre &lt;body&gt; e &lt;/body&gt; fica o conteúdo da página que aparece dentro do navegador. Um outro exemplo: &lt;name&gt;Joaquim José da Silva Xavier&lt;/name&gt;, o Tiradentes (&lt;local&gt;Fazenda do Pombal&lt;/local&gt;, batizado em &lt;data&gt;12 de novembro de 1746&lt;/data&gt; — &lt;local&gt;Rio de Janeiro&lt;/local&gt;, &lt;data&gt;21 de abril de 1792&lt;/data&gt;), foi um &lt;profissao&gt;dentista&lt;/profissao&gt;, &lt;profissao&gt;tropeiro&lt;/profissao&gt;, &lt;profissao&gt;minerador&lt;/profissao&gt;, &lt;profissao&gt;comerciante&lt;/profissao&gt;, &lt;profissao&gt;militar&lt;/profissao&gt; e &lt;profissao&gt;ativista político&lt;/profissao&gt; &lt;gentilico&gt;brasileiro&lt;/gentilico&gt;, que atuou nas capitanias de &lt;local&gt;Minas Gerais&lt;/local&gt; e &lt;local&gt;Rio de Janeiro&lt;/local&gt;. Onde podemos ver tags como &lt;name&gt;, &lt;local&gt;, &lt;data&gt;, etc. ao redor de certas informações, o que torna possível ao computador encontrar estas informações. Para tutorial gratuito (em inglês) sobre html, ver W3 School. 3.3.7 Formatos mais raros Há ainda a possibilidade de uso de estruturação de texto não muito comuns e com fins bem específicos. Por exemplo, Franzosi (2010) ao fazer análise da narrativa de jornais italianos da época de ascensão do Fascismo, passou textos não estruturados como este: Republicans plunged in Bissone di S. Cristina around 10pm of this month at the pub Prati. A guy, who went by the name of “captain,” took out a list of names and did the roll call loudly. Para o seguinte formato: [Semantic triplet 1: [Participant: [Actor: republicans]] [[Process: [[Verb: plunge] [Circumstances: [Space: [City: Bissone di S. Cristina] [[Location: pub] [Name: Prati]]]] [[Time: [Date: 05/07/1921] [Hour: 10pm]]]]] [Semantic triplet 2: [Participant: [Actor: captain]] [Process: [[Verb: does roll call] [Circumstances: [Type of action: loudly] [Instrument: list]]] [Participant: [Actor: workers]] Para tal, Franzosi desenvolveu um software para análise de narrativas textuais, o PC-ACE (Program for Computer-Assisted Coding of Events) e pôde ter uma noção melhor da violência cotidiana na época, gerado tabelas como esta: Lista da ocorrências diárias de triplets de violência, obtidas no jornal Avanti! (FRANZOSI, p.607) Frequência da distribuição dos triplets de violência no jornal Avanti! (FRANZOSI, p.607) E ainda fez um “mapa de calor” (“heat map”) com a localização da violência fascista na Itália Mapa de calor de ações fascistas de violência e localização de suas sedes insittucionais (FRANZOSI, p.609) Referência: FRANZOSI, Roberto P.. Sociology, narrative, and the quality versus quantity debate (Goethe versus Newton): Can computer-assisted story grammars help us understand the rise of Italian fascism (1919–1922)?. Theor Soc (2010) 39:593–629. DOI 10.1007/s11186-010-9131-3 3.4 Observações finais Os dados semi-estruturados não tem, portanto, formato de tabela, mas contêm indicações de informações mais abstratas, através de tags ou outras marcações. Com base nestas informações que faremos análises de texto de modo computacional. Este processo de transformação de dados não estruturados em estruturados é chamado de “datificação”. Os formatos como Json, csv, tsv, xml são importantes pois ao solicitar dados em diversos sites, muitos APIs retornam dados nestes formatos. "],["introdução-ao-r.html", "Capítulo 4 Introdução ao R 4.1 Obtendo Ajuda no R 4.2 Tipos de Dados no R (data types) 4.3 Estrutura de dados no R (Data Structures) 4.4 A suíte de pacotes tidyverse", " Capítulo 4 Introdução ao R Objetivos do capítulo: Apresentar como obter ajuda no R Apresentar os tipos de dados e as estruturas de dados no R Apresentar a suíte de pacotes do tidyverse Supondo que já tenha o R e o RStudio instalados (há diversos tutoriais de instalação no Youtube), vamos aos primeiros passos. É possível também rodar o R de modo online, de modo gratuito, sem instalar nada em seu computador, com o One Compiler e o Snippets, tendo mais de 19 mil pacotes instalados. O lado negativo é que não é um modo prático e tão rápido como rodar em seu computador, mas pode servir para pequenos testes. Snippets: um site para rodar R online Outro modo de rodar o R online é criar uma conta no RStudio Cloud, que possui também modalidade de conta gratuita, com algumas limitações: 15 horas de uso mensais, 1 Gb de Ram (pouco, mas funciona). Vantagem do Rstudio Cloud: a instalação de pacotes ocorre sem maiores problemas. Desvantagem: be mais lento que o RStudio Desktop. HISTORIA A linguagem R foi criada no ano de 1993 por Ross Ihaka e Robert Gentleman do Departamento de Estatística da Universidade de Auckland, da Nova Zelândia, se baseando na linguagem S (por sua vez, criada em 1976). O anúncio oficial de lançamento do R ocorreu em 23 de abril de 1997. - IHAKA, Ross; GENTLEMAN, Robert. “R: A Language for Data Analysis and Graphics”. Journal of Computational and Graphical Statistics. 5 (3): 299. set.1996 doi:10.2307/1390807. ISSN 1061-8600. JSTOR 1390807. R é uma linguagem aberta e gratuita, mutiplataforma (roda em Linux, Windows e Mac), publicada sob a licença General Public License (GNU). O R é criado por estatísticos e é bem popular no mundo acadêmico. O R vem com diversos pacotes e funções nativamente. Só para ter uma idéia desta quantidade: Pacote Número de funções base 1244 datasets 104 graphics 87 grDevices 112 methods 203 utils 215 stats 449 Dificilmente vamos dominar estes pacotes e mais as funções dos pacotes específicos que iremos instalar 4.1 Obtendo Ajuda no R A primeira coisa a se aprender é como conseguir ajuda, como conseguir a informação que precisamos. A busca por ajuda é uma constante, mesmo entre os mais experiente. Comando Descrição Exemplo de uso help.start() para abrir o sistema de ajuda em HTML no seu navegador help.start() help(\"função\") ou ?funcao() Acesso à documentação de funções, data sets e outros. No Rstudio, a ajuda é aberta na aba “Help”. Caso rode direto no console, sem o RStudio, uma vez dentro da página de ajuda, digite / para realizar busca e q para sair ?getwd ou help(getwd) help(função, package=\"NOME_PACOTE\") para obter ajudar de um pacote que não foi cerregado help(rlm, package=\"MASS\") Ajuda para a função rlm() do pacote MASS help(package = 'NOME_PACOTE') Mostra um índice de páginas de ajuda para o pacote help(package = 'dplyr' example() Mostra exemplos de uso da função example(grep) help.search() ou ??busca Para busca mais vaga. Caso não se lembre do nome exato da função, ou caso busque uma função que faça determinada tarefa ??regression mostra diversas funções de diversos pacotes que contém a palavra “regression” RSiteSearch() Busca no seu navegador (browser) padrão em sites especializados em R RSiteSearch(\"text analysis\") args() Mostra os argumentos que uma função pode receber. args(grep) apropos() Busca pelo nome de uma função. Útil quando não lembramos exatamente o nome de uma função apropos(\"grep\") demo() Lista todas as demonstrações de todos os seus pacotes demo() demo(package=\"package-name\") lista as demonstrações de um pacote particular demo(package=\"stats\") pacote:: No console do RStudio, apresenta uma lista com todas as funções do pacote, numa janela graphics:: Fonte: Versão expandidada de Getting Help with R. # Buscando por funções que contenham &quot;grep&quot; apropos(&quot;grep&quot;) ## [1] &quot;agrep&quot; &quot;agrepl&quot; &quot;grep&quot; &quot;grepl&quot; &quot;grepRaw&quot; # Mostrando os argumentos que uma função pode receber args(gsub) ## function (pattern, replacement, x, ignore.case = FALSE, perl = FALSE, ## fixed = FALSE, useBytes = FALSE) ## NULL Outra dica é buscar a documentação do pacote no site do cran ou buscar por problemas específicos no o site Stack Overflow. 4.1.1 Obtendo ajuda com o R Commander (Rcmdr) Para quem está aprendendo a usar estatística no R, uma boa dica é usar o pacote R Commander. Ao rodá-lo, uma janela aparece onde escolhemos o tipo de operação, e ele mostra o código para tal. O Rcmdr ensina como importar dados (SPSS, Excell, Stata, ruls, etc.), como realizar diversas operações estatísticas, como plotar gráficos. Ao rodar o Rcmdr pela primeira vez, pacotes adicionais serão instalados, como na imagem abaixo. Instalação do R Commander. Instalando pacotes adicionais Após a instalação de pacotes adicionais, O R Commander abre uma janela com os comandos. R Commander em ação Caso feche a janela do RCommander, rodar library(Rcmdr) novamente não abrirá a janela, uma vez que o pacote já está carregado. A solução é rodar então Commander() para ter a janela novamente. Caso queira fechar este pacote, basta digitar no Console detach(\"package:Rcmdr\", unload=TRUE) ou apenas feche o R. Dicas: R Commander LAURETTO, Marcelo. Introdução ao R Commander. (Pequeno tutorial do RCommander, em PDF e em português.) The R Commander: A Basic-Statistics GUI for R FOX, John. Getting Started With the R Commander. (Artigo do criador do RCommander) Video tutorial do R Commander em ação e em português: Aprendendo Estatística com o R Commander do Departamento de Estatística UFLA 4.1.2 Comentando o código Nas linguagens de programação é possível acrescentar comentários para orientar os humanos e que não serão lidos pelo computador. Cada linguagem tem o seu próprio padrão. No R, assim como em muitas linguagens de programação, os comentários no código são feito com a tralha # ou hashtag, jogo da velha, etc. Tudo que vier depois deste símbolo é então ignorado. print(&quot;Olá mundo!&quot;) ## [1] &quot;Olá mundo!&quot; print(&quot;Olá mundo R!&quot;) # comentário que o computador ignora ## [1] &quot;Olá mundo R!&quot; Se retirarmos o símbolo # temos então mensagem de erro. print(&quot;Olá mundo! Olá R&quot;) comentários que dará erro Error: unexpected symbol in &quot;print(&quot;Olá mundo! Olá R&quot;) comentários&quot; Execution halted 4.2 Tipos de Dados no R (data types) “Tudo que existe no R é um objeto” (John Chambers, apud WICKHAM) Tipos de dados se referem à forma mais simples de objetos, ou tipos de dados atômicos do R (R atomic data types) Há cerca de 25 tipos de dados no R, ou “base types”, mas os tipos de dados mais frequentes são: Tipo de dado no R Exemplo de uso 1 character (texto) “a”, “bla”, “Fulano de tal” 2 numeric (real ou decimal) 2, 43 3 integer (integral) 3L (O “L” é o modo que o R entende que é um integral) 4 logical (lógico) TRUE ou FALSE 5 complex (complexo) 1-4i (números complexos com partes reais e imaginárias) 6 raw (bytes, para arquivos com dados binários) O raw é de uso raro. &gt; charToRaw('olá') resulta em: 6f 6c c3 a1 Uma vez criados os dados, para examinar as características dos objetos há algumas funções no R Função no R para examinar tipos de dados Descrição Exemplo class() Que tipo de objeto é? class(3), class(\"bla\") length() Qual o tamanho do vetore? Quantos itens possui? | ` length(c(“bla”,“ble”))` attributes() Possui metadados? # Criando um texto (character). Tem de ser delimitado entre aspas minha_variavel_texto &lt;- &quot;bla&quot; minha_variavel_texto ## [1] &quot;bla&quot; class(minha_variavel_texto) ## [1] &quot;character&quot; # Criando uma variável numérica. Não pode ser delimitado por aspas. minha_variavel_numerica &lt;- 2 minha_variavel_numerica ## [1] 2 class(minha_variavel_numerica) ## [1] &quot;numeric&quot; # Criando um integral: minha_variavel_integral &lt;- 5L minha_variavel_integral ## [1] 5 class(minha_variavel_integral) ## [1] &quot;integer&quot; # Criando um variável lógica. Também sem aspas. ## 5 é maior que 3? minha_variavel_logica &lt;- 5 &gt; 3 minha_variavel_logica ## [1] TRUE class(minha_variavel_logica) ## [1] &quot;logical&quot; ## 5 é menor que 3? minha_variavel_logica2 &lt;- 5 &lt; 3 minha_variavel_logica2 ## [1] FALSE minha_variavel_logica3 &lt;- TRUE # É possível atribuir minha_variavel_logica3 ## [1] TRUE minha_variavel_logica4 &lt;- F # Também é possível usar abreviaturas &quot;T&quot; e &quot;F&quot; minha_variavel_logica4 ## [1] FALSE # Criando um número complexo: numero_complexo &lt;- 3 + 2i numero_complexo ## [1] 3+2i class(numero_complexo) ## [1] &quot;complex&quot; Na criação de tipos de dados, pode-se usar de coerção (coercion) através das funções as. Função Coerção para as.numeric Numeric as.integer Integer as.double Double as.character Character as.logical Boolean as.raw Raw EM CONSTRUÇÃO 4.3 Estrutura de dados no R (Data Structures) EM CONSTRUÇÃO Vimos os tipos de dados. Estes podem ser organizados de distintos modos, e a depender desta organização, temos diferentes estruturas de dados. O R possui diversas estruturas de dados, como: vetores numéricos e atômicos lista (list) fatores (factors) matriz (matrix) data frame 4.3.1 Vetor (vector) Vetor é a estrutura de dados mais comum no R. Criamos vetores com a função c() de “concatenate” ou “combine”. No caso de strings (character), deve-se colocar os valores dentro de aspas: meu_vetor &lt;- c(&quot;bla&quot;, &quot;ble&quot;, &quot;bli&quot;, &quot;blo&quot;, &quot;blu&quot;) # criando o vetor com strings meu_vetor # imprimindo na tela o vetor que acabamos de criar ## [1] &quot;bla&quot; &quot;ble&quot; &quot;bli&quot; &quot;blo&quot; &quot;blu&quot; class(meu_vetor) ## [1] &quot;character&quot; length(meu_vetor) # observando quantos itens possui nosso vetor ## [1] 5 meu_vetor[3] # acessando o item do 3 de nosso vetor ## [1] &quot;bli&quot; # Criando um vetor de valores booleanos # Não se usa aspas nesse caso vetor_logico &lt;- c(TRUE, FALSE, FALSE) # criando um vetor de valores booleanos class(vetor_logico) ## [1] &quot;logical&quot; # Criando um vetor com diferentes tipo de dados vetor_pan &lt;- c(&quot;bla&quot;, 3, TRUE, 5L) vetor_pan ## [1] &quot;bla&quot; &quot;3&quot; &quot;TRUE&quot; &quot;5&quot; # criando uma sequência numérica de 2 a 8. Desta vez não iremos salvar em uma variável c(2:8) ## [1] 2 3 4 5 6 7 8 Vetores podem ser de dois tipos: 1) atômicos, ou 2) listas. Mas o mais frequênte é encontrarmos “vetor” como sinônimo de “vetor atômico”, e listas serem consideradas como algo à parte. Para examinarmos os vetores, podemos usar as funções: typeof(), length(), class() e str() É ainda possível também atribuir nomes aos elementos de um vetor usando a função names() valores &lt;- c(12, 34, 13) # Criando um vetor de valores nomes &lt;- c(&quot;banana&quot;, &quot;uva&quot;, &quot;abacate&quot;) # Criando vetor com os nomes names(valores) &lt;- nomes # atribuindo o vetor &quot;nomes&quot; como titulo do vetor &quot;valores&quot; valores # imprimindo o vetor com nomes e valores ## banana uva abacate ## 12 34 13 names(valores) # imprimindo apenas os nomes ## [1] &quot;banana&quot; &quot;uva&quot; &quot;abacate&quot; class(valores) ## [1] &quot;numeric&quot; 4.3.2 Fator (Factor) Considere a seguinte tabela fictícia: Nome Altura(cm) Região do país Fulano 175 NE Ciclano 134 SE Beltrano 166 S João 187 S Maria 173 NE José 159 CO Joaquim 161 SE Se em uma coluna certos valores se repetem, podemos considerá-los como “fatores”. No caso a coluna “Nome” possui valores únicos, a coluta “Altura” também, mas a coluna “Região” possui uma quantidade limitada de valores que se repetirão: N,S,NO,SE,CO. Podemos então considerar os valores desta coluna como “fatores”. Fatores podem ser tanto strings, como no caso acima, como integrais, como no caso da idade de estudantes de uma mesma turma. Um exemplo prático: # Criando um vetor de caracteres com siglas de aeroportos brasileiros aeroportos &lt;- c(&quot;BSB&quot;, &quot;CON&quot;, &quot;BSB&quot;, &quot;VIC&quot;, &quot;GUA&quot;, &quot;FOR&quot;, &quot;MAO&quot;,&quot;GUA&quot;, &quot;CON&quot;, &quot;CON&quot;, &quot;REC&quot;, &quot;UDI&quot;, &quot;VIC&quot;, &quot;GUA&quot;) summary(aeroportos) ## Length Class Mode ## 14 character character # Se tento criar um barplot com os valores, dá erro: barplot(summary(aeroportos)) ## Error in barplot.default(summary(aeroportos)): &#39;height&#39; must be a vector or a matrix # Temos de transformar os dados em um fator, que vamos chamar de &quot;aeroportos.factor&quot; aeroportos.fator &lt;- factor(aeroportos) aeroportos.fator # Perceba que &quot;Levels&quot; mostra os valores sem repetição ## [1] BSB CON BSB VIC GUA FOR MAO GUA CON CON REC UDI VIC GUA ## Levels: BSB CON FOR GUA MAO REC UDI VIC summary(aeroportos.fator) # Mostrando a frequência de cada termo com a função &#39;summary()&#39; ## BSB CON FOR GUA MAO REC UDI VIC ## 2 3 1 3 1 1 1 2 # Agora é possível gerar o gráfico de barras (barplot) barplot(summary(aeroportos.fator)) Fatores são comuns em tabelas, mas são raros em análise textual. Assim, ao trabalhar com análise textual, convém mudar as opções globais de strings, para não serem consideradas fatores: options(stringsAsFactors = FALSE) 4.3.3 Matriz (Matrix) Vejamos as matrizes: # 1:12=Os elementos da nossa matriz, 1 a 12 # 4=linhas no eixo vertical 3=linhas noeixo horizontal minha.matriz &lt;- matrix(1:12, 4, 3) minha.matriz ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 ## [3,] 3 7 11 ## [4,] 4 8 12 # Acessando itens, linha 2, coluna 3 minha.matriz[2, 3] ## [1] 10 # Acessando uma linha inteira minha.matriz[2, ] ## [1] 2 6 10 Matrizes também são usadas na análise textual, como nas Document-Term-Matrix e TermDocument Matrix. Veremos em mais detalhes posteriormente, mas apenas para um primeiro contato, o DTM das frases “O rato roeu a roupa do rei”, “O rei riu do rato”, “A roupa do rato é de rei”, onde cada linha representa um documento (documento, neste caso, é cada uma das frases) e as colunas são as palavras. Os números são a frequência de cada termo em cada frase: Table 4.1: Um exemplo de DTM rato rei riu roeu roupa 1 1 0 1 1 1 1 1 0 0 1 1 0 0 1 4.3.4 Listas (list) Listas são conjuntos de vetores mais simples. Para criar uma lista, basta usar o comando list() minha_lista = list(c(T,T,F), &quot;Joaquim José da Silva Xavier&quot;, c(3,6,3,67,22) ) minha_lista ## [[1]] ## [1] TRUE TRUE FALSE ## ## [[2]] ## [1] &quot;Joaquim José da Silva Xavier&quot; ## ## [[3]] ## [1] 3 6 3 67 22 # Acessando o vetor 3 de nossa lista. No caso de listas, usamos o duplo colchete minha_lista[[3]] ## [1] 3 6 3 67 22 # Acessando o vetor 3 de nossa lista, item 5 minha_lista[[3]][5] ## [1] 22 É possível também “quebrar” um texto em vetores, através do comando strsplt() texto = &quot;Bla bla bla. Ble ble ble ble. Bli bli. Blo. Blu blu blu&quot; class(texto) ## [1] &quot;character&quot; strsplit(texto, &quot;\\\\.&quot;) ## [[1]] ## [1] &quot;Bla bla bla&quot; &quot; Ble ble ble ble&quot; &quot; Bli bli&quot; &quot; Blo&quot; ## [5] &quot; Blu blu blu&quot; No exemplo acima, o primeiro argumento em strplit() é a variável, o segundo argumento é o critério a ser usado para quebrar o texto. Alguns caracteres são considerados especiais e precisam ser indicados que queremos seu significado literal. É o caso de ponto final. Para conseguirmos usá-lo como ponto final, ao invés de usarmos \".\", usamos \\\\. . Note que ficaram espaços no início dos elementos dos vetores. Podemos melhorar nosso código acrescentando um espaço em branco após o ponto final: strsplit(texto, &quot;\\\\. &quot;) ## [[1]] ## [1] &quot;Bla bla bla&quot; &quot;Ble ble ble ble&quot; &quot;Bli bli&quot; &quot;Blo&quot; ## [5] &quot;Blu blu blu&quot; Se examinarmos o tipo de arquivo que temos, veremos que se trata de uma lista: texto2 &lt;- strsplit(texto, &quot;\\\\. &quot;) class(texto2) ## [1] &quot;list&quot; Se quisermos que este seja um vetor simples, e não lista: class(texto2) ## [1] &quot;list&quot; # usar o comando unlist() unlist(texto2) ## [1] &quot;Bla bla bla&quot; &quot;Ble ble ble ble&quot; &quot;Bli bli&quot; &quot;Blo&quot; ## [5] &quot;Blu blu blu&quot; class(unlist(texto2)) ## [1] &quot;character&quot; # Ou ainda, acessando o item 1 da lista strsplit(texto, &quot;\\\\. &quot;)[[1]] ## [1] &quot;Bla bla bla&quot; &quot;Ble ble ble ble&quot; &quot;Bli bli&quot; &quot;Blo&quot; ## [5] &quot;Blu blu blu&quot; 4.3.5 Data Frames Uma vantagem de se trabalhar com data frame é que as colunas tem nome e não apenas índice, e isto facilita nosso trabalho na hora de procurar as informações que queremos, pois buscar a coluna pelo seu número pode nos causar confusão. # Criando 3 vetores com valores aleatórios idade &lt;- as.integer(c(12,23,35)) genero &lt;- as.character(c(&quot;fem&quot;, &quot;mas&quot;, &quot;fem&quot;)) raça &lt;- as.character(c(&quot;pret&quot;, &quot;branc&quot;, &quot;pard&quot;)) # Jogando estes vetores no nosso dataframe, que vamos nomear de &quot;df&quot; df &lt;- data.frame(idade, genero, raça) df # vendo nosso dataframe no console ## idade genero raça ## 1 12 fem pret ## 2 23 mas branc ## 3 35 fem pard View(df) # ver o dataframe em uma nova janela pop-up class(df) # Qual a classe de nosso objeto, caso tenhamos esquecido? ## [1] &quot;data.frame&quot; names(df) # retornando apenas o nome das colunas ## [1] &quot;idade&quot; &quot;genero&quot; &quot;raça&quot; df$genero # Filtrando o dataframe &quot;df&quot; pela coluna &quot;genero&quot; ## [1] &quot;fem&quot; &quot;mas&quot; &quot;fem&quot; df[2] # Filtrando o dataframe pela coluna 2: df[numero] ## genero ## 1 fem ## 2 mas ## 3 fem df[,2] # Filtrando o dataframe pela coluna 2: ## [1] &quot;fem&quot; &quot;mas&quot; &quot;fem&quot; # Filtrando o dataframe pelo número da linha: df[número_da_Linha,] . Repare na necessidade da vírgula. df[1,] # imprimindo a linha 1 ## idade genero raça ## 1 12 fem pret df[3,] # imprimindo a linha 3 ## idade genero raça ## 3 35 fem pard df[1:2,] # retornando as linhas de 1 a 2 ## idade genero raça ## 1 12 fem pret ## 2 23 mas branc # media da coluna &quot;idade&quot; mean(df$idade) ## [1] 23.33333 summary(df) # Obtendo uma visão estatística ampla da tabela ## idade genero raça ## Min. :12.00 Length:3 Length:3 ## 1st Qu.:17.50 Class :character Class :character ## Median :23.00 Mode :character Mode :character ## Mean :23.33 ## 3rd Qu.:29.00 ## Max. :35.00 summary(df$genero) # sumário da coluna &quot;gênero&quot; do dataset &quot;df&quot; ## Length Class Mode ## 3 character character summary(df$idade) # sumário da coluna &quot;idade&quot; do dataset &quot;df&quot; ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 12.00 17.50 23.00 23.33 29.00 35.00 # testando se valores da coluna &quot;genero&quot; são &quot;fem&quot; df$genero==&quot;fem&quot; ## [1] TRUE FALSE TRUE # Filtrando apenas as linhas onde a coluna &quot;genero&quot; for &quot;fem&quot;. # Explicando de outro modo: quando &quot;genero&quot; igual a &quot;fem&quot; for verdadeiro, imprima df[df$genero==&quot;fem&quot;,] ## idade genero raça ## 1 12 fem pret ## 3 35 fem pard # Filtrando apenas as linhas onde a escolaridade for maior que 12. df[df$idade &gt; 12,] ## idade genero raça ## 2 23 mas branc ## 3 35 fem pard 4.3.5.1 Manipulando/Editando o data frame # adicionando (append) nova linha ao data.frame # Modo1: através do &#39;nrow&#39; df[nrow(df)+1,]=c(23, &quot;mas&quot;, &quot;branc&quot;) df ## idade genero raça ## 1 12 fem pret ## 2 23 mas branc ## 3 35 fem pard ## 4 23 mas branc # Modo 2: rbind() df &lt;- rbind(df, c(20,&quot;fem&quot;, &quot;pret&quot;)) df ## idade genero raça ## 1 12 fem pret ## 2 23 mas branc ## 3 35 fem pard ## 4 23 mas branc ## 5 20 fem pret # mudando um valor, em uma célula # data.frame[número_da_linha, número_da_coluna] = new_value df[4,1] = 14 df ## idade genero raça ## 1 12 fem pret ## 2 23 mas branc ## 3 35 fem pard ## 4 14 mas branc ## 5 20 fem pret # mudando um valor usando o nome da coluna df[4,&quot;raça&quot;] = &quot;pret&quot; df ## idade genero raça ## 1 12 fem pret ## 2 23 mas branc ## 3 35 fem pard ## 4 14 mas pret ## 5 20 fem pret # mudando valores com condicionais # onde coluna raça for &quot;branc&quot;, mudar na coluna &quot;raça&quot; para &quot;branco&quot; # se o segundo parâmetro não for inserido, todas células ganharão valor &quot;branco&quot; df[df$raça==&quot;branc&quot;, &quot;raça&quot;] = &quot;branco&quot; df ## idade genero raça ## 1 12 fem pret ## 2 23 mas branco ## 3 35 fem pard ## 4 14 mas pret ## 5 20 fem pret # deletando linhas # com base no numero da linha df[-c(1),] # deleta a linha 1 ## idade genero raça ## 2 23 mas branco ## 3 35 fem pard ## 4 14 mas pret ## 5 20 fem pret df &lt;- df[-c(1),] # para tornar a mudança permanente # com base no valor df = df[!df$a==&quot;bla&quot;,] 4.3.6 Considerações finais sobre estrutura de dados Estes não são os únicos tipos de estrutura de dados no R. Alguns pacotes podem criar o seu próprio tipo e não convém tentar cobrir todos. Apesar de considerar tipos diferentes, podemos considerar os diferentes tipos como subcategorias de outras. Data frames são um tipo restrito de listas. Listas, por sua vez, são um tipo de vetor. Veremos a seguir a suíte de pacotes do tidyverse que possui formato próprio de dados. 4.3.7 Criando funções no R Funções são úteis para evitar retrabalho. Possuem a seguinte sintaxe: nome_da_funcao &lt;- function(arg_1, arg_2, ...) { corpo da função } arg_1, arg_2 são os parâmetros de entrada na função. Ao chamar uma função, passa-se argumentos para esta processar. Argumentos também podem conter valores padrão, caso a informação não lhe seja repassada. # Criando função com valores padrão: minhafuncao &lt;- function(a = 2, b = 3) { resultado &lt;- a + b print(resultado) } # chamando a função sem nenhum argumento. minhafuncao() ## [1] 5 # chamando a função repassando argumentos. minhafuncao(10,4) ## [1] 14 # chamando a função repassando parte dos argumentos. minhafuncao(,4) ## [1] 6 4.3.8 Manipulando data e hora Data e hora é algo simples, mas que pode dar dor de cabeça se não usar os pacotes já prontos. O R usa o seguinte formato \"ano-mês-dia hora:minuto:segundo\" ou, por exemplo, 2021-03-09 13:05:03. O mês vem antes de dia e ano vem antes de todos já que assim é possível organizar as datas facilmente usando a ordem alfabética/numérica. P.s: Cuidado! não use data como nome para um objeto, já que data tem significado especial no R. Existe um padrão para datas usado em várias linguagens. Para consultar de modo fácil, olhe a ajuda ?strptime. # Dizendo ao R que nossa string &quot;12/05/1993 13:00:05&quot; é uma data minha_data = &quot;12/05/1993 13:00:05&quot; minha_data2 = strptime(minha_data, format = &quot;%d/%m/%Y %H:%M:%S&quot;) minha_data2 ## [1] &quot;1993-05-12 13:00:05 -03&quot; 4.3.8.1 Gerando uma sequencia de datas no R seq(from=as.Date(&quot;2020-11-29&quot;), to=as.Date(&quot;2021-01-02&quot;), by=&quot;day&quot;) ## [1] &quot;2020-11-29&quot; &quot;2020-11-30&quot; &quot;2020-12-01&quot; &quot;2020-12-02&quot; &quot;2020-12-03&quot; ## [6] &quot;2020-12-04&quot; &quot;2020-12-05&quot; &quot;2020-12-06&quot; &quot;2020-12-07&quot; &quot;2020-12-08&quot; ## [11] &quot;2020-12-09&quot; &quot;2020-12-10&quot; &quot;2020-12-11&quot; &quot;2020-12-12&quot; &quot;2020-12-13&quot; ## [16] &quot;2020-12-14&quot; &quot;2020-12-15&quot; &quot;2020-12-16&quot; &quot;2020-12-17&quot; &quot;2020-12-18&quot; ## [21] &quot;2020-12-19&quot; &quot;2020-12-20&quot; &quot;2020-12-21&quot; &quot;2020-12-22&quot; &quot;2020-12-23&quot; ## [26] &quot;2020-12-24&quot; &quot;2020-12-25&quot; &quot;2020-12-26&quot; &quot;2020-12-27&quot; &quot;2020-12-28&quot; ## [31] &quot;2020-12-29&quot; &quot;2020-12-30&quot; &quot;2020-12-31&quot; &quot;2021-01-01&quot; &quot;2021-01-02&quot; O parâmetro by aceita ainda week e month. O R também calcula anos bissextos. Fazendo um teste, gerando datas entre 28 de fevereiro e 1 de março de diferentes anos: # criando uma função para testes FevMarc &lt;- function(ano) { # nome_da_funcao &lt;- function(input){ minhaDataInicial=paste0(ano, &quot;-02-28&quot;) MinhaDataFinal=paste0(ano, &quot;-03-01&quot;) seq(from=as.Date(minhaDataInicial), to=as.Date(MinhaDataFinal), by=&quot;day&quot;) } for ( ano in c(2015:2021) ) { x= FevMarc(ano) print(x) } ## [1] &quot;2015-02-28&quot; &quot;2015-03-01&quot; ## [1] &quot;2016-02-28&quot; &quot;2016-02-29&quot; &quot;2016-03-01&quot; ## [1] &quot;2017-02-28&quot; &quot;2017-03-01&quot; ## [1] &quot;2018-02-28&quot; &quot;2018-03-01&quot; ## [1] &quot;2019-02-28&quot; &quot;2019-03-01&quot; ## [1] &quot;2020-02-28&quot; &quot;2020-02-29&quot; &quot;2020-03-01&quot; ## [1] &quot;2021-02-28&quot; &quot;2021-03-01&quot; 4.3.8.2 Converter data em nome por extenso do mês Dado um vetor de meses, podemos querer gerar o mês não como número, mas com seu nome. vetor_meses = c(1,4,7,3,2,12,6) library(lubridate) # carregando o pacote lubridate ## ## Attaching package: &#39;lubridate&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## date, intersect, setdiff, union # o locale pode ser que funcione com o valor &#39;pt_BR&#39; Meses &lt;- month(as.numeric(vetor_meses), label = TRUE, # produz os meses locale = &quot;pt_BR.utf8&quot;) # lingua Meses ## [1] jan abr jul mar fev dez jun ## 12 Levels: jan &lt; fev &lt; mar &lt; abr &lt; mai &lt; jun &lt; jul &lt; ago &lt; set &lt; ... &lt; dez # De modo resumido lubridate::month(as.numeric(vetor_meses), label = TRUE, locale = &quot;pt_BR.utf8&quot;) ## [1] jan abr jul mar fev dez jun ## 12 Levels: jan &lt; fev &lt; mar &lt; abr &lt; mai &lt; jun &lt; jul &lt; ago &lt; set &lt; ... &lt; dez Fazendo o caminho inverso, supondo que tenhamos um vetor com nome dos meses e queiramos colocá-los em formato dataPorExtenso &lt;- c(&#39;12 agosto 2020&#39;, &#39;17 dezembro 2021&#39;) as.Date(dataPorExtenso, format = &quot;%d %B %Y&quot;) ## [1] &quot;2020-08-12&quot; &quot;2021-12-17&quot; ver - Manipulação de Dados Podemos trabalhar com outros formatos de datas 4.4 A suíte de pacotes tidyverse EM CONSTRUÇÃO O tidyverse é uma suíte de pacotes bastante úteis e de uso mais fácil que os pacotes tradicionais do R. Dicas O artigo do Hadley Wickham, o criador da ideia, explicando a “filosofia” dos dados “tidy”. &gt; Hadley Wickham. Tidy Data. Journal of Statistical Software Para instalar a suíte de pacotes tidyverse install.packages(tidyverse) O tidyverse conta com diversos pacotes e estão sempre aumentando. library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── ## ✓ ggplot2 3.3.3 ✓ purrr 0.3.4 ## ✓ tibble 3.1.2 ✓ dplyr 1.0.6 ## ✓ tidyr 1.1.3 ✓ stringr 1.4.0 ## ✓ readr 1.4.0 ✓ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x ggplot2::annotate() masks NLP::annotate() ## x lubridate::as.difftime() masks base::as.difftime() ## x lubridate::date() masks base::date() ## x dplyr::filter() masks stats::filter() ## x lubridate::intersect() masks base::intersect() ## x dplyr::lag() masks stats::lag() ## x lubridate::setdiff() masks base::setdiff() ## x lubridate::union() masks base::union() # vendo os pacotes disponíveis no tidyverse: tidyverse_packages() ## [1] &quot;broom&quot; &quot;cli&quot; &quot;crayon&quot; &quot;dbplyr&quot; ## [5] &quot;dplyr&quot; &quot;dtplyr&quot; &quot;forcats&quot; &quot;googledrive&quot; ## [9] &quot;googlesheets4&quot; &quot;ggplot2&quot; &quot;haven&quot; &quot;hms&quot; ## [13] &quot;httr&quot; &quot;jsonlite&quot; &quot;lubridate&quot; &quot;magrittr&quot; ## [17] &quot;modelr&quot; &quot;pillar&quot; &quot;purrr&quot; &quot;readr&quot; ## [21] &quot;readxl&quot; &quot;reprex&quot; &quot;rlang&quot; &quot;rstudioapi&quot; ## [25] &quot;rvest&quot; &quot;stringr&quot; &quot;tibble&quot; &quot;tidyr&quot; ## [29] &quot;xml2&quot; &quot;tidyverse&quot; O ecossistema tidyverse Tidyverse ecosystem picture (author: Silvia Canelón, PhD. Original link) Para nós, destes pacotes, os mais interessantes são: Pacote tidyverse descrição readr Lê dados retangulares (de tabelas) como csv, tsv e fwf rvest usado para minerar dados na web de modo fácil tibble ggplot2 Famoso pacote de geração de gráficos dplyr Para manipulação facilitada de dados stringr Usado na manipução de strings libridate Para lidar com data e hora 4.4.1 Tibbles O “Tibble” é um modo de chamar objetos da classe “tbl_df”, um tipo de data-frame. O tibble, ao ser chamado, mostra apenas as 10 primeiras linhas, ao invés do datafrae inteiro, o que é bem útil quando temos tabelas que não sejam minúsculas. Além disso, um tibble não força caracteres para fatores, que é o modo como lidamos com strings ao fazermos a análise textual. Uma boa introdução aos tibbles (em inglês) temos no capítulo 10.Tibbles do livro “R for Data Science” de Wichham e Grolemund. 4.4.2 Lubridate: facilitando manipulação de datas Com o Lubridate é bem fácil manipular datas, ou mesmo alterar fuso horário. Documentarion Lubridate Tutorial do Lubridate em português do Curso-R Como trabalhar com Data e Hora na linguagem R "],["normalização-de-texto-e-expressões-regulares.html", "Capítulo 5 Normalização de texto e Expressões Regulares 5.1 Expressões regulares (RegEx) 5.2 Metacaracteres especiais 5.3 RegEx no R 5.4 Dicas/Sugestões: Regex no R", " Capítulo 5 Normalização de texto e Expressões Regulares Objetivos deste capítulo: Apresentar as expressões regulares (regex) Apresentar comandos regex no R A normalização de texto consiste em converter texto para formatos mais padronizados, e expressões regulares, ou “regex” são uma ferramenta importante neste processo. A normalização de texto, a limpeza dos dados, sua reestruturação no formato necessário pode tomar a maior parte do tempo em um projeto. Vamos partir de um problema inicial. Quem quiser saber sobre uso de remédios como azitromicina, ivermectina e cloroquina em pacientes com “síndrome respiratória aguda grave”, podemos baixar a tabela csv do opendatasus e olhar a coluna “OUT_ANTIV”. Lá vemos que não há padronização, os dados estão bagunçados (ou “messy data”). Por exemplo, “azitromicina” foi escrita também como “azitronicina”,“az” “azt”, “azitro”, isto é, além de abreviações diferentes, há erros de ortografia. Em casos assim, expressões regulares podem ajudar a normalizar este campo, isto é, deixando tudo num mesmo padrão. Esta parte de limpeza, padronização, também chamada de cleaning e data wrangling (algo como “disputa de dados”) consome boa parte do tempo “É dito, frequentemente, que 80% da análise de dados é gasto no processo de limpeza e preparação dos dados. A preparação dos dados é não só o primeiro passo, como deve ser repetido tantas vezes no curso da análise na medida que novos problemas aparecem ou novos dados são coletados.” It is often said that 80% of data analysis is spent on the process of cleaning and preparing the data (Dasu and Johnson 2003). Data preparation is not just a first step, but must be repeated many times over the course of analysis as new problems come to light or new datais collected. fonte: WICKHAM, Hadley. Tidy Data. Journal of Statistical Software. August 2014, Volume 59, Issue 10 Apesar desse número “80%” parecer ser ficcioso (veja aqui), não deixa de ser uma verdade que grande parte do tempo é gasto neste processo. 5.1 Expressões regulares (RegEx) Certa vez Jamie Zawinski fez uma piada sobre regex que se tornou bastante conhecida: Algumas pessoas, ao se defrontarem com um problema, pensam “Acho que vou usar expressões regulares”. Agora elas tem dois problemas. Talvez um pouco exagerada a piada, expressões regulares podem ser um pouco difíceis no início, mas depois que se aprende, fica difícil viver sem elas. Em editores de texto como Microsoft Word, OpenOffice, LibreOffice, GoogleDocs etc. é possível buscar por trechos de texto idênticos, é possível tornar a busca sensível a termos em maiúsculo e minúsculo. A busca utilizando regex permite isto e muito mais. Busca e subtituição com regex no Libre Office Calc Busca e subtituição com regex no Rstudio em Edit/Replace Find Trata-se de uma ferramenta coringa na hora de limpar ou transformar texto, estando presente nas mais diversas linguagens de programação, com sintaxe mais ou menos comum a todas elas. As expressões regulares (regular expressions), ou RegEx, RegExp, etc., mas mais conhecidas como regex, trata-se de busca e substituição avançada usando não só o texto exato, mas usando padrões de texto. Perguntas das expressões regulares: O quê? Números? Letras? Letras minúsculas ou maiúsculas? Palavras? Símbolos ou caracteres específicos? Quebras de linha? Tabulação? Quantas vezes? Um vez? Uma ou nenhuma? Uma ou várias vezes? Uma quantidade específica? Em regex chama-se isso de quantificadores. Onde? Antes ou de depois do quê? No início ou no fim da sentença? Em regex isso chama-se âncora. É possível delimitar a busca por letras específicas, por escopo de letras e/ou números, pode-se especificar a quantidade de caracteres, tirar ou acrescentar quebra de linha, acrescentar/retirar algo no início ou fim da linha. As regras do Regex são padronizadas em diversas linguagens de programação, apesar de algumas peculiaridades em cada uma, há uma gramática comum. Assim, aprendendo em uma linguagem, você aprendeu em outras.s Supondo que queira ter uma ferramenta simples de consulta do Qualis de revistas da área de sociologia. Baixando o relatório qualis capes de sociologia neste link é possível filtrar com base no nome ou na avaliação. Com regex podemos transformar uma tabela em csv Estado;sigla;capital;região; Acre;AC;Rio Branco;Norte; Alagoas;AL;Maceió;Nordeste; Amapá;AP;Macapá;Norte; Amazonas;AM;Manaus;Norte; Bahia;BA;Salvador;Nordeste; Ceará;CE;Fortaleza;Nordeste; para o formato de tabela em markdwon |Estado | sigla | capital| região| |------|----|-----|-----| |Acre |AC |Rio Branco |Norte| |Alagoas |AL |Maceió |Nordeste| |Amapá |AP |Macapá |Norte| |Amazonas |AM |Manaus |Norte| |Bahia |BA |Salvador |Nordeste| Para termos uma noção do poder do regex, com ele regex também é possível fazer transformações, por exemplo, como pegar datas como “20/03/2020” ,“13/01/1990” ,“04/06/2001”, que estão no formado dd/mm/aaaa (dia/mês/ano) e mudá-las para outra disposição, como aaaa-mm-dd (ano/mês/dia), passando para “2020-03-20”, “1990-01-13”, “2001-06-04”. Há uma sessão nesse manual dedicada a manipulação de datas. Vejamos os parâmetros que usamos com regex 5.1.1 Parâmetros das Regex 5.1.2 Sequência de escape (scaping) Há alguns caracteres especiais que não podem ser usados diretamente numa string, pois possuem um significado especial. As aspas são um caso. Assim, por exemplo: variavel &lt;- 'moinho d'agua' retorna erro. Para poder utilizar as aspas, temos de dizer ao programa para “escapar”. Isto é feito de modo geral com o caractere \\. Assim, para funcionar, nosso exemplo fica: variavel &lt;- 'moinho d\\'agua'. No caso de aspas simples ' ou duplas \", é possível escapar estes caracteres colocando um dentro do outro. Por exemplo: variavel &lt;- \"moinho d'agua\" ou variavel &lt;- 'O conto \"Trio em Lá Menor\" de Machado de Assis foi publicado em 1896.'. Outros caracteres que levam escape são: 5.2 Metacaracteres especiais Alguns caracteres tem significado especial \\n usado para quebra de linha, ou “newline”. \\t caractere de tabulação. \\\\ para usar a própria barra \\r retorno de carro ou “carriage return”, que move o cursor para a próxima linha, mas sem aparecer em seu início. De uso no Windows. \\v tab vertical \\f form feed 5.2.0.1 Âncoras símbolo descrição exemplo \\^a Início da string ^a busca “aaa aaa” a$ Fim da string a$ busca “aaaa aaa” \\b limite de palavra \\b[A-Za-z]\\b \\B NÃO limite de palavra \\\\&lt; Início de uma palavra \\\\&gt; Fim de uma palavra Exemplo de substituição com regex no Google Docs: acrescentou-se “Disse:” no início das células demarcadas 5.2.1 Operadores Descrição Exemplo . Pega um caractere único, qualquer um. Dado aba, abc, asa, a busca por ab. retorna aba, abc. A busca de a.. retorna aba, asa [..] Lista de caracteres, podendo usar a barra - para definir um escopo. Dado abcdefgh a busca por [a-d], retorna abcd [^..] Inversão do caso anterior. Pega todos os elementos, exceto os especificados ali Dado abcdefgh a busca por [^a-d], retorna efgh. | Operador booleano do tipo “ou”. Dado \"bananada, bananeira, bandolim\", a busca utilizando \"eira|dol\", retorna \"bananeira, bandolim\" (...) Agrupamento. É bastante usado com “backreference” Dado \"banana, bananada, bananeira\", a busca utilizando \"banan(ada|eira)\", retorna \"bananada, bananeira\" Podemos usar os colchetes [] para pegarmos um escopo. Se usamos [a-f], então nossa expressão pega qualquer coisa que contenha a,b,c,d,e, ou f, em minúscula. Assim, se quisermos pegar todas as letras minúsculas, usamos [a-z], se quisermos pegar todas as maiúsculas e minúsculas, usamos [A-Za-z]. Mas atenção, desta forma não detectamos caracteres acentuados, como “à”, “ç”, “ã”, etc. Para isso, acrescentamos à-ÿ que pega o escopo de caracteres acentuados. 5.2.2 Quantificadores Os quantificadores especificam quantas vezes o padrão anterior é repetido símbolo explicação exemplo * busca o item anterior zero ou mais vezes + busca o item anterior uma ou mais vezes ? o item anterior é opcional, e pode aparecer no máximo uma vez. O padrão bananas? encontra “banana” “bananas” “bananal” “bananada” {n} busca o item anterior exatamente n vezes {n,} busca o item anterior n vezes ou mais {n,m} busca o item anterior com no mínimo n vezes e com no máximo m vezes. (Faz sentido usálo com delimitadores como \\b) no_vec &lt;- c(\"no\", \"nono\", \"nonono\", \"nononono\", \"nonato\") grep(\"(no){2,3}\", no_vec, value=TRUE) [1] \"nono\" \"nonono\" \"nononono\" Os quantificadores podem ser combinados. Por exemplo: texto &lt;- &quot;O SR. PRESIDENTE (Omar Aziz. PSD - AM. Fala da Presidência.) – Os Srs. Senadores que as aprovam permaneçam como se encontram. (Pausa.)&quot; Temos dois textos entre parênteses. Assim se usarmos: stringr::str_extract(texto, &#39;\\\\(.*\\\\)&#39;) ## [1] &quot;(Omar Aziz. PSD - AM. Fala da Presidência.) – Os Srs. Senadores que as aprovam permaneçam como se encontram. (Pausa.)&quot; Pegamos o texto desde o primeiro parêntese até o último. Isto acontece por o símbolo de asterisco * ser “greedy” (voraz, guloso, segundo a terminologia de regex). Precisamos torná-lo “lazy” (preguiçoso) se quisermos pegar apenas o primeiro caso. O fazemos com a sequência de quantificadores *? stringr::str_extract(texto, &#39;\\\\(.*?\\\\)&#39;) ## [1] &quot;(Omar Aziz. PSD - AM. Fala da Presidência.)&quot; Exemplos: banana_vec &lt;- c(&quot;banana&quot;, &quot;bananas&quot;, &quot;bananal&quot;, &quot;bananada&quot;, &quot;bananeira&quot;, &quot;bandolim&quot;) grep(&quot;ban.*&quot;, banana_vec, value=T) ## [1] &quot;banana&quot; &quot;bananas&quot; &quot;bananal&quot; &quot;bananada&quot; &quot;bananeira&quot; &quot;bandolim&quot; grep(&quot;banan.*&quot;, banana_vec, value=T) ## [1] &quot;banana&quot; &quot;bananas&quot; &quot;bananal&quot; &quot;bananada&quot; &quot;bananeira&quot; grep(&quot;banana.*&quot;, banana_vec, value=T) ## [1] &quot;banana&quot; &quot;bananas&quot; &quot;bananal&quot; &quot;bananada&quot; grep(&quot;banana.?&quot;, banana_vec, value=TRUE) ## [1] &quot;banana&quot; &quot;bananas&quot; &quot;bananal&quot; &quot;bananada&quot; grep(&quot;banana.?$&quot;, banana_vec, value=TRUE) ## [1] &quot;banana&quot; &quot;bananas&quot; &quot;bananal&quot; # A sequência &quot;an&quot; pode aparecer uma ou mais vezes repetida, como &quot;ban&quot;, &quot;banan&quot;, &quot;bananan&quot;, &quot;banananan&quot;, etc. grep(&quot;b(an)+&quot;, banana_vec, value=T) ## [1] &quot;banana&quot; &quot;bananas&quot; &quot;bananal&quot; &quot;bananada&quot; &quot;bananeira&quot; &quot;bandolim&quot; grep(&quot;b[an]{2}&quot;, banana_vec, value=T) ## [1] &quot;banana&quot; &quot;bananas&quot; &quot;bananal&quot; &quot;bananada&quot; &quot;bananeira&quot; &quot;bandolim&quot; grep(&quot;b(an){2}&quot;, banana_vec, value=T) ## [1] &quot;banana&quot; &quot;bananas&quot; &quot;bananal&quot; &quot;bananada&quot; &quot;bananeira&quot; # se quiser delimitar a busca, restringindo a encontrar apenas o termo exato (exact match), deve-se usar o delimitador de palavra &quot;\\b&quot;. E para usá-lo aqui, deve-se acrescentar mais uma barra para o escape, ficando &quot;\\\\b&quot;. grep(&quot;\\\\bbanana\\\\b&quot;, banana_vec, value=T) ## [1] &quot;banana&quot; grep(&quot;\\\\bbanana.\\\\b&quot;, banana_vec, value=T) ## [1] &quot;bananas&quot; &quot;bananal&quot; A História do comando GREP: O comando grep exite graças à uma demanda da análise textual. Na década de 1960, um problema intrigava diversos pesquisadores. Quem são os autores de cada texto dos textos dos Federalistas? Os Federalist Papers foram 85 textos publicados de modo anônimo sob pseudônimo de “Publius”, podendo ser de James Madinson, Alexander Hamilton, John Jay em 1787 e 1788. Quem escreveu qual? Lee McMahon comentou deste problema com seus colegas. Ele queria buscar por certas palavras através de vários textos. Ken Thompson - personagem importante na história da programação - escutou aquilo e no dia seguinte voltou com o programa que veio a ser chamado grep: “g” de “Global”, procurar em vários documentos “re” de regular expressions “p” de print. Se achar o padrão de regex, então imprima na tela. Poderia ainda ser apagar, ou substituir também. O grep se tornou parte obrigatória do arcabouço computacional na programação em geral, principalmente nos sistemas baseados em Unix (Mac e Linux). Esta história foi contada em Where GREP Came From - Computerphile. A identificação de autoria pelo padrão de escrita veio a ser chamado de “digitais linguísticas” (linguistic fingerprint). Posteriormente, um grupo de pesquisadores da Universidade de Aston usaram de técnica semelhante (porém mais avançada) para identificar a identidade real de Satoshi Nakamoto, criador do Bitcoin e da tecnologia de blockchain. 5.2.3 Classes POSIX POSIX Descrição Equivalente [:digit:] ou \\\\d Dígitos: 0 1 2 3 4 5 6 7 8 9. [0-9] [:lower:] letras em minúsculo (Lower-case) [a-z] [:upper:] Caracteres maiúsculos [A-Z] [:alpha:] Cracteres alfabéticos: [:lower:] and [:upper:]. [a-zA-Z] [:alnum:] ou \\w Caracteres alfanuméricos: [:alpha:] e [:digit:]. [a-zA-Z0-9] ou [A-z0-9] \\W Não palavra [^A-z0-9] [:blank:] Caracteres vazios, como espaço e tab. [:cntrl:] Caracteres de controle, como \\n, \\r, [\\x00-\\x1F\\x7F] [:graph:] Caracteres gráficos : [:alnum:] e [:punct:]. [:print:] Caracteres impimíveis: [:alnum:], [:punct:] and space. [:space:] Caracteres de espaço: tab \\t, nova linha \\n, tab vertical, form feed, carriage return \\r, espaço e outros similares [:xdigit:] Dígitos hexadecimais: 0 1 2 3 4 5 6 7 8 9 A B C D E F a b c d e f. \\s espaço em branco [ ] \\S (maiúsculo) tudo, exceto espaço em branco [^ ] [:punct:] Caracteres de pontuação: ! ” # $ % &amp; ’ ( ) * + , - . / : ; &lt; = &gt; ? @ [  ] ^ _ ` { } ~ . | 5.2.4 Backreference Podemos reaproveitar trechos através de backreference. Ao fazer substituições nos textos, um modo de delimitar ou reorganizar os elementos é feita com o uso de backreference. Indicamos o que queremos reutilizar com os parênteses, e reutilizamos através de numeração como ‘\\1’, ‘\\2’. nome &lt;- &quot;Fulano Silva&quot; gsub(&#39;(.*) (.*)&#39;, &#39;\\\\2,\\\\1.&#39;, nome) ## [1] &quot;Silva,Fulano.&quot; Onde \\\\0 indica toda a string, \\\\1 indica o primeiro caso, \\\\2 indica o segundo e assim por diante. 5.2.5 Dicas/Sugestões SICSS 2019 – Basic text analysis with grep. Video do YouTube do Summer Institute in Computational Social Science. https://www.rexegg.com/regex-quickstart.html 5.2.6 Indicação de leitura sobre Regex JURAFSKY, Dan.; MARTIN, James H.Regular Expressions, Text Normalization, Edit Distance in __ Speech and language processing: An introduction to speech recognition, computational linguistics and natural language processing. Upper Saddle River, NJ: Prentice Hall, 2020. Cap.2 p.2-28 FRIEDL, Jeffrey E. F. Mastering Regular Expressions, 3rd Edition. 2006. O’Reilly Media, Inc. ISBN: 9780596528126. Há opção free trial do livro. cheatsheet de expressões regulares (em inglês) Softwares Para lidar com dados bagunçados (“messy data”), uma sugestão é usar o OpenRefine. Antes “Google Refine”, mas após a Google abandonar o projeto, virou “Open Refine”, livre e de código aberto, ajuda a padronizar dados de tabelas, planilhas, limpando, corrigindo, filtrando (com regex), clusterizando ou transformando em outro formato. No site há tutoriais. Ele abre no seu navegador de internet, mas roda direto do seu computador. Usam esta ferramenta do OpenRefine, por exemplo, o Basômetro do Estadão Dados, que mede o apoio que o governo federal tem no legislativo e o ProPublica de jornalismo investigativo independente, No próprio site do OpenRefine há tutoriais e no ProPublica, ambos em inglês). 5.3 RegEx no R O uso de regex vem por padrão no R, no pacote base, mas há também uma expansão de regex com o pacote stringr usado para lidarmos com strings e tem uma sintaxe um pouco diferente. 5.3.1 Grep Como vimos, o grep serve para filtrar linhas que contenham algo que buscamos. Primeiro, vamos criar duas variáveis com texto. nosso_texto &lt;- &quot;Estamos aprendendo expressão regular no R. Bla bla bla no no no.&quot; poema &lt;- c(&quot;E agora, José?&quot;, &quot;A festa acabou,&quot; , &quot;a luz apagou,&quot;, &quot;o povo sumiu,&quot;, &quot;a noite esfriou&quot;, &quot;e agora, José?&quot;) Usaremos estas frases como exemplos a seguir: 5.3.1.1 grepl() Com o grepl(\"Termo\", Nome_variável) verificamos se uma expressão está presente em nosso texto e ele nos retorna booleanos TRUE(verdadeiro, termo encontrado) ou FALSE (falso, termo não encontrado). grepl(&quot;Estamos&quot;, nosso_texto) ## [1] TRUE grepl(&quot;paralelepípedo&quot;, nosso_texto) ## [1] FALSE grepl(&quot;festa&quot;, poema) ## [1] FALSE TRUE FALSE FALSE FALSE FALSE 5.3.1.2 grep(“padrão”, variável) Tendo várias linhas em um vetor, como por exemplo: grep(&quot;festa&quot;, poema) ## [1] 2 grep(&quot;José&quot;, poema) ## [1] 1 6 grep(&quot;josé&quot;, poema) ## integer(0) No primeiro e segundo exemplo, buscamos por “festa” e “José” e o R nos retornou o local do termo buscado no vetor. No terceiro exemplo, o termo “josé” - todo em minúsculo - não existia, e por isso o R retornou integer(0). Se quisermos ver as respectivas linhas, fazemos assim: poema[grep(&quot;José&quot;, poema)] ## [1] &quot;E agora, José?&quot; &quot;e agora, José?&quot; poema[grep(&quot;festa&quot;, poema)] ## [1] &quot;A festa acabou,&quot; Ou ainda, usando value = TRUE) para mostrar o texto encontrado, e não apenas o índice. grep(&quot;José&quot;, poema, value = TRUE) ## [1] &quot;E agora, José?&quot; &quot;e agora, José?&quot; grep(&quot;festa&quot;, poema, value = TRUE) ## [1] &quot;A festa acabou,&quot; O grep em sua versão completa no R, pode ter ainda as seguintes designações: &gt; grep(padrão, nome_variável, ignore.case = FALSE, perl = FALSE, value = FALSE, fixed = FALSE, useBytes = FALSE, invert = FALSE) Campo do grep Descrição padrão padrão regex a ser buscado nome_variável Nome da variável. Se o grep vier após um pipe (%&gt;%), é substituído por ponto . ignore.case = FALSE se ignore.case = TRUE , então a busca não distinguirá maiúscula de minúscula perl = FALSE usa o padrão Perl de regex value = FALSE Imprime os índices. Se TRUE, mostra os elementos. `| |fixed = FALSE| considera a busca como texto puro, sem nenhuma expressão regex| |useBytes = FALSE| seTRUEa busca é feita byte-a-byte ao invés de caractere a caractere| |invert = FALSE| Seinvert = TRUE`, então serão exibidas todas as linhas, exceto as que contenham o padrão buscado. https://stat.ethz.ch/R-manual/R-devel/library/base/html/grep.html 5.3.2 gsub() Se quisermos substituir trechos de nossa frase, usamos gsub(\"o que será retirado\", \"O que queremos colocar no lugar\", nome_variável) gsub(&quot;Bla&quot;, &quot;paralelepípedo&quot;, nosso_texto) ## [1] &quot;Estamos aprendendo expressão regular no R. paralelepípedo bla bla no no no.&quot; gsub(&quot;bla&quot;, &quot;paralelepípedo&quot;, nosso_texto) ## [1] &quot;Estamos aprendendo expressão regular no R. Bla paralelepípedo paralelepípedo no no no.&quot; Também podemos usar gsub para limpar nosso texto, colocando \"\" no campo intermediário da nossa função gsub. Por exemplo, se quisermos retirar os “no”: gsub(&quot;no&quot;, &quot;&quot;, nosso_texto) ## [1] &quot;Estamos aprendendo expressão regular R. Bla bla bla .&quot; Ou se quisermos retirar os “bla”: gsub(&quot;bla&quot;, &quot;&quot;, nosso_texto) ## [1] &quot;Estamos aprendendo expressão regular no R. Bla no no no.&quot; gsub(&quot;[bB]la&quot;, &quot;&quot;, nosso_texto) ## [1] &quot;Estamos aprendendo expressão regular no R. no no no.&quot; gsub(&quot;?&quot;, &quot;&quot;, poema) ## [1] &quot;E agora, José?&quot; &quot;A festa acabou,&quot; &quot;a luz apagou,&quot; &quot;o povo sumiu,&quot; ## [5] &quot;a noite esfriou&quot; &quot;e agora, José?&quot; gsub(&quot;,&quot;, &quot;&quot;, poema) ## [1] &quot;E agora José?&quot; &quot;A festa acabou&quot; &quot;a luz apagou&quot; &quot;o povo sumiu&quot; ## [5] &quot;a noite esfriou&quot; &quot;e agora José?&quot; A retirada da vírgula ocorreu normalmente. Já a interrogação continuou, uma vez que é um caractere usado com significado específico no regex. Devemos então indicar que se trata de uma interrogação comum. gsub(&quot;\\\\?&quot;, &quot;&quot;, poema) ## [1] &quot;E agora, José&quot; &quot;A festa acabou,&quot; &quot;a luz apagou,&quot; &quot;o povo sumiu,&quot; ## [5] &quot;a noite esfriou&quot; &quot;e agora, José&quot; E retirando também a vírgula: gsub(&quot;[\\\\?,]&quot;, &quot;&quot;, poema) ## [1] &quot;E agora José&quot; &quot;A festa acabou&quot; &quot;a luz apagou&quot; &quot;o povo sumiu&quot; ## [5] &quot;a noite esfriou&quot; &quot;e agora José&quot; 5.3.3 Exercício: Qual o Qualis de certas revistas? Quais revistas possuem certo Qualis? Vamos montar um script de busca dos Qualis de revistas de sociologia. Vasculhando a internet, encontrei uma lista com os Qualis das revistas de sociologia em pdf e salvei no meu computador. Precisamos extrair a informação do pdf, precisamos dos pacotes pdftools e tidyverse. library(pdftools) ## Using poppler version 21.06.1 library(tidyverse) meupdf &lt;- pdf_text(&quot;/home/alisson/Documentos/Programação/R/1-Qualis-Sociologia-Atualizado.pdf&quot;) %&gt;% readr::read_lines() # carregando o arquivo pdf na variável &quot;meupdf&quot; grep(&quot;Soziolo&quot;, meupdf, value=T) # filtrando a tabela, toda linha que tiver &quot;Soziol&quot; irá aparecer. ## [1] &quot;0340-918X Soziologie (Opladen) A1 SOCIOLOGIA Atualizado&quot; ## [2] &quot;0863-1808 Berliner Journal fur Soziologie A2 SOCIOLOGIA Atualizado&quot; ## [3] &quot;0340-1804 Zeitschrift fur Soziologie A2 SOCIOLOGIA Atualizado&quot; grep(&quot;A1&quot;, meupdf, value=T) # filtrando a tabela, toda linha que tiver &quot;A1&quot; irá aparecer, não importa em que coluna esteja. ## [1] &quot;0335-5322 Actes de la Recherche en Sciences Sociales A1 SOCIOLOGIA Atualizado&quot; ## [2] &quot;0002-7294 American Anthropologist A1 SOCIOLOGIA Atualizado&quot; ## [3] &quot;1042-0533 American Journal of Human Biology A1 SOCIOLOGIA Atualizado&quot; ## [4] &quot;0003-2573 Análise Social A1 SOCIOLOGIA Atualizado&quot; ## [5] &quot;1573-3416 An International Journal of Politics, Culture and Society (Dordrecht. Online) A1 SOCIOLOGIA Atualizado&quot; ## [6] &quot;0004-0002 Archives of Sexual Behavior A1 SOCIOLOGIA Atualizado&quot; ## [7] &quot;0261-3050 Bulletin of Latin American Research A1 SOCIOLOGIA Atualizado&quot; ## [8] &quot;1983-8239 Caderno CRH (Online) A1 SOCIOLOGIA Atualizado&quot; ## [9] &quot;0103-4979 Caderno CRH (UFBA. Impresso) A1 SOCIOLOGIA Atualizado&quot; ## [10] &quot;0104-8333 Cadernos Pagu (UNICAMP. Impresso) A1 SOCIOLOGIA Atualizado&quot; ## [11] &quot;0010-4159 Comparative Politics A1 SOCIOLOGIA Atualizado&quot; ## [12] &quot;1351-0487 Constellations (Oxford. Print) A1 SOCIOLOGIA Atualizado&quot; ## [13] &quot;0094-3061 Contemporary Sociology (Washington) A1 SOCIOLOGIA Atualizado&quot; ## [14] &quot;1354-067X Culture &amp; Psychology A1 SOCIOLOGIA Atualizado&quot; ## [15] &quot;0011-3204 Current Anthropology A1 SOCIOLOGIA Atualizado&quot; ## [16] &quot;0011-3921 Current Sociology (Print) A1 SOCIOLOGIA Atualizado&quot; ## [17] &quot;0011-5258 Dados (Rio de Janeiro. Impresso) A1 SOCIOLOGIA Atualizado&quot; ## [18] &quot;1351-0347 Democratization (London) A1 SOCIOLOGIA Atualizado&quot; ## [19] &quot;0101-7330 Educação &amp; Sociedade (Impresso) A1 SOCIOLOGIA Atualizado&quot; ## [20] &quot;1044-3983 Epidemiology (Cambridge, Mass., Print) A1 SOCIOLOGIA Atualizado&quot; ## [21] &quot;0185-4186 Estudios Sociológicos A1 SOCIOLOGIA Atualizado&quot; ## [22] &quot;0141-9870 Ethnic and Racial Studies (Print) A1 SOCIOLOGIA Atualizado&quot; ## [23] &quot;0873-6561 Etnográfica (Lisboa A1 SOCIOLOGIA Atualizado&quot; ## [24] &quot;0014-2182 Etudes Rurales A1 SOCIOLOGIA Atualizado&quot; ## [25] &quot;1368-4310 European Journal of Social Theory A1 SOCIOLOGIA Atualizado&quot; ## [26] &quot;0104-5970 História, Ciências, Saúde-Manguinhos (Impresso) A1 SOCIOLOGIA Atualizado&quot; ## [27] &quot;1678-4758 História, Ciências, Saúde-Manguinhos (Online) A1 SOCIOLOGIA Atualizado&quot; ## [28] &quot;1806-9983 Horizontes Antropológicos (Online) A1 SOCIOLOGIA Atualizado&quot; ## [29] &quot;0104-7183 Horizontes Antropológicos (UFRGS. Impresso) A1 SOCIOLOGIA Atualizado&quot; ## [30] &quot;0539-0184 Information sur les Sciences Sociales (Paris) A1 SOCIOLOGIA Atualizado&quot; ## [31] &quot;0891-4486 International Journal of Politics, Culture and Society A1 SOCIOLOGIA Atualizado&quot; ## [32] &quot;2182-4096 International Journal on Working Conditions A1 SOCIOLOGIA Atualizado&quot; ## [33] &quot;0192-5121 International Political Science Review A1 SOCIOLOGIA Atualizado&quot; ## [34] &quot;0269-2171 International Review of Applied Economics A1 SOCIOLOGIA Atualizado&quot; ## [35] &quot;0020-8701 International Social Science Journal (Print) A1 SOCIOLOGIA Atualizado&quot; ## [36] &quot;0268-5809 International Sociology A1 SOCIOLOGIA Atualizado&quot; ## [37] &quot;1468-795X Journal of Classical Sociology A1 SOCIOLOGIA Atualizado&quot; ## [38] &quot;1467-6443 Journal of Historical Sociology (Online) A1 SOCIOLOGIA Atualizado&quot; ## [39] &quot;0022-216X Journal of Latin American Studies (Print) A1 SOCIOLOGIA Atualizado&quot; ## [40] &quot;0022-4537 Journal of Social Issues (Print) A1 SOCIOLOGIA Atualizado&quot; ## [41] &quot;0094-582X Latin American Perspectives A1 SOCIOLOGIA Atualizado&quot; ## [42] &quot;0023-8791 Latin American Research Review A1 SOCIOLOGIA Atualizado&quot; ## [43] &quot;0102-6445 Lua Nova (Impresso) A1 SOCIOLOGIA Atualizado&quot; ## [44] &quot;1678-4944 Mana (Rio de Janeiro. Online) A1 SOCIOLOGIA Atualizado&quot; ## [45] &quot;0104-9313 Mana (UFRJ. Impresso) A1 SOCIOLOGIA Atualizado&quot; ## [46] &quot;0101-3300 Novos Estudos CEBRAP (Impresso) A1 SOCIOLOGIA Atualizado&quot; ## [47] &quot;0102-4469 Perspectiva Teológica (Belo Horizonte) A1 SOCIOLOGIA Atualizado&quot; ## [48] &quot;0048-3931 Philosophy of the Social Sciences A1 SOCIOLOGIA Atualizado&quot; ## [49] &quot;0191-4537 Philosophy &amp; Social Criticism A1 SOCIOLOGIA Atualizado&quot; ## [50] &quot;0276-5624 Research in Social Stratification and Mobility A1 SOCIOLOGIA Atualizado&quot; ## [51] &quot;0048-7333 Research Policy A1 SOCIOLOGIA Atualizado&quot; ## [52] &quot;0102-6909 Revista Brasileira de Ciências Sociais (Impresso) A1 SOCIOLOGIA Atualizado&quot; ## [53] &quot;0254-1106 Revista Crítica de Ciências Sociais A1 SOCIOLOGIA Atualizado&quot; ## [54] &quot;0034-7701 Revista de Antropologia (USP. Impresso) A1 SOCIOLOGIA Atualizado&quot; ## [55] &quot;0188-2503 Revista Mexicana de SociologÍa A1 SOCIOLOGIA Atualizado&quot; ## [56] &quot;0390-6701 Revue Internationale de Sociologie A1 SOCIOLOGIA Atualizado&quot; ## [57] &quot;0037-7686 Social Compass (Imprimé) A1 SOCIOLOGIA Atualizado&quot; ## [58] &quot;0037-7732 Social Forces A1 SOCIOLOGIA Atualizado&quot; ## [59] &quot;0303-8300 Social Indicators Research A1 SOCIOLOGIA Atualizado&quot; ## [60] &quot;0277-9536 Social Science &amp; Medicine (1982) A1 SOCIOLOGIA Atualizado&quot; ## [61] &quot;0102-6992 Sociedade e Estado (UnB. Impresso) A1 SOCIOLOGIA Atualizado&quot; ## [62] &quot;0038-0199 Sociologia Ruralis (Print) A1 SOCIOLOGIA Atualizado&quot; ## [63] &quot;1517-4522 Sociologias (UFRGS. Impresso) A1 SOCIOLOGIA Atualizado&quot; ## [64] &quot;0735-2751 Sociological Theory A1 SOCIOLOGIA Atualizado&quot; ## [65] &quot;0038-0296 Sociologie du Travail A1 SOCIOLOGIA Atualizado&quot; ## [66] &quot;1069-4404 Sociology of Religion A1 SOCIOLOGIA Atualizado&quot; ## [67] &quot;0340-918X Soziologie (Opladen) A1 SOCIOLOGIA Atualizado&quot; ## [68] &quot;0103-2070 Tempo Social (USP. Impresso) A1 SOCIOLOGIA Atualizado&quot; ## [69] &quot;0002-7162 The Annals of the American Academy of Political and Social Science A1 SOCIOLOGIA Atualizado&quot; ## [70] &quot;0263-2764 Theory, Culture &amp; Society A1 SOCIOLOGIA Atualizado&quot; ## [71] &quot;0725-5136 Thesis Eleven (Print) A1 SOCIOLOGIA Atualizado&quot; ## [72] &quot;0730-8884 Work and Occupations A1 SOCIOLOGIA Atualizado&quot; ## [73] &quot;0305-750X World Development A1 SOCIOLOGIA Atualizado&quot; A busca está funcionando! No entanto, vale lembrar, não conseguimos aqui delimitar nossa busca a colunas específicas. Poderíamos fazer com que buscássemos apenas na coluna com classificação. Há outros modos de importarmos estes dados e fazermos esta busca no R. Este foi um modo simples para treinarmos o regex no R. 5.3.4 Transformando strings em vetores com strsplit() É possível quebrar uma string em vários elementos de um vetor usando um padrão regex # nosso texto inicial rato_vetor &lt;- &quot;O rato roeu. Roeu a roupa. A roupa que era do rei. Qual rei? O rei de Roma!&quot; # Criando um vetor tendo como critério de quebra o ponto final, deve-se usar o símbolo de escape &quot;\\\\.&quot; strsplit(rato_vetor, &quot;\\\\.&quot;) ## [[1]] ## [1] &quot;O rato roeu&quot; &quot; Roeu a roupa&quot; ## [3] &quot; A roupa que era do rei&quot; &quot; Qual rei? O rei de Roma!&quot; # Vetores a partir da pontuação strsplit(rato_vetor, &quot;\\\\.|\\\\?&quot;) ## [[1]] ## [1] &quot;O rato roeu&quot; &quot; Roeu a roupa&quot; ## [3] &quot; A roupa que era do rei&quot; &quot; Qual rei&quot; ## [5] &quot; O rei de Roma!&quot; # Uma dica. Os elementos de vetores estão ficando com espaço em branco. Podemos resolver isso: strsplit(rato_vetor, &quot;(\\\\.|\\\\?) ?&quot;) ## [[1]] ## [1] &quot;O rato roeu&quot; &quot;Roeu a roupa&quot; &quot;A roupa que era do rei&quot; ## [4] &quot;Qual rei&quot; &quot;O rei de Roma!&quot; No pacote stringr o comando equivalente é o str_split(). Atenção: Ao tentar usar regex com espaços em branco (whitespace) e ele não funciona, é bom saber que há diversos tipos de espaços em branco. Há diferentes tipos de espaço em branco 5.3.5 Exemplo regex Voltando ao nosso exemplo anterior, podemos transformar as datas abaixo, que estão no formado dd/mm/aaaa (dia/mês/ano) para o formato aaaa-mm-dd (ano-mês-dia), para que possamos, por exemplo, ordenar as datas de modo fácil. Original Converter para 20/03/2020 “2020-03-20” 13/01/1990 “1990-01-13” 04/06/2001 “2001-06-04” datas &lt;- c(&#39;20/03/2020&#39;, &#39;13/01/1990&#39;, &#39;04/06/2001&#39;) gsub(&#39;([0-9]{2})/([0-9]{2})/([0-9]{4})&#39;,&#39;\\\\3-\\\\2-\\\\1&#39;, datas) ## [1] &quot;2020-03-20&quot; &quot;1990-01-13&quot; &quot;2001-06-04&quot; Vale lembrar, o R possui funções (como o strptime() ) e pacotes específicos para lidar com datas. O exemplo acima foi apenas um exercício. O R possui diversos pacotes para lidar com regex. As funções do base, que já vem com o R possuem diversas funções. Há ainda o pacote Google RE2 que é uma versão rápida desenvolvida pela Google. 5.4 Dicas/Sugestões: Regex no R Cheatsheet de expressões regulares no R (em inglês) RECOMENDADO “Regular Expressions as used in R”.Expressão regular no R. Documento oficial. (em inglês) Albert Y. Kim. Regular Expressions in R Regular Expressions with The R Language. Site dedicao às RegEx em várias linguagens de programação. Data Wrangling Cheatsheet em português, tradução de Augusto Queiroz de Macedo Regular expressions PENG, Roger. R Programming for Data Science. 2020. Bookdown online. J. Kyle Armstrong Fundamentals of Data Wrangling with R 2021. bookdown online "],["análise-textual-text-mining.html", "Capítulo 6 Análise Textual (text mining) 6.1 Ngrams", " Capítulo 6 Análise Textual (text mining) CAPÍTULO AINDA EM CONSTRUÇÃO Conteúdo planejado: Introdução à análise textual via computador Tipos de abordagens: bag of words, semantic parsing. Frequência de termos (bag of words, n-grams, skipgrams, TF-IDF) nuvem de palavras (wordclouds), Polarized tag cloud, pyramid plot. Correlação de palavras, tipos de distâncias, dendogramas parts-of-speech keyword extraction redes de palavras (word networks) Inteligência Artificial: clusterização; topic modelling Análise de sentimentos. A análise computacional de textos é praticamente um sinônimo de Mineração de texto (text mining) Há diversas funções nativas do R que usamos na mineração de texto/análise textual, mas também há diversos pacotes focados em análise textual com diversas ferramentas, como o quanteda (QUantitative ANalysis TExtual DAta), tm (Text Mining Package) e qdap (Quantitative Discourse Analysis Package). Estes são alguns dos mais famosos, com diversas ferramentas, mas há alguns outros pacotes focados em funções mais específicas, como o pacote wordcloud, por exemplo. O pacote quanteda acompanha outros, como o quanteda.textstats, quanteda.textplots e o quanteda.textmodels que aconselhamos instalar também. 6.0.1 Palavras em contexto (keyword-in-context) CAPÍTULO AINDA EM CONSTRUÇÃO Podemos ver como certas palavras são usadas em diversas frases no texto para ter uma ideia melhor do contexto em que aparecem. No quanteda, usamos a função kwic(Dados, pattern = \"padrão\"). 6.1 Ngrams 6.1.1 GoogleNgrams A Google pegou sua enorme base de dados dos milhares de livros do Google Books e extraiu os termos mais frequentes, e os colocou disponível para consulta no site Goolge Books Ngram Viewer. O Google Ngrams facilitou a busca por ngrams nesta base de dados, naquilo que chamavam de “culturonomics”. O nome não pegou, a ferramenta tem suas limitações, mas ainda assim pode ser bem útil. A base de dados possui 5.2 milhões de livros, cerca de 4% de todos os livros já publicados. Para mais informações sobre a base de dados e sobre o GoogleNgram no site. Tanto o Python (com o get-ngrams) como o R (ngramr) possuem pacotes que usam os dados do Google Ngram. Instalando o pacote install.packages(&#39;ngramr&#39;) Carregando os pacote library(ngramr) E um exemplo de uso library(ggplot2) ng &lt;- ngram(c(&quot;Max Weber&quot;, &quot;Émile Durkheim&quot;), year_start = 1890) ggplot(ng, aes(x=Year, y=Frequency, colour=Phrase)) + geom_line() Um exemplo com mais opções: ggram(c(&quot;monarchy&quot;, &quot;democracy&quot;), year_start = 1500, year_end = 2000, corpus = &quot;eng_gb_2012&quot;, ignore_case = TRUE, geom = &quot;area&quot;, geom_options = list(position = &quot;stack&quot;)) + labs(y = NULL) É possível mudar entre diferentes corpus, que neste caso representam as diferentes línguas, como “eng_us_2019”, “eng_gb_2019”, “chi_sim_2019”, “fre_2019”, “ger_2019”, “heb_2019”, “ger_2012”, “spa_2012”, “rus_2012”, “ita_2012”. Para ver todos os corpus disponíveis veja no site busque a sessão “Corpora”. Infelizmente, não há corpus em português no Google Ngram. Dicas Ngramr: PDF com a documentação do ngramr instalação/Primeiros passos com o Ngramr na página do Github do ngramr Dicas Analise Textual Julia Silge Learn tidytext with my new learnr course. Um curso interativo do pacote Tidytext. Textos sobre análise textual CASTELFRANCHI, Yurij. A análise de textos auxiliada pelo computador: um laboratório a céu aberto para as ciências sociais. Journal of Science Communication 16(02)(2017)C04 GRIMMER, Justin.STEWART, Brandom. Text as Data: The Promise and Pitfalls of Automatic ContentAnalysis Methods for Political Texts. Political Analysis(2013) pp. 1–31. doi:10.1093/pan/mps028. TREADWELL, Donald. Content Analysis: Understanding Text and Image in Numbers. Understanding Text and Image in Numbers. In __ Introducing Communication Research: paths of Inquiry. Sage. 2014. (Capítulo sobre análise de conteúdo) Link para diversos artigos de Gary King sobre Automated Text Analysis. Vídeos BROWN, Taylor W. Workshop on automated text analysis no Summer Institute in Computational Social Science na Universidade de Oxford em 2019. Em inglês, sem legendas, usando o pacote Quanteda. Parte 1 e Parte 2. O material da aula no Google Drive. "],["análise-de-redes-sociais.html", "Capítulo 7 Análise de Redes Sociais 7.1 O pacote Igraph 7.2 Os pacotes ggraph e tidygraph . Construindo grafos com o tidyverse. 7.3 Redes de palavras 7.4 Redes de citação 7.5 Gráfico de centralidade 7.6 Comunidades 7.7 Sugestões de links", " Capítulo 7 Análise de Redes Sociais O R possui diversos pacotes para análise de rede, como o igraph, statnet, e do tidyverse temos tydygraph e ggraph. 7.1 O pacote Igraph Instalando o pacote igraph: install.packages(&quot;igraph&quot;) # instalando o pacote chamando o pacote já instalado library(igraph) # chamando o pacote já instalado ## ## Attaching package: &#39;igraph&#39; ## The following objects are masked from &#39;package:dplyr&#39;: ## ## as_data_frame, groups, union ## The following objects are masked from &#39;package:purrr&#39;: ## ## compose, simplify ## The following object is masked from &#39;package:tidyr&#39;: ## ## crossing ## The following object is masked from &#39;package:tibble&#39;: ## ## as_data_frame ## The following objects are masked from &#39;package:lubridate&#39;: ## ## %--%, union ## The following objects are masked from &#39;package:stats&#39;: ## ## decompose, spectrum ## The following object is masked from &#39;package:base&#39;: ## ## union Pegando o famoso poema de Drummond: “João amava Teresa que amava Raimundo que amava Maria que amava Joaquim que amava Lili que não amava ninguém” E o transformando em um grafo (os “gráficos” em análise de rede recebem este nome): library(igraph) g &lt;- graph.empty(directed = TRUE) # &quot;directed&quot; implica distinguir entre &quot;de&quot; e &quot;para&quot; na relação entre os nós. # Adicionando os vértices. g &lt;- g + vertex(&quot;João&quot;) g &lt;- g + vertex(&quot;Teresa&quot;) g &lt;- g + vertex(&quot;Raimundo&quot;) g &lt;- g + vertex(&quot;Maria&quot;) g &lt;- g + vertex(&quot;Joaquim&quot;) g &lt;- g + vertex(&quot;Lili&quot;) # Especificando as relações entres os vértices, os edges g &lt;- g + edges(&quot;João&quot;, &quot;Teresa&quot;) g &lt;- g + edges(&quot;Teresa&quot;, &quot;Raimundo&quot;) g &lt;- g + edges(&quot;Raimundo&quot;, &quot;Maria&quot;) g &lt;- g + edges(&quot;Maria&quot;, &quot;Joaquim&quot;) g &lt;- g + edges(&quot;Joaquim&quot;, &quot;Lili&quot;) plot.igraph(g) # plotando o grafo Neste caso, poderíamos ter feito este mesmo grafo com código mais compacto: library(igraph) g &lt;- graph.empty(directed=TRUE) # Adicionando os vértices. g &lt;- g + vertex(c(&quot;João&quot;, &quot;Teresa&quot;, &quot;Raimundo&quot;, &quot;Maria&quot;, &quot;Joaquim&quot;, &quot;Lili&quot;)) # Adicionando os edges em pares g &lt;- g + edges(c(&quot;João&quot;, &quot;Teresa&quot;, &quot;Teresa&quot;, &quot;Raimundo&quot;, &quot;Raimundo&quot;, &quot;Maria&quot;,&quot;Maria&quot;, &quot;Joaquim&quot;,&quot;Joaquim&quot;, &quot;Lili&quot;)) plot.igraph(g) Se o grafo sobe ou desce, pouco importa para nós aqui, importa as pessoas e as relações entre elas. Repare que os edges são entendidos aos pares. Se fizéssemos um vetor sem as devidas repetições, teríamos um gráfico errado das relações: g &lt;- graph.empty(directed=TRUE) g &lt;- g + vertex(c(&quot;João&quot;, &quot;Teresa&quot;, &quot;Raimundo&quot;, &quot;Maria&quot;, &quot;Joaquim&quot;, &quot;Lili&quot;)) g &lt;- g + edges(c(&quot;João&quot;, &quot;Teresa&quot;, &quot;Raimundo&quot;, &quot;Maria&quot;,&quot;Joaquim&quot;, &quot;Lili&quot;)) plot.igraph(g) Ou com código modo mais econômico ainda: g &lt;- graph.formula( João --+ Teresa --+ Raimundo --+ Maria --+ Joaquim --+ Lili ) plot(g) 7.1.1 Clusterização EM CONSTRUÇÃO 7.2 Os pacotes ggraph e tidygraph . Construindo grafos com o tidyverse. O pacote ggraph é um pacote elaborado por Thomas Lin Pedersen, o mesmo do ggplot2, e pretende ser uma extensão deste, usando a mesma gramática de gráficos, o que nos dá grande flexibilidade visual. Com o ggraph é possível construir graficamente redes, mas ele vai além dos grafos, construindo também dendogramas, diferentes tipos de árvores, matrizes, gráficos hierárquicos, diagrama de arc, sunburst, etc. Para inserir os dados no ggraph, é necessário colocá-lo no formato “tidy” do “tidyverse”, e fazemos isso com o pacote tidygraph: Para instalar, usamos os comandos: install.packages(&#39;ggraph&#39;) install.packages(&#39;tidygraph&#39;) Carregando os pacotes library(ggraph) library(tidygraph) ## ## Attaching package: &#39;tidygraph&#39; ## The following object is masked from &#39;package:igraph&#39;: ## ## groups ## The following object is masked from &#39;package:stats&#39;: ## ## filter Seguindo o exemplo na página do criador do GGraph Thomas Lin Pedersen, vamos usar o dataset highschool do ggraph, que contém dados sobre a evolução da amizade entre garotos numa escola do ensino médio no Illinois, que nos anos de 1957 e 1958 responderam à pergunta: “Com que colegas desta escola você anda mais frequentemente?”. Esta pesquisa apareceu originalmente nos livros “Introduction to Mathematical Sociology” e “The Adolescent Society”, ambos do sociólogo James Coleman. Para obter mais informações sobre este dataset, basta digitar no console: help(highschool). str(highschool) # observando a estrutura do data frame ## &#39;data.frame&#39;: 506 obs. of 3 variables: ## $ from: num 1 1 1 1 1 2 2 3 3 4 ... ## $ to : num 14 15 21 54 55 21 22 9 15 5 ... ## $ year: num 1957 1957 1957 1957 1957 ... head(highschool,10) # observado as primeiras linhas do data frame ## from to year ## 1 1 14 1957 ## 2 1 15 1957 ## 3 1 21 1957 ## 4 1 54 1957 ## 5 1 55 1957 ## 6 2 21 1957 ## 7 2 22 1957 ## 8 3 9 1957 ## 9 3 15 1957 ## 10 4 5 1957 Vemos que há na coluna 1 de (“from”) pessoa número X para (“to”) para pessoa Y no ano (“year”). Assim, a pessoa 1 teve contato com as pessoas 14, 15, 21, 54 e 55 em 1957. A pessoa 2 teve contato com as pessoas 21 e 22, e assim por diante. Vamos preparar os dados para plotar o grafo com ggraph. Antes, é necessário colocá-lo no formato “tidy” do “tidyverse”, e fazemos isso com o pacote tidygraph. A função as_tbl_graph() do pacote tidygraph funciona como a função grouped_df(), que agrupa nós (nodes) e arestas (edges). as_tbl_graph(highschool) ## # A tbl_graph: 70 nodes and 506 edges ## # ## # A directed multigraph with 1 component ## # ## # Node Data: 70 x 1 (active) ## name ## &lt;chr&gt; ## 1 1 ## 2 2 ## 3 3 ## 4 4 ## 5 5 ## 6 6 ## # … with 64 more rows ## # ## # Edge Data: 506 x 3 ## from to year ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 13 1957 ## 2 1 14 1957 ## 3 1 20 1957 ## # … with 503 more rows Vamos criar uma lista chamada “grafo”, adicionando um campo novo com mutate(), de nome “Popularidade” e que vai medir a centralidade de grau através da função centrality_degree() do tidygraph. Centralidade de grau é a medida mais simples de centralidade, que conta o número de conexões, as arestas (“edges”) de cada nó. grafo &lt;- as_tbl_graph(highschool) %&gt;% mutate(Popularidade = centrality_degree(mode = &#39;in&#39;)) grafo ## # A tbl_graph: 70 nodes and 506 edges ## # ## # A directed multigraph with 1 component ## # ## # Node Data: 70 x 2 (active) ## name Popularidade ## &lt;chr&gt; &lt;dbl&gt; ## 1 1 2 ## 2 2 0 ## 3 3 0 ## 4 4 4 ## 5 5 5 ## 6 6 2 ## # … with 64 more rows ## # ## # Edge Data: 506 x 3 ## from to year ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 13 1957 ## 2 1 14 1957 ## 3 1 20 1957 ## # … with 503 more rows Plotando o grafo: ggraph(grafo, layout = &#39;kk&#39;) + geom_edge_fan(aes(colour = stat(index)), show.legend = FALSE) + geom_node_point(aes(size = Popularidade, colour= Popularidade), show.legend = TRUE) + scale_colour_gradient(low = &quot;steelblue&quot;, high = &quot;black&quot;) + facet_edges(~year) + theme_graph(foreground = &#39;steelblue&#39;, fg_text_colour = &#39;yellow&#39;) + labs(title = &quot;Evolução da amizade entre adolescentes de uma escola no Illinóis&quot;, caption = &quot;Fonte: Elaboração própria a partir dos dados de Coleman(1961) apud ggraph (v.2.0.5; Pedersen)&quot;) layout define como os nós serão alocados. O layout do ggraph possui os mesmos do igraph e outros mais como, hive plots, treemaps e circle packing. kk indica que está sendo usado o algortimo Kamada-Kawai para dispersar os nós e facilitar nossa visualização. geom_edge_fan() desenha os laços de modo curvo. Há diversas outras opções possíveis neste caso, como: geom_edge_arc, geom_edge_bend, geom_edge_diagonal,geom_edge_elbow, geom_edge_fan, geom_edge_hive, geom_edge_link, geom_edge_parallel. Substitua geom_edge_fan() por alguns estes e vjea a diferença no grafo. colour indica que a intensidade das ligações será por um gradiente de cor, stat() indica que segue ali uma informação estatística. geom_node_point() mostra os nós como pontos/círculos e permite que sejam plotados em diferentes tamanhos, cores e formas em aes(). size = Popularidade indica que o tamanho dos nós é controlado pela variável “Popularidade” que criamos. colour = Popularidade indica que além do tamanho, a cor também vaira conforme a popularidade. Para mais opções, digite ?geom_node_point() no console ou consulte a documentação do ggraph. scale_colour_gradient(low = \"steelblue\", high = \"black\") é opcional, podendo ser retirada. Especifica o gradiente de cores do comando anterior (no caso, nós), qual o valor mais baixo até o mais elevado. Sua informação é redundante com relação ao tamanho dos nós, mas ajuda a reforçar tal informação. facet_edges() função de “faceting”, de criar facetas, gráficos multiplos, e o símbolo de til ~ seguido de year indica que o critério aqui são as categorias dentro de ano, que no caso, são 1957 e 1958. Os nós são repetidos em cada painel. Caso tivéssesmos usado facet_edges(1957) teríamos o grafo apenas do ano 1957. theme_graph especifica as cores das legendas nas facetas labs(title = \" indica o título e caption o rodapé. Veja também cheatsheet/folha de dicas do ggraph Para ver os diferentes layouts possíves do ggraph e seus respectivos códigos, para além dos grafos clique aqui CAPÍTULO A SER EXPANDIDO 7.3 Redes de palavras EM CONSTRUÇÃO 7.4 Redes de citação EM CONSTRUÇÃO 7.5 Gráfico de centralidade EM CONSTRUÇÃO 7.6 Comunidades EM CONSTRUÇÃO 7.7 Sugestões de links AQUINO, Jackson A. “Análise de redes sociais”, capítulo 12 de ___. R para cientistas sociais. Ilhéus, BA: EDITUS, 2014. 157 p. ISBN: 978-85-7455-369-6. (PDF gratuito bem introdutório com R) HIGGINS, Silvio S.; RIBEIRO, Antônio Carlos. Análise de redes em Ciências Sociais. Brasília: ENAP. 2018. 229p. (PDF Gratuito de livro. Bom para aprender os conceitos/teorias básicos de análise de rede). Static and dynamic network visualization with R Manual online do ipgraph para R; PDF do Manual do igraph para R (ambos em inglês) ggraph Documentation no Cran. d’ANDRÉA, Carlos Frederico de Brito. Pesquisando plataformas online: conceitos e métodos. EDUFBA. 2020. A obra visa introduzir os Estudos de Plataforma, um campo de estudos que, desde o início da década de 2010, discute as especificidades políticas e materiais das mídias sociais e de outras plataformas online. Datificação, algoritmos, governança e os modelos de negócio das plataformas são algumas das dimensões sintetizadas no livro. De modo didático, o autor apresenta um conjunto de leituras e de experimentações metodológicas conduzidas com um diversificado grupo de colaboradoras(es) no país e no exterior. (ebook PDF e Epub gratuitos) RECUERO, Raquel. Introdução à análise de redes sociais online. EDUFBA.2017. A Análise de Redes Sociais (ARS) é uma abordagem de pesquisa cuja popularidade tem aumentado nos últimos anos, principalmente, entre os pesquisadores da área de Comunicação. É nesse âmbito que várias obras, entre artigos e livros, vêm surgindo e introduzindo o estudo dessas estruturas a partir da análise de redes e da compreensão da representação dessas redes sociais na internet. Este livro é uma pequena compilação dos principais conceitos e elementos para a compreensão e a aplicação da ARS. É baseado em uma breve apresentação e histórico do paradigma, os principais conceitos, suas métricas e, finalmente, suas formas de representação e visualização. (ebook PDF e Epub gratuitos). LIZARDO Omar; JILBERT Isaac. Social Networks: An Introduction. 2021. (ebook online) "],["links-úteis.html", "Capítulo 8 Links úteis 8.1 Humanidades digitais 8.2 Análise de redes sociais 8.3 Textos sobre análise textual 8.4 Processamento Linguagem Natural (PLN ou NLP) 8.5 Revistas / Journals acadêmicos 8.6 Dados Abertos 8.7 Vídeos 8.8 Sites / Blogs 8.9 Organizações 8.10 Links de Podcasts sobre Digital Humanities 8.11 Links Programação 8.12 Links de Cursos Online 8.13 Grupos de discussão/Forum 8.14 Links de folhas de dicas (Cheat-sheets)", " Capítulo 8 Links úteis Segue aqui uma lista com links para livros, artigos, sites, manutais, tutoriais, vídeos e canais de vídeos. A maioria e a prioridade é de material gratuito, sobre temas gerais relacionados a humanidades digitais e em especial, à sociologia. Há também sugestões mais focadas nos assuntos específicos nos capítulos. O critério de seleção aqui foi o de contribuição para esclarecer o potencial das humanidades digitais para quem inicia na área de humanidades digitais/sociologia digital. Os itens não foram selecionados tanto pela contribuição científica, mas com base no critério de exemplos que podem ajudar a esclarecer o potencial das humanidades digitais para quem inicia. 8.1 Humanidades digitais Introduction to Digital Humanities Textbook A digital textbook designed for UCLA’s Introduction to Digital Humanities course website. (ebook disponível gratuito online) EDMOND, Jennifer Digital Technology and the Practices of Humanities Research. (Livro gratuito) The Data Journalism Handbook. Towards a Critical Data Practice. 415p. 2021. ISBN 9789048542079. (Livro GRATUITO). BAUER, Paul C. Computational Social Science: Theory &amp; Application. (livro online grátis). 8.2 Análise de redes sociais ver - Ver sugestões Ver sugestões no capítulo “Análise de Redes Sociais”. 8.3 Textos sobre análise textual GRIMMER, Justin.STEWART, Brandom. Text as Data: The Promise and Pitfalls of Automatic ContentAnalysis Methods for Political Texts. Political Analysis(2013) pp. 1–31. doi:10.1093/pan/mps028. CASTELFRANCHI, Yurij. A análise de textos auxiliada pelo computador: um laboratório a céu aberto para as ciências sociais. Journal of Science Communication 16(02)(2017)C04 TREADWELL, Donald. Content Analysis: Understanding Text and Image in Numbers. Understanding Text and Image in Numbers. In __ Introducing Communication Research: paths of Inquiry. Sage. 2014. (Capítulo sobre análise de conteúdo) 8.3.1 Sociologia Digital / Sociologia Computacional KATEMBERA, Serge. “Sociologia digital ou sociologia do digital?” V. 2 N. 1 (2020): Dossiê Ambiente e Sociedade. (Artigo) NASCIMENTO, Leonardo F. Sociologia digital : uma breve introdução. EDUFBA. 2020. (Livro gratuito em PDF e epub) Podcast “New Work in Digital Humanities”. Episódio: Neil Selwyn, “What is Digital Sociology?” (Polity, 2019) BERNAU, John A. Text Analysis with JSTOR Archives. November 12, 2018 https://doi.org/10.1177/2378023118809264. (Aplicação de métodos de humanidades digitais, usando a base do Jstor, para uma rápida análise de variação temática em revistas sociológicas). FUSSEY, Pete e ROTH, Silke (ed.) Digitizing Sociology. edição da revista “Sociology” da British Sociological Association. Evans J, Foster JG. Computation and the Sociological Imagination. Contexts. 2019;18(4):10-15. doi:10.1177/1536504219883850 8.3.1.1 Netnografia, etnografia digital MARKHAM, Annette N. 2013. Fieldwork in Social Media. Qualitative Communication Research 2, 4 (Dec. 2013), 434–446. https://doi.org/10.1525/qcr.2013.2.4.434 SHAFFER, David Williamson. Quantitative Etnography. Boswell Press. 2017. “This is a book about understanding why, in the digital age, the old distinctions between qualitative and quantitative research methods, between the sciences and humanities, and between numbers and understanding, limit the kinds of questions we can ask, in some cases, and lead us accept superficial answers in others. Quantitative Ethnography is a research method that goes beyond those distinctions to help us understand how to make sense of our increasingly data-rich world…”. (PDF da Introdução disponível gratuitamente) 8.4 Processamento Linguagem Natural (PLN ou NLP) JURAFSKY, D.; MARTIN, J. Speech and language processing: An introduction to speech recognition, computational linguistics and natural language processing. Upper Saddle River, NJ: Prentice Hall, 2020. link pdf dos capítulos individuais, link livro completo. (Um manual bastante extenso e mais teórico sobre PLN) BIRD, S.; KLEIN, E.; LOPER., E. Natural language processing with Python – analyzing text with the natural language toolkit. (Livro online gratuito, baseado em Python 3.) Playlist no Youtube das aulas da Stanford University sobre PLN 8.5 Revistas / Journals acadêmicos SIGCAS Computers and Society da Association for Computing Machinery Digital Scholarship in the Humanities. Revista da Universidade de Oxford. Sessão com artigos gratuitos Journal: Digital humanities Quartely International Journal of Digital Humanities Socius: Sociological Research for a Dynamic World. Special Collection: Data Visualization. Jornal de acesso livre International Journal of Humanities and Arts Computing (Universidade de Edimburgo) magazén International Journal for Digital and Public Humanities. (open access). Revista do Dipartimento di Studi Umanistici da Università Ca’ Foscari de Veneza Journal of Digital Social Research. Open Access Reviews in Digital Humanities. (O Review já não publica desde 2014) Journal of Digital Humanities. (O Journal of DH tem uma sessão de resenha de novas ferramentas disponíveis). Journal of Cultural Analytics. Department of Languages, Literatures, and Cultures at McGill University, Canada. open-access journal dedicated to the computational study of culture 8.6 Dados Abertos Episódio 005 do Podcast “História FM Acesso à informação: como fazer uso da Lei de Acesso? com Maria Vitória Ramos e Luiz Fernando Toledo, do projeto Fiquem Sabendo. Ou ainda no Spotify. 5 estrelas dos dados abertos (Tim Berners-Lee, criador do termo “dados abertos”. Site explica o que são dados abertos e seus 5 níveis. Em português) Busca do Google por base de dados Abertos Dados abertos. (Se não sabe onde encontrar algum dado específico que você procura, veja aqui) SHIKIDA, Claudio D., MONASTERIO, Leonardo, NER, Pedro Fernando. Guia Brasileiro de Análise de Dados: Armadilhas e Soluções. Brasília. 2021. ( Tópicos: Causalidade, Pobreza e Desigualdade, Análise de dados em Saúde, Educação, Crimes e Violência, Macroeconomia, Mercado e Trabalho e Opinião Pública.) Dados Abertos: Fórum de discussão. Workshop do Henrique Xavier, no canal “Base dos Dados” sobre como explorar os dados do Diário Oficial da União. Como usar a biblioteca basedosdados no R. Pacote basedosdados do R para baixar diversas bases de dados. Data sets do Brasil.io, um “repositório de dados públicos disponibilizados em formato acessível” como “Eleições Brasil”, “Cursos e notas de corte do Prouni 2018”, gastos de deputados e do governo federal, salários de magistrados, etc. 8.7 Vídeos Vídeos do 1º Summerschool of Digital Humanities da Universidade de Heidelberg, Alemanha, ocorrido em 2017. link. (Em inglês) 8.8 Sites / Blogs Text analysis info Textual Analysis - University of Notre Dame Site Digital Humanities Now. (Agrega oportunidade de emprego, notícias, bolsas de pesquisa). site Go Digital Humanities. (Novidades sobre humanidades digitais, como eventos). Blog Digital Society Blog. (Blog do Institut für Internet und Gesellschaft da Alexander von Humboldt). Site The Programming Historian, com diverso conteúdos em português. 8.9 Organizações Alliance of Digital Humanities Organizations (ADHO). Global Network of Internet and Society Research Centers (NoC) i. (Catálogo com grupos de pesquisa sobre internet e sociedade ao redor do mundo). 8.10 Links de Podcasts sobre Digital Humanities Complexity: Peter Dodds on Text-Based Timeline Analysis &amp; New Instruments for The Science of Stories New Work in Digital Humanities. New Books Network. Interviews with digital humanists about their new work. Lista com mais podcasts dedicados às digital humanities 8.11 Links Programação 8.11.1 R introdução LENTE, Caio. Zen do R. “O objetivo deste livro é ensinar ao leitor que não costuma programar algumas formas simples de melhorar a organização de seus projetos de análise de dados em R”. (Livro online gratuito). AQUINO, Jackson A.. R para cientistas sociais. Ilhéus, BA: EDITUS, 2014. 157 p. ISBN: 978-85-7455-369-6. (PDF gratuito) FERREIRA, E.B.;. de OLIVEIRA, M.S. Introdução à Estatística com R. Unifal. 2020. (PDF gratuito). Livro Curso-R (em construção) Blog: Curso-R KUBRUSLY, Jessica. Uma Introdução à Programação com o R. (ebook online em português) FREIRE, Sergio Miranda. Introdução ao R. (Livro online, no formato bookdown) WICKHAM, Hadley. Advanced R. 2nd edition. (Livro online gratuito, no formato bookdown, em inglês, para entender melhor os conceitos do R). 8.11.2 R - tópicos específicos WICKHAM, H. GROLEMUND, Garret. R for Data Science. O’Reilly. 2017. (Livro Online, em inglês). IRIZARRY, Rafael A. Introduction to Data Science. Livro Online, em inglês, feito como notas de aulas da HarvardX Data Science Series CLARK, Michael. R &amp; Social Science. Getting Started with applied use of R in the Social Sciences. (pequeno manual em PDF da Universidade de Notre Dame) ggplot2 on-line version of work-in-progress 3rd edition of “ggplot2: elegant graphics for data analysis” SILGE, Julia, ROBINSON, David Text Mining with R. (ebook online) RYDBERG-COX, Jeff. Statistical Methods for Studying Literature Using R da Universidade Missouri-Kansas City. R-tutor- An R Introduction to Statistics. Site com tutoriais diversos sobre R. Em Inglês. Diversos links para livros gratuitos de R no bookdown. 8.11.3 Links Python HEINOLD, Brian. A Practical Introduction to Python Programming. 2012. 263 p. (PDF de introdução ao Python) Jake VanderPlas. Python Data Science Handbook: Essential Tools for Working with data. (livro online). 8.11.4 Python - tópicos específicos KARSDORP, Folgert. Python Programming for the Humanities. Livro online. Pyhton Humanities. Site com introdução às DH em Python, com alguns tutoriais. Constellate What do you want to learn today?. (Projeto Constellate da JStor junto a diversas universidades. Conta com sessão com tutoriais sobre análise textual, a maioria em Python. 8.12 Links de Cursos Online EdX, Cursos online de Universidades como Harvard, MIT, etc. gratuitos como ouvinte. Paga-se pelo certificado. Há cursos grautuitos, como “Introducing Text Analytics and Natural Language Processing with Python”, “Introduction to Digital Humanities”, “Data Science: Visualization”, “Using Python for Research”. DataCamp. Cursos mais práticos, em R, Python, SQL, com exercícios, totalmente online (não é necessário instalar nada em seu computador). As partes iniciais dos cursos costumam ser gratuitas, mas há parte paga com anuidades. Udemy Possui cursos gratuitos e cursos pagos. É possível encontrar cursos pagos por volta de R$20,00. Coursera. Possui parceria com mais de 200 universidades e empresas como o Google e IBM. Cognitive Class Da IBM, Cursos gratuitos em data science, alguns gratuitos com certificado. Cursos em inglês e espanhol. Possui cursos como “Data Visualization with R”. Big Data University. Cursos em Portugues. Curso gratuito de Estatística e probabilidade da Khan Academy, em português. Inclui teste de hipótese, regressão. 8.13 Grupos de discussão/Forum 8.13.1 Telegram Processamento de Linguagem Natural em Português PT-Br Data Science - Python R Brasil R humanidades Análise Textual-Humanidades Digitais 8.14 Links de folhas de dicas (Cheat-sheets) “Cheat sheets” seriam a tradução para “cola”, aquela feita para consulta em exames na escola. Em programação, refere-se a uma tabela muito bem resumida, com o que há de essencial em determinado assunto. Text Analysis Glossary Um glossário de termos usados na análise textual, em inglês, do projeto Constellate (JStor e diversas universidades) Lista com mais de 100 cheat-sheets em R e Python, sobre Machine Learning: link Git-GitHub, Git-Gitlab, Cheatsheets do R como: Base R, Base R em portguês, Data-table em português, Data Import, Data e horários com Lubridate, trabalhe com mais facilidade com listas e funções com o purr, stringr, dplyr, R Reference Card, Usar Python no R com reticulate, ggplot, R Markdown com o R Markdown Cheatsheet ou o Reference Guide, Regular Expressions, R Programming por Arianne Colton e Sean Chen, Uma lista com estes e outros cheatsheets aqui do Rstudio aqui. "]]
