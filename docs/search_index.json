[["objetivos-deste-manual.html", "Introdução à Análise Textual aplicada à Sociologia Objetivos deste manual Plano do livro", " Introdução à Análise Textual aplicada à Sociologia Alisson Soares Primeira versão em 22 de abril de 2021. Última atualização em 18 maio de 2021 Objetivos deste manual Sou Alisson Soares, sociólogo, e comecei há pouco tempo no campo das humanidades digitais. Achei por bem montar este manual como uma forma de compartilhar o que venho aprendendo. Algumas ferramentas, softwares serão de modo gráfico, mas por vezes vamos recorrer à programação. A principal linguagem aqui será o R, mas o uso de Python não está completamente excluído. O objetivo deste manual é: Ser um livro em progresso constante. Ser introdutório, mas também sempre indicar bons materiais para maior aprofundamento dos temas tratados. Sempre que possível, oferecer material em português, apesar de nem sempre isto ser possível. Oferecer uma introdução às ferramentas computacionais utilizadas em pesquisa das ciências sociais. Usar preferencialmente ferramentas gratuitas e de código aberto. Sempre que possível, apresentar exemplos de pesquisa e ciência social utilizando estas ferramentas. Plano do livro Os seguintes capítulos serão incorporados futuramente e estão em elaboração: Rápida introdução à programação (conceitos básicos como variável, loops, condicionais if-else, função, etc.) Normalização e expressões regulares. Rápida introdução ao R (tipos de dados, estrutura de dados, importação de dados, ambientes virtuais, suíte tidyverse) Introdução à análise textual via computador Frequência de termos (bag of words, n-grams, skipgrams, TF-IDF) dendogramas Correlação, tipos de distâncias parts-of-speech keyword extraction Introdução à análise de redes. Inteligência Artificial: clusterização; topic modelling Análise de sentimentos. Glossário de termos "],["intro.html", "Introdução O termo “humanidade digitais”", " Introdução “the best digital theory-building of the past decade stems from social and computational origins. As such, it is increasingly apparent that digital sociologists need to develop a computational as well as a sociological imagination.” O termo “humanidade digitais” Segundo um estudo do LinkedIn em 15 países, prevendo 150 milhões de vagas nos próximos 5 anos, das áreas de trabalhos digitais em alta e 2021, duas se relacionam a Humanidades Digitais: Big data Analytics (segundo lugar) e “Text, image and voice processing” (6º lugar). Profissionalmente, pode ser interessante entrar nesta área. Existem diversos termos correlatos a “Humanidades digitais” (“Digital humanities”), como “E-humanities,”Cultural Analytics“,”ciências sociais computacionais“,”sociologia digital“,”história digital“,”etnografia digital“,”etnografia quantitativa“,”Análise cultural quantitativa\" (quantitative cultural analysis) “cultural analytics”, “humanities data science”, “humanities data analysis” “humanities computing”, “distant reading”, “computational social science”, etc. Embora estes termos não sejam todos sinônimos entre si, há diversos pontos de contato. Em quase todos eles há a junção de algo de humanidades e algo de digital, demonstrando seu caráter transdisciplinar. A área possui contribuições de diversas áreas, como das ciências da informação, linguística, ciências da computação, história, sociologia, comunicação, etc. O termo é controverso mesmo entre os participantes, uma vez que é amplo demais. Uma definição ampla define o termo como intersecção entre humanidades e computação. Porém, uma busca no Google por um texto já configura uso de ferramenta digital. Outra definição mais restrita envolveria aprender a pensar como o computador e a partir disto extrair diversas novas possibilidades. Uma introdução aos diferentes significados do termo pode ser vista no artigo “Humanidades digitais” do grupo de pesquisas da USP. Em termos mais práticos, Humanidades digitais inclui a análise de: sons/música, imagens. textos. Nosso objetivo aqui Há todo um repertório de ferramentas comuns utilizados tanto por cientistas de dados como por humanistas digitais, com diferentes fins. Nosso foco será aprender diversas destas ferramentas de uso comum, mas focando em seu uso na sociologia. "],["história-da-análise-textual.html", "Capítulo 1 História da Análise Textual 1.1 Linha do tempo da história da Análise Textual", " Capítulo 1 História da Análise Textual “May we hope that when things come to such a crisis, human labor of the literary sort may be in part superseded by machinery? Machinery has done wonders, and when we think of what literature is becoming, it is certainly to be wished that we could read it by machinery, and by machinery digest it” (Andrew Stauffer In London’s Daily News. 15 de Setembro de 1869 apud Catherine DeRose.) A análise textual abarca campos do conhecimento bem variados, como psicologia, ciências da computação, ciências da informação, linguística, ciência política, sociologia, etc. Apresentamos aqui uma linha do tempo com alguns dos principais eventos relacionados à análise textual, bem como ao seu uso com computadores. De modo resumido, podemos pensar nos primeiros desenvolvimentos ao final do século XIX, a introdução do computador e mais recentemente, a introdução da inteligência artificial como pontos marcantes nesta cronologia. Fizemos aqui uma breve cronologia, que não pretende ser extensiva, com todos eventos importantes, mas apenas demarcar alguns pontos interessantes, para ajudar a dar alguma ideia àqueles que iniciam no campo das humanidades digitais e na sociologia digital. 1.1 Linha do tempo da história da Análise Textual 1887 Medenhall. Analisa o comprimento de palavras: MENDENHALL, T. C. . The characteristic curves of composition. Science.Vol ns-9, Issue 214S. 11 March 1887. baixar pdf Wincenty Lutoslawski. Análise de palavras raras na obra de Platão. Cunhou o termo “estilometria” Lutoslawski’s Origin and Growth of Plato’s Logic - The Origin and Growth of Plato’s Logic, 1888 Benjamin Bourdon (1860-1943, psicólogo e professor da Université de Rennes): Ao pesquisar sobre a expressão de emoções através de palavras, analisou o livro “Exodus” da Bíblia e calculou frequências, classificou e eliminou as stopwords. “In 1888, in a research on the expression of emotions through words, Benjamin Bourdon analysed the Exodus of the Bible and calculated the frequencies by rearranging and classifying them, eliminating the stop words” fonte. 1888 Friedrich Kaeding (1855 - 1929), cria índices de frequência para estruturação de sistemas estenográficos (sistema de escrita por abreviações para que a escrita seja tão rápida como a fala). 1934 Harold Laswell (1902-1978, cientista político) produz a primeira contagem de palavras chave. 1934 Vygostky produz a primeira análise quantitativa de narrativa 1949 Robert Busa (padre jesuíta) junto à IBM com o projeto Index Thomisticus, que levou 34 anos, envolveu cerca de 70 pessoas, sobre as obras de São Tomás de Aquino. “The IBM… considered this first enterprise of using a computer for linguistic and lexicographic goals as a pilot-project” fonte, indexando mais de 10 milhões de palavras. Busa, R. (1980). “The Annals of Humanities Computing: The Index Thomisticus”. Computers and the Humanities. 14 (2): 83–90. doi:10.1007/BF02403798. ISSN 0010-4817. ROCKWELL, Geoffrey; PASSAROTTI, Marco (2019-05-27). “The Index Thomisticus as a Big Data Project”. Umanistica Digitale (5). doi:10.6092/issn.2532-8816/8575. 1950 Gottschalk usa Content Analysis para rastrear temas freudianos. GOTTSCHALK, Louis A. The Measurement of Psychological States Through the Content Analysis of Verbal Behavior. University of California Press. 1969. 317p. Gottschalk-Gleser Content Analysis Method of Measuring the Magnitude of Psychological Dimensions 1950 Alan Turing aplica Inteligência Artificial a textos. 1952 Bereleson publica o primeiro manual de análise de conteúdo. BERELSON, B. (1952). Content analysis in communication research. New York: Hafner. 1954 Primeira tradução automática de texto (Georgetown–IBM experiment) do russo ao inglês. HUTCHINS, John. The first public demonstration of machine translation: the Georgetown-IBM system, 7th January 1954. 2006. Press Release da IBM 1963 Mosteller e Wallace analisam a autoria dos Federalist Papers. MOSTELLER, F., WALLACE, D. L.. 1963. Inference in an authorship problem. Journal of the American Statistical Association 58:275–309. 1965 Tomashevsky formaliza análise quantitativa de narrativa. TOMASHEVSKY, B. (1965). Thematics. In L T. Lemon &amp; M. I. Reis (Eds. &amp; Trans.), Russian formalist criticism: Four essays (pp. 61-95). Lincoln: University of Nebraska Press. (Original de 1925) 1966 Stones e Bales usam computador para medir propriedades psicométricas de textos na RAND. 1980 Declínio do formalismo chomskyano; nascimento do Processamento de Linguagem Natural (PLN). 1980 Aplicação de Aprendizado de máquinas (Machine Learning) ao Processamento de Linguagem Natural 1981 Walter Weintraub e a contagem de parts-of-speech. WEINTRAUB, Walter. Verbal Behavior: adaptation and psychopathology. Springer:NY. 1981. SOBEL, Dava. Language patterns reveal problems in personality. NYT. Oct. 27,1981. 1985 Schrodt introduz codificação automática de eventos (Automated Event Coding). SCHRODT, Philip A. Automated Coding Of International Event Data Using Sparse Parsing Techniques. 2000. 1986 James W. Pennebaker desenvolve LIWC (Linguistic Inquiry and Word Count). 1989 Roberto Franzosi (perfil no Research Gate) (sociólogo) traz a análise de narrativa quantitativa (quantitative Narrative Analysis) para as Ciências Sociais. 1998 Primeiro desenvolvimento de Topic Models. 1998 John W Mohr conduz a primeira análise quantitativa de visões de mundo. 1999 Peter Bearman (sociólogo) et al. aplicam métodos de rede a narrativas “Narrative network”. 2001 David M. Blei et al desenvolvem a LDA (Latent Dirichlet Allocation). David M. Blei, Andrew Y. Ng, Michael I. Jordan. Latent Dirichlet Allocation. Journal of Machine Learning Research 3 (2003) 993-1022. github Blei Lab. 2003 MALLET (MAchine Learning for LanguagE Toolkit), um dos primeiros sistemas de topic models, é criado. “MALLET is a Java-based package for statistical natural language processing, document classification, clustering, topic modeling, information extraction, and other machine learning applications to text. MALLET includes sophisticated tools for document classification: efficient routines for converting text to”features“, a wide variety of algorithms (including Naïve Bayes, Maximum Entropy, and Decision Trees), and code for evaluating classifier performance using several commonly used metrics.” 2005 Quin et al analisam discursos políticos usando topic models. Kevin M Quinn, Burt L Monroe, Michael Colaresi,Michael H Crespin, and Dragomir R Radev. 2010. How to analyze political attention with minimal assumptions and costs. American Journal of Political Science54(1):209–228 2010 Gary King e Daniel Hopkins trazem Topic Models ao mainstream. Hopkins, Daniel, and Gary King. 2010. “A Method of Auto-mated Nonparametric Content Analysis for Social Science.”American Journal of Political Science54(1): 229–47. Fonte: Versão ampliada, baseado parcialmente em: “SICSS 2018 - History of Quantitative Text Analysis” slides, video. Pretende-se posteriormente expandir esta seção, explicando em mais detalhes alguns dos exemplos acima "],["exemplos-de-pesquisas-em-humanidades-digitais.html", "Capítulo 2 Exemplos de Pesquisas em Humanidades Digitais 2.1 Bibliometria / cientometria / cienciometria 2.2 Exemplo: Google Trends como Proxy para epidemias 2.3 Exemplo: Como as fake news sobre a pandemia de Covid-19 se assemelham/divergem entre os países? 2.4 Exemplo: Mudança de significado de palavras 2.5 Exemplo: Análise de complexidade musical 2.6 Exemplo: Polarização 2.7 Exemplo de integração quali-quanti: Complementando dados qualitativos 2.8 Exemplo de integração quali-quanti: Depurando dados quantitativos", " Capítulo 2 Exemplos de Pesquisas em Humanidades Digitais Para entender os potenciais das humanidades digitais para pesquisa, nada melhor que observarmos exemplos de pesquisas. Aqui seguem alguns de exemplos, selecionados pelo potencial de integração entre humanidades e métodos digitais, mais que quanto ao possível mérito/demérito científico. 2.1 Bibliometria / cientometria / cienciometria O estudo das citações, das rede de citações em artigos científicos foi talvez um dos pioneiros no uso de algumas das técnicas aqui descritas, existindo já há décadas. Chamada de “bibliometria”, “cientometria” ou “cienciometria”, ela conta citações de determinados autores em artigos científicos e tenta avaliar o quão influentes estes são. 2.1.1 Exemplo: Filósofos da ciência na Sociologia Por exemplo, HEDSTRÖM et al (1998), buscando saber a influência dos principais filósofos da ciência (Hempel, Kuhn, Popper e Wittgenstein) na sociologia em diferentes países e regiões (países nórdicos, EUA, Grã Bretanha, Alemanha e França), analisou o número de artigos nas principais revistas sociológicas que os citaram. (Fonte: HEDSTRÖM et al. 1998. p. 343) Pelos dados ali apresentados, Popper seria o filósofo mais influente na Europa, principalmente nos países de língua alemã, ao passo que Kuhn seria mais dominante nos EUA. HEDSTRÖM, Peter; SWEDBERG; and UDÉHN, Lars. Popper’s Situational Analysis and Contemporary Sociology. Philosophy of the Social Sciences 1998; 28; 339-64] 2.1.2 Exemplo: A teoria dos sistemas sociais de Niklas Luhmann Stephen Roth analisou a chamada diferenciação funcional dos subsistemas da sociedade mundial, isto é, como os subssitemas como política, economia, religião, ciência, direito, meios de comunicação de massa, etc. se autonomizam em relação aos outros, entre os anos de 1800 e 2000, e para tal utilizou dados do Google Ngram viewer, que por sua vez se baseia no Google Books (mais detalhes sobre estas ferramentes na seção sobre frequência de palavras). Ele encontrou, por exemplo, declínio da presença relativa (isto é, proporcional a cada ano) da palavra “Deus” (god) nos livros em inglês ao longo do tempo. Fonte: Roth (2014, p.46). E se examinarmos os termos relacionados aos Meios de Comunicação de Massa, podemos ver a importância relativa do termo “imprensa” (press) aumentando ao longo do tempo. Fonte: Roth(2014, p.47). Roth, Steffen. 2014. “Fashionable Functions: A Google Ngram View of Trends in Functional Differentiation (1800-2000)” International Journal of Technology and Human Interaction, 34–58. Uma versão posterior, um pouco mais elaborada em: Steffen Roth, Carlton Clark, Nikolay Trofimov, Artur Mkrtichyan, Markus Heidingsfelder, Laura Appignanesi, Miguel Pérez-Valls, Jan Berkel, Jari Kaivo-oja. Futures of a distributed memory. A global brain wave measurement (1800–2000). Technological Forecasting &amp; Social Change 118 (2017) 307–323 2.1.3 Exemplo: Tendência de termos chave da Sociologia Além de contar citação nas referências, pode-se contar as palavras mais frequentes no corpo do texto e compará-las. Bernau (2018), por exemplo, coletou dados do JSTOR’s Data for Research e plotou um gráfico longitudinal (ao longo do tempo) de frequência de palavas com termos chaves da sociologia, como “classe”, “raça” e “gênero” da revista American Sociological Review. Ele também disponibilizou o script em R que desenvolveu para esta análise. BERNAU, John A. Text Analysis with JSTOR Archives. November 12, 2018. https://doi.org/10.1177/2378023118809264 2.1.4 Exemplo: Tendências da filosofia Inspirado neste trabalho, Brian Weatherson usou também o pacote jstor_dfr, baixou dados da filosofia, e os clusterizou através de topic modeling. Podemos ver as tendências gerais de vários ramos da filosofia ao longo do tempo: Tendencias na história da filosofia O resultado e mais informações estão em seu livro online: WEATHERSON, Brian . A History of Philosophy Journals. Volume 1: Evidence from Topic Modeling, 1876-2013) 2.2 Exemplo: Google Trends como Proxy para epidemias A busca no Google por certos sintomas de doenças, ou melhor, a variação na busca por certas doenças e sintomas correlatos pode indicar que variação real da doença. Isso acontece, por exemplo, com sintomas de gripe. Um pico no aumento das buscas pelos sintomas indica um prenúncio do aumento das infecções, a ser checado/validado posteriormente. No artigo Google Trends: A Web-Based Tool for Real-Time Surveillance of Disease Outbreaks., os autores explicam que a ferramenta lançada em 2018: &gt; “Google Flu Trends can detect regional outbreaks of influenza 7–10 days before conventional Centers for Disease Control and Prevention surveillance systems” Para funcionar, há certas pré-condições sociais. Vale para gripe, e a ferramenta também prevê aumentos da Covid-19. Na reportagem da Piaui “No carnaval, buscas por “sintomas covid” voltaram a subir; sete dias depois, número de novos casos bateu recorde” de 09 de março de 2021 compara as buscas no google com casos reais. Gráfico: buscas por “sintomas Covid no Google” versus casos reais ver também “Sintomas Covid” en Google trends:.Un indicador alternativo para el seguimiento de la incidencia de casos. com exemplos da Espanha, México, Chile e Argentina. No entanto, a ferramenta que parecia promissora falhou em prever o pico de gripe de 2013, sobrestimando por 140%. A empresa achou melhor terminar o projeto, conforme um artigo da Wired de 2015. Mais detalhes podem lidos no artigo The Parable of Google Flu: Traps in Big Data Analysis. O Instituto de Ciências Cognitvas de Osnabrück leva a ideia adiante, com modelo mais complexo, utilizando dados de redes sociais como Twitter e através do Watson da IBM.(site do projeto). 2.3 Exemplo: Como as fake news sobre a pandemia de Covid-19 se assemelham/divergem entre os países? O relatório de 23/11/2020 sobre isolamento científico, intitulado “scientific [self] isolation” do Laut, Centro de Análise da Liberdade e do Autoritarismo, cruzou as checagens de fake news de 129 países diferentes (há uma plataforma que traduz as reportagens de fact checking de todo mundo para o inglês) e investigou a distribuição de notícias falsas sobre os tratamentos da Covid19. Os pesquisadores procederam então uma distribuição num plano das discussões nos países conforme sua semelhança. Quanto mais próximos, mais semelhantes os debates ao redor do tema. Encontraram então que o Brasil é o país mais isolado em sua discussão envolvendo certos medicamentos, no canto superior direito. grafico 2.4 Exemplo: Mudança de significado de palavras Kulkarni et al (2015) mostraram como através de ferramentas computacionais é possível identificar a mudança de significado de termos, seja ao longo de um século (com dados do Google NGram), seja em dinâmicas mais rápidas, como no twitter. Um dos termos analisado foi o “gay”: Linguistic Change da palavra “gay” Várias outras palavras foram analisadas, como “tape” que significava “fita adesiva”, mas passou a significar também “fita cassete” nos anos 1970; ou “apple” e “windows” que ganharam novo significado com a indústria da computação. KULKARNI, V., Al-Rfou, R; PEROZZI, B e SKIENA, S. Statistically Significant Detection of Linguistic Change. WWW 2015, May 18–22, 2015. http://dx.doi.org/10.1145/2736277.2741627 . 2.5 Exemplo: Análise de complexidade musical A reportagem da Folha de São Paulo Música brasileira foi simplificada ao longo das décadas, diz pesquisa cita o trabalho do cientista de dados Leonardo Sales (blog do autor), que analisou os acordes e vocabulário das letras, com base em 44 mil cifras e 102 mil letras raspadas de sites como cifras.com.br e letras.com.br em uma série de postagens: parte 1, parte 2, parte 3 sobre as letras, parte 4. Os códigos para raspagem de dados destes sites estão disponíveis em Python. grafico Como quase toda notação, há vantagens e desvantagens. Uma desvantagem, que levantou críticas, é que a análise se baseia em cifra, muito utilizada em músicas mais populares, mas inadequada para estilos mais complexos, como jazz. 2.6 Exemplo: Polarização Uma boa parte de pesquisas em política com métodos digitais se dedicou a analisar o fenômeno da polarização política. Christopher A. Bail, Lisa P. Argyle, Taylor W. Brown, John P. Bumpus, Haohan Chen, M. B. Fallin Hunzaker, Jaemin Lee, Marcus Mann, Friedolin Merhout, and Alexander Volfovsky. 2018. Exposure to opposing views on social media can increase political polarization. Proceedings of the National Academy of Sciences 115, 37 (Sept. 2018), 9216–9221. https://doi.org/10.1073/pnas.1804840115 Publisher: National Academy of Sciences Section: Social Sciences. Parte do trabalho de Franzosi descrevemos na seção sobre dados estruturados. Roberto P. Franzosi. Sociology, narrative, and the quality versus quantity debate (Goethe versus Newton): Can computer-assisted story grammars help us understand the rise of Italian fascism (1919–1922)? . Theor Soc (2010) 39:593–629 DOI 10.1007/s11186-010-9131-3 2.7 Exemplo de integração quali-quanti: Complementando dados qualitativos Uma dica de integração quali-quanti (qualitativo e quantitativo) usando análise textual vem do PEW Research. Aqui explicam como a partir de uma análise de grupos focais feita em 2019 com grupos dos EUA e da Grã Bretanha sobre atitudes frente a globalização/nacionalismo, complementaram com pesquisa quantitativa de análise textual, analisando as diferenças entre os grupos de cada país. Usaram técnicas como frequência de palavras, correlação de palavras e Topic modelling. Através destas análises, encontraram tópicos que se mostraram relevantes a serem incorporados em surveys futuros. DEVLIN, Kat.“How quantitative methods can supplement a qualitative approach when working with focus groups”. medium. Dec 18, 2020. 2.8 Exemplo de integração quali-quanti: Depurando dados quantitativos Nem sempre uma tabela de dados estruturados tem tudo estruturado, do modo que a sua pergunta de pesquisa necessite. Dentro de um campo específico de uma tabela pode-se precisar de desmembrar ainda mais os dados. Eis aqui um exemplo. Os pesquisadores do Dadoscope queriam investigar se houve aumento na abertura de igrejas evangélicas durante os anos Lula e Dilma. Os pesquisadores baixaram dados da Receita Federal referente ao Cadastro Nacional de Pessoa Jurídica e filtraram por “94.91–0–00 — Atividades de organizações religiosas ou filosóficas” no campo “Classificação Nacional de Atividades Econômicas”. O problema é que isto agrega não só igrejas evangélicas, como também católicas e de outras religiões e até agremiações filosóficas e institutos de psicanálise. Aqui entra a integração: Para tentarmos realizar a classificação das 150 mil igrejas evangélicas de maneira semi-supervisionada nós usamos Snorkel, uma biblioteca escrita em Python… foi preciso treinar um algoritmo de classificação usando uma amostra dos dados. De forma sucinta, os dados são separados em amostras que são usadas para treino, teste e validação da classificação. Para classificar os mais de 150 mil nomes únicos presentes na amostra de treino, criamos funções que classificam de forma grosseira as igrejas (e.g., se a palavra “assembleia” estiver presente, classificar a igreja como “evangélica”). Depois de escrever dezenas dessas funções, comparamos sua acurácia com uma amostra de teste de 5% dos nomes únicos, manualmente classificados por dois pesquisadores. Feito isso, usamos uma rede neural que combina em camadas estas funções e voilá: 91.8% de acurácia. Sabemos que este resultado não é perfeito, mas ele torna o trabalho de classificação viável. O artigo completo “Exclusivo: Igrejas evangélicas pentecostais tiveram boom de crescimento nos governos Lula e Dilma” pode ser lido aqui: artigo na Forum e o mesmo artigo na Medium. Caderno de notas no dadoscope para entender melhor como o processo foi realizado. Sobre a ferramenta utilizada, a Snorkel, ver a página do Github, introdução (em inglês) ao Snorkel. Por fim, outra dica para pensar a integração de dados quantitativos e qualitativos é a palestra de Dr. Christof Schöch: The Convergence of Quantitative and Qualitative Approaches, ocorrida no 1st Summerschool of Digital Humanities: Distant Reading - Potentials and Applications, em inglês. "],["organização-dos-dados-quanto-a-sua-estrutura.html", "Capítulo 3 Organização dos dados quanto a sua estrutura 3.1 Dados estruturados 3.2 Dados não estruturados 3.3 Dados semi-estruturados 3.4 Observações finais", " Capítulo 3 Organização dos dados quanto a sua estrutura Objetivos do capítulo: Apresentar a distinção entre dados estruturados, dados não estruturados e dados semi estruturados Apresentar alguns formatos de arquivos frequentes na análise textual e humanidades digitais, como csv e tsv, Json, markdown, yaml, LaTex, BibTex, xml e html. Podemos pensar a organização de dados quanto à sua estrutura de três formas: dados estruturados, dados semi estruturados e dados não estruturados. 3.1 Dados estruturados Formatos de arquivos estruturados são csv,xml, json, xls, xlsx, etc. Muitos destes possuem formato de tabela, o que torna bastante fácil encontrar a informação buscada. 3.1.1 Os formatos csv (comma separeted values) e tsv. O formato csv (comma separeted values ou “valores separados por vírgula”) é um dos mais simples, consiste de arquivo de texto simples, com valores separados por um caractere (ou conjunto de caracteres) que separam os valores em cada linha, sendo geralmente vírgula ou ponto e vírgula ou tabulação (tecla tab). Qualquer caractere ou conjunto de caracteres pode ser usado como separador de campos. Na imensa maioria dos casos cada linha é separada pela quebra de linha. Por exemplo, a seguinte tabela: Estado sigla capital região Acre AC Rio Branco Norte Alagoas AL Maceió Nordeste Amapá AP Macapá Norte Amazonas AM Manaus Norte Bahia BA Salvador Nordeste Ceará CE Fortaleza Nordeste Em abrirmos o csv no bloco de notas (notepad): Estado;sigla;capital;região; Acre;AC;Rio Branco;Norte; Alagoas;AL;Maceió;Nordeste; Amapá;AP;Macapá;Norte; Amazonas;AM;Manaus;Norte; Bahia;BA;Salvador;Nordeste; Ceará;CE;Fortaleza;Nordeste; O separador de campo neste arquivo CSV é o ponto e vírgula ;. Ao pedirmos ao computador para localizar qual a designação da sigla “AP”, ele saberá buscar facilmente esta informação. No caso ali, a vírgula é o separador de campos, mas qualquer outro caractere pode ser usado como separador. O formato .tsv, por exemplo, é separado por tabulação - ou o símbolo \\t. Mas é possível encontrar arquivo csv, porém com separador tipo “ ou”;\". 3.1.2 O formato Json O Json (“JavaScript Object Notation”, isto é “Notação de Objetos JavaScript”), é organizado no esquema de pares nome/valor. Por exemplo, ao separarmos primeiro nome firstName de sobrenome lastName no Json: {&quot;employees&quot;:[ { &quot;firstName&quot;:&quot;João&quot;, &quot;lastName&quot;:&quot;da Silva&quot; }, { &quot;firstName&quot;:&quot;Ana&quot;, &quot;lastName&quot;:&quot;Maria&quot; }, { &quot;firstName&quot;:&quot;Joaquim&quot;, &quot;lastName&quot;:&quot;Xavier&quot; } ]} O arquivo json inicia e termina com colchetes [] Todo Json é delimitado por chaves {}, os dados são representados no esquema nome/valor `\"nome\": \"valor\". estes são separados por vírgula. DICA: Caso queira mais detalhes sobre o formato Json: Video introdutório sobre o formato Json do canal Código Fonte TV JSON // Dicionário do Programador. Video introdutório, porém mais prático, focado na estrutura do mesmo: JSON em 6 minutos do canal “Canal TI”. Para ver as regras de sintaxe do Json. 3.2 Dados não estruturados Os dados não estruturados são a forma como encontramos em livros impressos, artigos, jornais, revistas, etc. São a forma de texto que nós humanos lemos normalmente. Por exemplo: “Algum tempo hesitei se devia abrir estas memorias pelo principio ou pelo fim, isto é, se poria em primeiro logar o meu nascimento ou a minha morte. Supposto o uso vulgar seja começar pelo nascimento, duas considerações me levaram a adoptar differente methodo: a primeira é que eu não sou propriamente um autor defunto, mas um defunto autor, para quem a campa foi outro berço; a segunda é que o escripto ficaria assim mais galante e mais novo. Moysés, que tambem contou a sua morte, não a poz no introito, mas no cabo: differença radical entre este livro e o Pentateuco….” Este tipo de texto, não estruturado, é alvo do Processamento de linguagem natural (PLN)/ Natural Language Process (NLP). 3.3 Dados semi-estruturados Dados semi-estruturados são um meio termo entre os estruturados e os semi estruturados. Por vezes são chamados de “auto-descritivos”. Vejamos exemplos destes. 3.3.1 Exemplos de dados semi-estruturados 3.3.1.1 Markup Códigos especiais, ou linguagem “markup” é uma notação de documento que tem duas apresentações, uma simplificada como texto normal para humanos, e outra com os “markup” para que o computador entenda. 3.3.2 O formato Markdown Um exemplo bem simples de markup é o Mardown, usado na escrita rápida de textos. Exemplo de markdown 3.3.3 O formato YAML O YAML (“YAML Ain’t Markup Language”) é um padrão de serialização de dados que prima por ser “human friendly”, isto é, de fácil leittura também para humanos. Em arquivos markdown tem-se usado o yaml como cabeçalho, com informações para a renderização do pdf, como título, subtítulo, resumo, palavras chave, etc. Ao converter markdown para o formato final, o computador irá interpretar estas informações. Um exemplo de yaml no arquivo markdown: --- title: &quot;Título do meu pdf&quot; subtitle: subtitulo qualquer author: Fulano de Tal # comentário qualquer fontsize: 12pt urlcolor: blue geometry: margin=2.5cm abstract: &gt; meu resumo bla bla bla bla --- # Titulo Texto texto texto texto texto texto texto ## Subtitulo Texto texto texto texto texto texto texto O cabeçalho em yaml é delimitado no seu início e fim por três traços consecutivos ---. Repare que o símbolo tralha # dentro do yml é interepretado como comentário, já no markdown, indica capítulo. DICA: Um modo prático de trabalhar na elaboração de textos - principalmente acadêmicos - com markdown e yaml é renderizá-lo com o pandoc, que é um canivete suíço na transformação de formatos de texto. Com ele, pode-se criar pdfs, html, doc, docs, odt, etc. a partir de seu arquivo markdown. Pandoc funciona via linha de comando. 3.3.4 O Formato LaTex O LaTex é uma linguagem usada na confecção, principalmente de textos (livros, artigos) acadêmicos, bem como apresentações. O formato LaTex permite grande flexibilidade, e é muito usado para escrever fórmulas matemáticas e gerar as referências bibliográficas automaticamente. Por isso, o LaTex é muito usado no contexto acadêmico. O seu formato mínimo pode ser visto assim: \\documentclass{article} \\begin{document} Olá Mundo \\end{document} Exemplo simples de texto em LaTex e sua renderização Ou em um exemplo um pouco mais elaborado: Exemplo de LaTex com o software Gummi no Linux Perceba que antes de \\begin{document}, isto é, no cabeçalho do documento temos várias informações, entre elas o título do artigo na linha 5 em title{}, e em \\author{}, nas linhas de 6 a 8, temos os autores. Temos também delimitados os capítulos ou seções, no caso ali em section{}. DICA: Para renderizar textos .tex em pdfs deve-se usar um interpretador. O pandoc é uma opção. Embora seja possível usar apenas o interpretador/conversor e um bloco de notas, o mais comum em Tex e LaTex é usar algum programa focado. O TeXstudio é uma boa opção. Caso use Linux e queria a renderização à medida que edita o texto, olhe o Gummi. Caso queira fazer os documentos em LaTex sem ter de “programar”, dê uma olhada no LyX. Há também editores de LaTex online, como o overleaf, que possibilita trabalhar em equipe, observando as alterações feitas por cada pessoa 3.3.5 O formato BibTex Um formato “irmão” do LaTex e markdown é o BibTex, um formato estruturado, com dados bibliográficos usado como fonte para gerar automaticamente a bibliografia ao final do texto renderizado em formatos como Tex, LaTex e markdown. As referências nesse formato ficam salvos num grande arquivo .bib. Um exemplo de citação dentro do bib: @book{Coleman:IntroMathSociology, address = {New York}, pages = {570}, publisher = {The Free Press of Glencoe Collie, Macmillan Limited}, title = {Introduction to Mathematical Sociology}, year = {1964} } O @book indica o tipo, podendo ser também, por exemplo, @article para artigos, @inbook para parte de um livro, @phdthesis para tese de phd (há mais opções). Para uma lista completa, ver The 14 BibTeX entry types. O Coleman:IntroMathSociology é o ID, a identificação única, que é também usado na citação do LaTex (Por exemplo, usando \\cite{Coleman:IntroMathSociology} dentro do Tex) ou do Markdown (usando [@IntroMathSociology] dentro do texto) para que o compilador saiba qual texto está sendo citado no texto. Podemos usar o texto que quisermos ali, desde que sem espaço. Exemplo de citação usando bibtex no LaTex Este arquivo bib que contém as referências bibliográficas, como é texto puro, pode ser editado num editor de texto comum, como o notepad, Gedit, etc. Mas o mais indicado é usar um software gestor de bibliografia, como o JabRef ou o KBibTex. O KBibTex possui menos recursos que o JabRef mas dá plenamente conta do recado, sendo inclusive o gerenciador que utilizo. Além de ser usado para gerar pdfs com as referências, o formato também pode ser usado em pesquisas bibliométricas. 3.3.6 Os formatos xml e html No caso, nome seria “FirstName” e seu valor seria “João”, nome seria “lastName” e seu valor “da Silva” E esses mesmos dados no formato xml: &lt;employees&gt; &lt;employee&gt; &lt;firstName&gt;João&lt;/firstName&gt; &lt;lastName&gt;da Silva&lt;/lastName&gt; &lt;/employee&gt; &lt;employee&gt; &lt;firstName&gt;Ana&lt;/firstName&gt; &lt;lastName&gt;Maria&lt;/lastName&gt; &lt;/employee&gt; &lt;employee&gt; &lt;firstName&gt;Joaquim&lt;/firstName&gt; &lt;lastName&gt;Xavier&lt;/lastName&gt; &lt;/employee&gt; &lt;/employees&gt; Algumas linguagens usadas em texto são chamadas de markup, onde o que é mostrado na tela, para humanos lerem, difere do que o computador “entende”. Exemplo é o xml acima, o html ou ainda linguagens como markdown e LaTex. O html tem por base o xml. O html possui basicamente a seguinte estrutura &lt;!doctype html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Titulo da pagina&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;Título do capítulo&lt;/h1&gt; &lt;p&gt;Texto texto texto&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; Se você copiar o conteúdo acima e salvar num arquivo com o nome, digamos teste.html e abrí-lo com seu navegador de internet (Firefox, Chrome, Opera, etc.), verá como funciona esta ideia de markup. No caso do html, os valores são delimitados por tags, como: &lt;ALGO&gt;conteúdo&lt;/ALGO&gt;, onde &lt;/ indica que estamos fechando a tag. Assim, em &lt;h1&gt;Título do capítulo&lt;/h1&gt;, o texto “Título do capítulo” está entre a tag “h1”. Os arquivos html, assim como o LaTex, possui duas partes principais. O cabeçalho (head) e o corpo (body). Figure 3.1: Tatuagem head/body. Autor desconhecido Entre &lt;head&gt; e seu fechamento, &lt;/head&gt; ficam os metadados, como título, data, etc. Entre &lt;body&gt; e &lt;/body&gt; fica o conteúdo da página que aparece dentro do navegador. Um outro exemplo: &lt;name&gt;Joaquim José da Silva Xavier&lt;/name&gt;, o Tiradentes (&lt;local&gt;Fazenda do Pombal&lt;/local&gt;, batizado em &lt;data&gt;12 de novembro de 1746&lt;/data&gt; — &lt;local&gt;Rio de Janeiro&lt;/local&gt;, &lt;data&gt;21 de abril de 1792&lt;/data&gt;), foi um &lt;profissao&gt;dentista&lt;/profissao&gt;, &lt;profissao&gt;tropeiro&lt;/profissao&gt;, &lt;profissao&gt;minerador&lt;/profissao&gt;, &lt;profissao&gt;comerciante&lt;/profissao&gt;, &lt;profissao&gt;militar&lt;/profissao&gt; e &lt;profissao&gt;ativista político&lt;/profissao&gt; &lt;gentilico&gt;brasileiro&lt;/gentilico&gt;, que atuou nas capitanias de &lt;local&gt;Minas Gerais&lt;/local&gt; e &lt;local&gt;Rio de Janeiro&lt;/local&gt;. Onde podemos ver tags como &lt;name&gt;, &lt;local&gt;, &lt;data&gt;, etc. ao redor de certas informações, o que torna possível ao computador encontrar estas informações. 3.3.7 Formatos mais raros Há ainda a possibilidade de uso de estruturação de texto não muito comuns e com fins bem específicos. Por exemplo, Franzosi (2010) ao fazer análise da narrativa de jornais italianos da época de ascensão do Fascismo, passou textos não estruturados como este: Republicans plunged in Bissone di S. Cristina around 10pm of this month at the pub Prati. A guy, who went by the name of “captain,” took out a list of names and did the roll call loudly. Para o seguinte formato: [Semantic triplet 1: [Participant: [Actor: republicans]] [[Process: [[Verb: plunge] [Circumstances: [Space: [City: Bissone di S. Cristina] [[Location: pub] [Name: Prati]]]] [[Time: [Date: 05/07/1921] [Hour: 10pm]]]]] [Semantic triplet 2: [Participant: [Actor: captain]] [Process: [[Verb: does roll call] [Circumstances: [Type of action: loudly] [Instrument: list]]] [Participant: [Actor: workers]] Para tal, Franzosi desenvolveu um software para análise de narrativas textuais, o PC-ACE (Program for Computer-Assisted Coding of Events) e pôde ter uma noção melhor da violência cotidiana na época, gerado tabelas como esta: Lista da ocorrências diárias de triplets de violência, obtidas no jornal Avanti! (FRANZOSI, p.607) Frequência da distribuição dos triplets de violência no jornal Avanti! (FRANZOSI, p.607) E ainda fez um “mapa de calor” (“heat map”) com a localização da violência fascista na Itália Mapa de calor de ações fascistas de violência e localização de suas sedes insittucionais (FRANZOSI, p.609) Referência: FRANZOSI, Roberto P.. Sociology, narrative, and the quality versus quantity debate (Goethe versus Newton): Can computer-assisted story grammars help us understand the rise of Italian fascism (1919–1922)?. Theor Soc (2010) 39:593–629. DOI 10.1007/s11186-010-9131-3 3.4 Observações finais Os dados semi-estruturados não tem, portanto, formato de tabela, mas contêm indicações de informações mais abstratas, através de tags ou outras marcações. Com base nestas informações que faremos análises de texto de modo computacional. Este processo de transformação de dados não estruturados em estruturados é chamado de “datificação”. Os formatos como Json, csv, tsv, xml são importantes pois ao solicitar dados em diversos sites, muitos APIs retornam dados nestes formatos. "],["introdução-ao-r.html", "Capítulo 4 Introdução ao R 4.1 Obtendo Ajuda no R 4.2 Tipos de Dados no R (data types) 4.3 Estrutura de dados no R (Data Structures) 4.4 A suíte de pacotes tidyverse", " Capítulo 4 Introdução ao R Objetivos do capítulo: Apresentar como obter ajuda no R Apresentar os tipos de dados e as estruturas de dados no R Apresentar a suíte de pacotes do tidyverse Supondo que já tenha o R e o RStudio instalados (há diversos tutoriais de instalação no Youtube), vamos aos primeiros passos. É possível também rodar o R de modo online, de modo gratuito, sem instalar nada em seu computador, com o One Compiler e o Snippets, tendo mais de 19 mil pacotes instalados. O lado negativo é que não é um modo prático e tão rápido como rodar em seu computador, mas pode servir para pequenos testes. Snippets: um site para rodar R online 4.1 Obtendo Ajuda no R A primeira coisa a se aprender é como conseguir ajuda, como conseguir a informação que precisamos. A busca por ajuda é uma constante, mesmo entre os mais experiente. Comando Descrição Exemplo de uso help.start() para abrir o sistema de ajuda em HTML no seu navegador help.start() help(\"função\") ou ?funcao() Acesso à documentação de funções, data sets e outros. No Rstudio, a ajuda é aberta na aba “Help”. Caso rode direto no console, sem o RStudio, uma vez dentro da página de ajuda, digite / para realizar busca e q para sair ?getwd ou help(getwd) help(função, package=\"NOME_PACOTE\") para obter ajudar de um pacote que não foi cerregado help(rlm, package=\"MASS\") Ajuda para a função rlm() do pacote MASS help(package = 'NOME_PACOTE') Mostra um índice de páginas de ajuda para o pacote help(package = 'dplyr' example() Mostra exemplos de uso da função example(grep) help.search() ou ??busca Para busca mais vaga. Caso não se lembre do nome exato da função, ou caso busque uma função que faça determinada tarefa ??regression mostra diversas funções de diversos pacotes que contém a palavra “regression” RSiteSearch() Busca no seu navegador (browser) padrão em sites especializados em R RSiteSearch(\"text analysis\") args() Mostra os argumentos que uma função pode receber. args(grep) apropos() Busca pelo nome de uma função. Útil quando não lembramos exatamente o nome de uma função apropos(\"grep\") demo() Lista todas as demonstrações de todos os seus pacotes demo() demo(package=\"package-name\") lista as demonstrações de um pacote particular demo(package=\"stats\") Fonte: Versão expandidada de Getting Help with R. # Buscando por funções que contenham &quot;grep&quot; apropos(&quot;grep&quot;) ## [1] &quot;agrep&quot; &quot;agrepl&quot; &quot;grep&quot; &quot;grepl&quot; &quot;grepRaw&quot; # Mostrando os argumentos que uma função pode receber args(gsub) ## function (pattern, replacement, x, ignore.case = FALSE, perl = FALSE, ## fixed = FALSE, useBytes = FALSE) ## NULL Outra dica é buscar a documentação do pacote no site do cran ou buscar por problemas específicos no o site Stack Overflow. 4.1.1 Obtendo ajuda com o R Commander (Rcmdr) Para quem está aprendendo a usar estatística no R, uma boa dica é usar o pacote R Commander. Ao rodá-lo, uma janela aparece onde escolhemos o tipo de operação, e ele mostra o código para tal. O Rcmdr ensina como importar dados (SPSS, Excell, Stata, ruls, etc.), como realizar diversas operações estatísticas, como plotar gráficos. Ao rodar o Rcmdr pela primeira vez, pacotes adicionais serão instalados, como na imagem abaixo. Instalação do R Commander. Instalando pacotes adicionais Após a instalação de pacotes adicionais, O R Commander abre uma janela com os comandos. R Commander em ação Caso feche a janela do RCommander, rodar library(Rcmdr) novamente não abrirá a janela, uma vez que o pacote já está carregado. A solução é rodar então Commander() para ter a janela novamente. Caso queira fechar este pacote, basta digitar no Console detach(\"package:Rcmdr\", unload=TRUE) ou apenas feche o R. Dicas: R Commander LAURETTO, Marcelo. Introdução ao R Commander. (Pequeno tutorial do RCommander, em PDF e em português.) The R Commander: A Basic-Statistics GUI for R FOX, John. Getting Started With the R Commander. (Artigo do criador do RCommander) Video tutorial do Rcommander em ação e em português: Aprendendo Estatística com o R Commander do Departamento de Estatística UFLA 4.1.2 Comentando o código Nas linguagens de programação é possível acrescentar comentários para orientar os humanos e que não serão lidos pelo computador. Cada linguagem tem o seu próprio padrão. No R, assim como em muitas linguagens de programação, os comentários no código são feito com a tralha # ou hashtag, jogo da velha, etc. Tudo que vier depois deste símbolo é então ignorado. print(&quot;Olá mundo!&quot;) ## [1] &quot;Olá mundo!&quot; print(&quot;Olá mundo R!&quot;) # comentário que o computador ignora ## [1] &quot;Olá mundo R!&quot; Se retirarmos o símbolo # temos então mensagem de erro. print(&quot;Olá mundo! Olá R&quot;) comentários que dará erro Error: unexpected symbol in &quot;print(&quot;Olá mundo! Olá R&quot;) comentários&quot; Execution halted 4.2 Tipos de Dados no R (data types) “Tudo que existe no R é um objeto” (John Chambers, apud WICKHAM) Tipos de dados se referem à forma mais simples de objetos, ou tipos de dados atômicos do R (R atomic data types) Há cerca de 25 tipos de dados no R, ou “base types”, mas os cinco tipos de dados mais frequentes são: Tipo de dado no R Exemplo de uso 1 character (texto) “a”, “bla”, “Fulano de tal” 2 numeric (real ou decimal) 2, 43 3 integer (integral) 3L (O “L” é o modo que o R entende que é um integral) 4 logical (lógico) TRUE ou FALSE 5 complex (complexo) 1+4i (números complexos com partes reais e imaginárias) Uma vez criados os dados, para examinar as características dos objetos há algumas funções no R Função no R para examinar tipos de dados Descrição Exemplo class() Que tipo de objeto é? class(3), class(\"bla\") length() Qual o tamanho dos vetores? Quantos itens possui? length(c(\"bla\",\"ble\")) attributes() Possui metadados? # Criando um texto (character). Tem de ser delimitado entre aspas minha_variavel_texto &lt;- &quot;bla&quot; minha_variavel_texto ## [1] &quot;bla&quot; class(minha_variavel_texto) ## [1] &quot;character&quot; # Criando uma variável numérica. Não pode ser delimitado por aspas. minha_variavel_numerica &lt;- 2 minha_variavel_numerica ## [1] 2 class(minha_variavel_numerica) ## [1] &quot;numeric&quot; # Criando um integral: minha_variavel_integral &lt;- 5L minha_variavel_integral ## [1] 5 class(minha_variavel_integral) ## [1] &quot;integer&quot; # Criando um variável lógica. Também sem aspas. ## 5 é maior que 3? minha_variavel_logica &lt;- 5 &gt; 3 minha_variavel_logica ## [1] TRUE class(minha_variavel_logica) ## [1] &quot;logical&quot; ## 5 é menor que 3? minha_variavel_logica2 &lt;- 5 &lt; 3 minha_variavel_logica2 ## [1] FALSE minha_variavel_logica3 &lt;- TRUE # É possível atribuir minha_variavel_logica3 ## [1] TRUE minha_variavel_logica4 &lt;- F # Também é possível usar abreviaturas &quot;T&quot; e &quot;F&quot; minha_variavel_logica4 ## [1] FALSE # Criando um número complexo: numero_complexo &lt;- 3 + 2i numero_complexo ## [1] 3+2i class(numero_complexo) ## [1] &quot;complex&quot; Na criação de tipos de dados, pode-se usar de coerção (coercion) através das funções as. Função Coerção para as.numeric Numeric as.integer Integer as.double Double as.character Character as.logical Boolean EM CONSTRUÇÃO 4.3 Estrutura de dados no R (Data Structures) EM CONSTRUÇÃO Vimos os tipos de dados. Estes podem ser organizados de distintos modos, e a depender desta organização, temos diferentes estruturas de dados. O R possui diversas estruturas de dados, como: vetores numéricos e atômicos lista (list) fatores (factors) matriz (matrix) data frame 4.3.1 Vetor (vector) Vetor é a estrutura de dados mais comum no R. Criamos vetores com a função c() de “concatenate” ou “combine”. No caso de strings (character), deve-se colocar os valores dentro de aspas: meu_vetor &lt;- c(&quot;bla&quot;, &quot;bla&quot;, &quot;ble&quot;, &quot;bli&quot;) # criando o vetor com strings meu_vetor # imprimindo na tela o vetor que acabamos de criar ## [1] &quot;bla&quot; &quot;bla&quot; &quot;ble&quot; &quot;bli&quot; class(meu_vetor) ## [1] &quot;character&quot; length(meu_vetor) # observando quantos itens possui nosso vetor ## [1] 4 meu_vetor[3] # acessando o item do 3 de nosso vetor ## [1] &quot;ble&quot; # Criando um vetor de valores booleanos # Não se usa aspas nesse caso vetor_logico &lt;- c(TRUE, FALSE, FALSE) # criando um vetor de valores booleanos class(vetor_logico) ## [1] &quot;logical&quot; # Criando um vetor com diferentes tipo de dados vetor_pan &lt;- c(&quot;bla&quot;, 3, TRUE, 5L) vetor_pan ## [1] &quot;bla&quot; &quot;3&quot; &quot;TRUE&quot; &quot;5&quot; # criando uma sequência numérica de 2 a 8. Desta vez não iremos salvar em uma variável c(2:8) ## [1] 2 3 4 5 6 7 8 Vetores podem ser de dois tipos: 1) atômicos, ou 2) listas. Mas o mais frequênte é encontrarmos “vetor” como sinônimo de “vetor atômico”, e listas serem consideradas como algo à parte. Para examinarmos os vetores, podemos usar as funções: typeof(), length(), class() e str() É ainda possível também atribuir nomes aos elementos de um vetor usando a função names() valores &lt;- c(12, 34, 13) # Criando um vetor de valores nomes &lt;- c(&quot;banana&quot;, &quot;uva&quot;, &quot;abacate&quot;) # Criando vetor com os nomes names(valores) &lt;- nomes # atribuindo o vetor &quot;nomes&quot; como titulo do vetor &quot;valores&quot; valores # imprimindo o vetor com nomes e valores ## banana uva abacate ## 12 34 13 names(valores) # imprimindo apenas os nomes ## [1] &quot;banana&quot; &quot;uva&quot; &quot;abacate&quot; class(valores) ## [1] &quot;numeric&quot; 4.3.2 Fator (Factor) Considere a seguinte tabela fictícia: Nome Altura(cm) Região do país Fulano 175 NE Ciclano 134 SE Beltrano 166 S João 187 S Maria 173 NE José 159 CO Joaquim 161 SE Se em uma coluna certos valores se repetem, podemos considerá-los como “fatores”. No caso a coluna “Nome” possui valores únicos, a coluta “Altura” também, mas a coluna “Região” possui uma quantidade limitada de valores que se repetirão: N,S,NO,SE,CO. Podemos então considerar os valores desta coluna como “fatores”. Fatores podem ser tanto strings, como no caso acima, como integrais, como no caso da idade de estudantes de uma mesma turma. Um exemplo prático: # Criando um vetor de caracteres com siglas de aeroportos brasileiros aeroportos &lt;- c(&quot;BSB&quot;, &quot;CON&quot;, &quot;BSB&quot;, &quot;VIC&quot;, &quot;GUA&quot;, &quot;FOR&quot;, &quot;MAO&quot;,&quot;GUA&quot;, &quot;CON&quot;, &quot;CON&quot;, &quot;REC&quot;, &quot;UDI&quot;, &quot;VIC&quot;, &quot;GUA&quot;) summary(aeroportos) ## Length Class Mode ## 14 character character # Se tento criar um barplot com os valores, dá erro: barplot(summary(aeroportos)) ## Error in barplot.default(summary(aeroportos)): &#39;height&#39; must be a vector or a matrix # Temos de transformar os dados em um fator, que vamos chamar de &quot;aeroportos.factor&quot; aeroportos.fator &lt;- factor(aeroportos) aeroportos.fator # Perceba que &quot;Levels&quot; mostra os valores sem repetição ## [1] BSB CON BSB VIC GUA FOR MAO GUA CON CON REC UDI VIC GUA ## Levels: BSB CON FOR GUA MAO REC UDI VIC summary(aeroportos.fator) # Mostrando a frequência de cada termo com a função &#39;summary()&#39; ## BSB CON FOR GUA MAO REC UDI VIC ## 2 3 1 3 1 1 1 2 # Agora é possível gerar o gráfico de barras (barplot) barplot(summary(aeroportos.fator)) Fatores são comuns em tabelas, mas são raros em análise textual. Assim, ao trabalhar com análise textual, convém mudar as opções globais de strings, para não serem consideradas fatores: options(stringsAsFactors = FALSE) 4.3.3 Matriz (Matrix) Vejamos as matrizes: # 1:12=Os elementos da nossa matriz, 1 a 12 # 4=linhas no eixo vertical 3=linhas noeixo horizontal minha.matriz &lt;- matrix(1:12, 4, 3) minha.matriz ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 ## [3,] 3 7 11 ## [4,] 4 8 12 # Acessando itens, linha 2, coluna 3 minha.matriz[2, 3] ## [1] 10 # Acessando uma linha inteira minha.matriz[2, ] ## [1] 2 6 10 Matrizes também são usadas na análise textual, como nas Document-Term-Matrix e TermDocument Matrix. Veremos em mais detalhes posteriormente, mas apenas para um primeiro contato, o DTM das frases “O rato roeu a roupa do rei”, “O rei riu do rato”, “A roupa do rato é de rei”, onde cada linha representa um documento (documento, neste caso, é cada uma das frases) e as colunas são as palavras. Os números são a frequência de cada termo em cada frase: Table 4.1: Um exemplo de DTM rato rei riu roeu roupa 1 1 0 1 1 1 1 1 0 0 1 1 0 0 1 4.3.4 Listas (list) Listas são conjuntos de vetores mais simples. Para criar uma lista, basta usar o comando list() minha_lista = list(c(T,T,F), &quot;Joaquim José da Silva Xavier&quot;, c(3,6,3,67,22) ) minha_lista ## [[1]] ## [1] TRUE TRUE FALSE ## ## [[2]] ## [1] &quot;Joaquim José da Silva Xavier&quot; ## ## [[3]] ## [1] 3 6 3 67 22 # Acessando o vetor 3 de nossa lista. No caso de listas, usamos o duplo colchete minha_lista[[3]] ## [1] 3 6 3 67 22 # Acessando o vetor 3 de nossa lista, item 5 minha_lista[[3]][5] ## [1] 22 É possível também “quebrar” um texto em vetores, através do comando strsplt() texto = &quot;Bla bla bla. Ble ble ble ble. Bli bli. Blo. Blu blu blu&quot; class(texto) ## [1] &quot;character&quot; strsplit(texto, &quot;\\\\.&quot;) ## [[1]] ## [1] &quot;Bla bla bla&quot; &quot; Ble ble ble ble&quot; &quot; Bli bli&quot; &quot; Blo&quot; ## [5] &quot; Blu blu blu&quot; No exemplo acima, o primeiro argumento em strplit() é a variável, o segundo argumento é o critério a ser usado para quebrar o texto. Alguns caracteres são considerados especiais e precisam ser indicados que queremos seu significado literal. É o caso de ponto final. Para conseguirmos usá-lo como ponto final, ao invés de usarmos \".\", usamos \\\\. . Note que ficaram espaços no início dos elementos dos vetores. Podemos melhorar nosso código acrescentando um espaço em branco após o ponto final: strsplit(texto, &quot;\\\\. &quot;) ## [[1]] ## [1] &quot;Bla bla bla&quot; &quot;Ble ble ble ble&quot; &quot;Bli bli&quot; &quot;Blo&quot; ## [5] &quot;Blu blu blu&quot; Se examinarmos o tipo de arquivo que temos, veremos que se trata de uma lista: texto2 &lt;- strsplit(texto, &quot;\\\\. &quot;) class(texto2) ## [1] &quot;list&quot; Se quisermos que este seja um vetor simples, e não lista: class(texto2) ## [1] &quot;list&quot; # usar o comando unlist() unlist(texto2) ## [1] &quot;Bla bla bla&quot; &quot;Ble ble ble ble&quot; &quot;Bli bli&quot; &quot;Blo&quot; ## [5] &quot;Blu blu blu&quot; class(unlist(texto2)) ## [1] &quot;character&quot; # Ou ainda, acessando o item 1 da lista strsplit(texto, &quot;\\\\. &quot;)[[1]] ## [1] &quot;Bla bla bla&quot; &quot;Ble ble ble ble&quot; &quot;Bli bli&quot; &quot;Blo&quot; ## [5] &quot;Blu blu blu&quot; 4.3.5 Data Frames Uma vantagem de se trabalhar com data frame é que as colunas tem nome e não apenas índice, e isto facilita nosso trabalho na hora de procurar as informações que queremos, pois buscar a coluna pelo seu número pode nos causar confusão. # Criando 3 vetores com os valores idade &lt;- as.integer(c(12,23,35)) genero &lt;- as.character(c(&quot;fem&quot;, &quot;mas&quot;, &quot;fem&quot;)) raça &lt;- as.character(c(&quot;pret&quot;, &quot;branc&quot;, &quot;pard&quot;)) # jogando estes vetores no nosso dataframe que vamos nomear de &quot;df&quot; df &lt;- data.frame(idade, genero, raça) df # vendo nosso dataframe no console ## idade genero raça ## 1 12 fem pret ## 2 23 mas branc ## 3 35 fem pard View(df) # ver o dataframe em uma nova janela pop-up class(df) # Qual a classe de nosso objeto, caso tenhamos esquecido? ## [1] &quot;data.frame&quot; names(df) # retornando apenas o nome das colunas ## [1] &quot;idade&quot; &quot;genero&quot; &quot;raça&quot; df$genero # Filtrando o dataframe &quot;df&quot; pela coluna &quot;genero&quot; ## [1] &quot;fem&quot; &quot;mas&quot; &quot;fem&quot; df[2] # Filtrando o dataframe pela coluna 2: df[numero] ## genero ## 1 fem ## 2 mas ## 3 fem df[,2] # Filtrando o dataframe pela coluna 2: ## [1] &quot;fem&quot; &quot;mas&quot; &quot;fem&quot; # Filtrando o dataframe pelo número da linha: df[número_da_Linha,] . Repare na necessidade da vírgula. df[1,] # imprimindo a linha 1 ## idade genero raça ## 1 12 fem pret df[3,] # imprimindo a linha 3 ## idade genero raça ## 3 35 fem pard df[1:2,] # retornando as linhas de 1 a 2 ## idade genero raça ## 1 12 fem pret ## 2 23 mas branc # media da coluna &quot;idade&quot; mean(df$idade) ## [1] 23.33333 summary(df) # Obtendo uma visão estatística ampla da tabela ## idade genero raça ## Min. :12.00 Length:3 Length:3 ## 1st Qu.:17.50 Class :character Class :character ## Median :23.00 Mode :character Mode :character ## Mean :23.33 ## 3rd Qu.:29.00 ## Max. :35.00 summary(df$genero) # sumário da coluna &quot;gênero&quot; do dataset &quot;df&quot; ## Length Class Mode ## 3 character character summary(df$idade) # sumário da coluna &quot;idade&quot; do dataset &quot;df&quot; ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 12.00 17.50 23.00 23.33 29.00 35.00 # testando se valores da coluna &quot;genero&quot; são &quot;fem&quot; df$genero==&quot;fem&quot; ## [1] TRUE FALSE TRUE # Filtrando apenas as linhas onde a coluna &quot;genero&quot; for &quot;fem&quot;. # Explicando de outro modo: quando &quot;genero&quot; igual a &quot;fem&quot; for verdadeiro, imprima df[df$genero==&quot;fem&quot;,] ## idade genero raça ## 1 12 fem pret ## 3 35 fem pard # Filtrando apenas as linhas onde a escolaridade for maior que 12. df[df$idade &gt; 12,] ## idade genero raça ## 2 23 mas branc ## 3 35 fem pard 4.3.6 Considerações finais sobre estrutura de dados Estes não são os únicos tipos de estrutura de dados no R. Alguns pacotes podem criar o seu próprio tipo e não convém tentar cobrir todos. Apesar de considerar tipos diferentes, podemos considerar os diferentes tipos como subcategorias de outras. Data frames são um tipo restrito de listas. Listas, por sua vez, são um tipo de vetor. Veremos a seguir a suíte de pacotes do tidyverse que possui formato próprio de dados. 4.4 A suíte de pacotes tidyverse EM CONSTRUÇÃO "],["análise-de-redes-sociais.html", "Capítulo 5 Análise de Redes Sociais 5.1 O pacote Igraph 5.2 Os pacotes ggraph e tidygraph . Construindo grafos com o tidyverse. 5.3 Redes de palavras 5.4 Redes de citação 5.5 Gráfico de centralidade 5.6 Comunidades 5.7 Sugestões de links", " Capítulo 5 Análise de Redes Sociais O R possui diversos pacotes para análise de rede, como o igraph, statnet, e do tidyverse temos tydygraph e ggraph. 5.1 O pacote Igraph Instalando o pacote igraph: install.packages(&quot;igraph&quot;) # instalando o pacote chamando o pacote já instalado library(igraph) # chamando o pacote já instalado ## ## Attaching package: &#39;igraph&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## decompose, spectrum ## The following object is masked from &#39;package:base&#39;: ## ## union Pegando o famoso poema de Drummond: “João amava Teresa que amava Raimundo que amava Maria que amava Joaquim que amava Lili que não amava ninguém” E o transformando em um grafo (os “gráficos” em análise de rede recebem este nome): library(igraph) g &lt;- graph.empty(directed=TRUE) # &quot;directed&quot; implica distinguir entre &quot;de&quot; e &quot;para&quot; na relação entre os nós. # Adicionando os vértices. g &lt;- g + vertex(&quot;João&quot;) g &lt;- g + vertex(&quot;Teresa&quot;) g &lt;- g + vertex(&quot;Raimundo&quot;) g &lt;- g + vertex(&quot;Maria&quot;) g &lt;- g + vertex(&quot;Joaquim&quot;) g &lt;- g + vertex(&quot;Lili&quot;) # Especificando as relações entres os vértices, os edges g &lt;- g + edges(&quot;João&quot;, &quot;Teresa&quot;) g &lt;- g + edges(&quot;Teresa&quot;, &quot;Raimundo&quot;) g &lt;- g + edges(&quot;Raimundo&quot;, &quot;Maria&quot;) g &lt;- g + edges(&quot;Maria&quot;, &quot;Joaquim&quot;) g &lt;- g + edges(&quot;Joaquim&quot;, &quot;Lili&quot;) plot.igraph(g) # plotando o grafo Neste caso, poderíamos ter feito este mesmo grafo com código mais compacto: library(igraph) g &lt;- graph.empty(directed=TRUE) # Adicionando os vértices. g &lt;- g + vertex(c(&quot;João&quot;, &quot;Teresa&quot;, &quot;Raimundo&quot;, &quot;Maria&quot;, &quot;Joaquim&quot;, &quot;Lili&quot;)) # Adicionando os edges em pares g &lt;- g + edges(c(&quot;João&quot;, &quot;Teresa&quot;, &quot;Teresa&quot;, &quot;Raimundo&quot;, &quot;Raimundo&quot;, &quot;Maria&quot;,&quot;Maria&quot;, &quot;Joaquim&quot;,&quot;Joaquim&quot;, &quot;Lili&quot;)) plot.igraph(g) Se o grafo sobe ou desce, pouco importa para nós aqui, importa as pessoas e as relações entre elas. Repare que os edges são entendidos aos pares. Se fizéssemos um vetor sem as devidas repetições, teríamos um gráfico errado das relações: g &lt;- graph.empty(directed=TRUE) g &lt;- g + vertex(c(&quot;João&quot;, &quot;Teresa&quot;, &quot;Raimundo&quot;, &quot;Maria&quot;, &quot;Joaquim&quot;, &quot;Lili&quot;)) g &lt;- g + edges(c(&quot;João&quot;, &quot;Teresa&quot;, &quot;Raimundo&quot;, &quot;Maria&quot;,&quot;Joaquim&quot;, &quot;Lili&quot;)) plot.igraph(g) Ou com código modo mais econômico ainda: g &lt;- graph.formula( João --+ Teresa --+ Raimundo --+ Maria --+ Joaquim --+ Lili ) plot(g) 5.1.1 Clusterização EM CONSTRUÇÃO 5.2 Os pacotes ggraph e tidygraph . Construindo grafos com o tidyverse. O pacote ggraph é um pacote elaborado por Thomas Lin Pedersen, o mesmo do ggplot2, e pretende ser uma extensão deste, usando a mesma gramática de gráficos, o que nos dá grande flexibilidade visual. Com o ggraph é possível construir graficamente redes, mas ele vai além dos grafos, construindo também dendogramas, diferentes tipos de árvores, matrizes, gráficos hierárquicos, diagrama de arc, sunburst, etc. Para inserir os dados no ggraph, é necessário colocá-lo no formato “tidy” do “tidyverse”, e fazemos isso com o pacote tidygraph: Para instalar, usamos os comandos: install.packages(&#39;ggraph&#39;) install.packages(&#39;tidygraph&#39;) Carregando os pacotes library(ggraph) ## Loading required package: ggplot2 ## ## Attaching package: &#39;ggplot2&#39; ## The following object is masked from &#39;package:NLP&#39;: ## ## annotate library(tidygraph) ## ## Attaching package: &#39;tidygraph&#39; ## The following object is masked from &#39;package:igraph&#39;: ## ## groups ## The following object is masked from &#39;package:stats&#39;: ## ## filter Seguindo o exemplo na página do criador do GGraph Thomas Lin Pedersen, vamos usar o dataset highschool do ggraph, que contém dados sobre a evolução da amizade entre garotos numa escola do ensino médio no Illinois, que nos anos de 1957 e 1958 responderam à pergunta: “Com que colegas desta escola você anda mais frequentemente?”. Esta pesquisa apareceu originalmente nos livros “Introduction to Mathematical Sociology” e “The Adolescent Society”, ambos do sociólogo James Coleman. Para obter mais informações sobre este dataset, basta digitar no console: help(highschool). str(highschool) # observando a estrutura do data frame ## &#39;data.frame&#39;: 506 obs. of 3 variables: ## $ from: num 1 1 1 1 1 2 2 3 3 4 ... ## $ to : num 14 15 21 54 55 21 22 9 15 5 ... ## $ year: num 1957 1957 1957 1957 1957 ... head(highschool,10) # observado as primeiras linhas do data frame ## from to year ## 1 1 14 1957 ## 2 1 15 1957 ## 3 1 21 1957 ## 4 1 54 1957 ## 5 1 55 1957 ## 6 2 21 1957 ## 7 2 22 1957 ## 8 3 9 1957 ## 9 3 15 1957 ## 10 4 5 1957 Vemos que há na coluna 1 de (“from”) pessoa número X para (“to”) para pessoa Y no ano (“year”). Assim, a pessoa 1 teve contato com as pessoas 14, 15, 21, 54 e 55 em 1957. A pessoa 2 teve contato com as pessoas 21 e 22, e assim por diante. Vamos preparar os dados para plotar o grafo com ggraph. Antes, é necessário colocá-lo no formato “tidy” do “tidyverse”, e fazemos isso com o pacote tidygraph. A função as_tbl_graph() do pacote tidygraph funciona como a função grouped_df(), que agrupa nós (nodes) e arestas (edges). as_tbl_graph(highschool) ## # A tbl_graph: 70 nodes and 506 edges ## # ## # A directed multigraph with 1 component ## # ## # Node Data: 70 x 1 (active) ## name ## &lt;chr&gt; ## 1 1 ## 2 2 ## 3 3 ## 4 4 ## 5 5 ## 6 6 ## # … with 64 more rows ## # ## # Edge Data: 506 x 3 ## from to year ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 13 1957 ## 2 1 14 1957 ## 3 1 20 1957 ## # … with 503 more rows Vamos criar uma lista chamada “grafo”, adicionando um campo novo com mutate(), de nome “Popularidade” e que vai medir a centralidade de grau através da função centrality_degree() do tidygraph. Centralidade de grau é a medida mais simples de centralidade, que conta o número de conexões, as arestas (“edges”) de cada nó. grafo &lt;- as_tbl_graph(highschool) %&gt;% mutate(Popularidade = centrality_degree(mode = &#39;in&#39;)) grafo ## # A tbl_graph: 70 nodes and 506 edges ## # ## # A directed multigraph with 1 component ## # ## # Node Data: 70 x 2 (active) ## name Popularidade ## &lt;chr&gt; &lt;dbl&gt; ## 1 1 2 ## 2 2 0 ## 3 3 0 ## 4 4 4 ## 5 5 5 ## 6 6 2 ## # … with 64 more rows ## # ## # Edge Data: 506 x 3 ## from to year ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 13 1957 ## 2 1 14 1957 ## 3 1 20 1957 ## # … with 503 more rows Plotando o grafo: ggraph(grafo, layout = &#39;kk&#39;) + geom_edge_fan(aes(colour = stat(index)), show.legend = FALSE) + geom_node_point(aes(size = Popularidade, colour= Popularidade), show.legend = TRUE) + scale_colour_gradient(low = &quot;steelblue&quot;, high = &quot;black&quot;) + facet_edges(~year) + theme_graph(foreground = &#39;steelblue&#39;, fg_text_colour = &#39;yellow&#39;) + labs(title = &quot;Evolução da amizade entre adolescentes de uma escola no Illinóis&quot;, caption = &quot;Fonte: Elaboração própria a partir dos dados de Coleman(1961) apud ggraph (v.2.0.5; Pedersen)&quot;) layout define como os nós serão alocados. O layout do ggraph possui os mesmos do igraph e outros mais como, hive plots, treemaps e circle packing. kk indica que está sendo usado o algortimo Kamada-Kawai para dispersar os nós e facilitar nossa visualização. geom_edge_fan() desenha os laços de modo curvo. Há diversas outras opções possíveis neste caso, como: geom_edge_arc, geom_edge_bend, geom_edge_diagonal,geom_edge_elbow, geom_edge_fan, geom_edge_hive, geom_edge_link, geom_edge_parallel. Substitua geom_edge_fan() por alguns estes e vjea a diferença no grafo. colour indica que a intensidade das ligações será por um gradiente de cor, stat() indica que segue ali uma informação estatística. geom_node_point() mostra os nós como pontos/círculos e permite que sejam plotados em diferentes tamanhos, cores e formas em aes(). size = Popularidade indica que o tamanho dos nós é controlado pela variável “Popularidade” que criamos. colour = Popularidade indica que além do tamanho, a cor também vaira conforme a popularidade. Para mais opções, digite ?geom_node_point() no console ou consulte a documentação do ggraph. scale_colour_gradient(low = \"steelblue\", high = \"black\") é opcional, podendo ser retirada. Especifica o gradiente de cores do comando anterior (no caso, nós), qual o valor mais baixo até o mais elevado. Sua informação é redundante com relação ao tamanho dos nós, mas ajuda a reforçar tal informação. facet_edges() função de “faceting”, de criar facetas, gráficos multiplos, e o símbolo de til ~ seguido de year indica que o critério aqui são as categorias dentro de ano, que no caso, são 1957 e 1958. Os nós são repetidos em cada painel. Caso tivéssesmos usado facet_edges(1957) teríamos o grafo apenas do ano 1957. theme_graph especifica as cores das legendas nas facetas labs(title = \" indica o título e caption o rodapé. Veja também cheatsheet/folha de dicas do ggraph Para ver os diferentes layouts possíves do ggraph e seus respectivos códigos, para além dos grafos clique aqui CAPÍTULO A SER EXPANDIDO 5.3 Redes de palavras EM CONSTRUÇÃO 5.4 Redes de citação EM CONSTRUÇÃO 5.5 Gráfico de centralidade EM CONSTRUÇÃO 5.6 Comunidades EM CONSTRUÇÃO 5.7 Sugestões de links AQUINO, Jackson A. “Análise de redes sociais”, capítulo 12 de ___. R para cientistas sociais. Ilhéus, BA: EDITUS, 2014. 157 p. ISBN: 978-85-7455-369-6. (PDF gratuito bem introdutório com R) HIGGINS, Silvio S.; RIBEIRO, Antônio Carlos. Análise de redes em Ciências Sociais. Brasília: ENAP. 2018. 229p. (PDF Gratuito de livro. Bom para aprender os conceitos/teorias básicos de análise de rede). Manual online do ipgraph para R; PDF do Manual do igraph para R (ambos em inglês) ggraph Documentation no Cran. d’ANDRÉA, Carlos Frederico de Brito. Pesquisando plataformas online: conceitos e métodos. EDUFBA. 2020. A obra visa introduzir os Estudos de Plataforma, um campo de estudos que, desde o início da década de 2010, discute as especificidades políticas e materiais das mídias sociais e de outras plataformas online. Datificação, algoritmos, governança e os modelos de negócio das plataformas são algumas das dimensões sintetizadas no livro. De modo didático, o autor apresenta um conjunto de leituras e de experimentações metodológicas conduzidas com um diversificado grupo de colaboradoras(es) no país e no exterior. (ebook PDF e Epub gratuitos) RECUERO, Raquel. Introdução à análise de redes sociais online. EDUFBA.2017. A Análise de Redes Sociais (ARS) é uma abordagem de pesquisa cuja popularidade tem aumentado nos últimos anos, principalmente, entre os pesquisadores da área de Comunicação. É nesse âmbito que várias obras, entre artigos e livros, vêm surgindo e introduzindo o estudo dessas estruturas a partir da análise de redes e da compreensão da representação dessas redes sociais na internet. Este livro é uma pequena compilação dos principais conceitos e elementos para a compreensão e a aplicação da ARS. É baseado em uma breve apresentação e histórico do paradigma, os principais conceitos, suas métricas e, finalmente, suas formas de representação e visualização. (ebook PDF e Epub gratuitos). LIZARDO Omar; JILBERT Isaac. Social Networks: An Introduction. 2021. (ebook online) "],["links-úteis.html", "Capítulo 6 Links úteis 6.1 Humanidades digitais 6.2 Análise de redes sociais 6.3 Textos sobre análise textual 6.4 Processamento Linguagem Natural (PLN ou NLP) 6.5 Revistas / Journals acadêmicos 6.6 Sobre Dados Abertos 6.7 Vídeos 6.8 Sites / Blogs 6.9 Organizações 6.10 Links de Podcasts sobre Digital Humanities 6.11 Links Programação 6.12 Links de Cursos Online 6.13 Grupos de discussão/Forum 6.14 Links de folhas de dicas (Cheat-sheets)", " Capítulo 6 Links úteis Segue aqui uma lista com links para livros, artigos, sites, manutais, tutoriais, vídeos e canais de vídeos. A maioria e a prioridade é de material gratuito, sobre temas gerais relacionados a humanidades digitais e em especial, à sociologia. Há também sugestões mais focadas nos assuntos específicos nos capítulos. O critério de seleção aqui foi o de contribuição para esclarecer o potencial das humanidades digitais para quem inicia na área de humanidades digitais/sociologia digital. Os itens não foram selecionados tanto pela contribuição científica, mas com base no critério de exemplos que podem ajudar a esclarecer o potencial das humanidades digitais para quem inicia. 6.1 Humanidades digitais Introduction to Digital Humanities Textbook A digital textbook designed for UCLA’s Introduction to Digital Humanities course website. (ebook disponível gratuito online) EDMOND, Jennifer Digital Technology and the Practices of Humanities Research. (Livro gratuito) The Data Journalism Handbook. Towards a Critical Data Practice. 415p. 2021. ISBN 9789048542079. (Livro GRATUITO). BAUER, Paul C. Computational Social Science: Theory &amp; Application. (livro online grátis). 6.2 Análise de redes sociais Ver sugestões no capítulo “Análise de Redes Sociais”. 6.3 Textos sobre análise textual GRIMMER, Justin.STEWART, Brandom. Text as Data: The Promise and Pitfalls of Automatic ContentAnalysis Methods for Political Texts. Political Analysis(2013) pp. 1–31. doi:10.1093/pan/mps028. CASTELFRANCHI, Yurij. A análise de textos auxiliada pelo computador: um laboratório a céu aberto para as ciências sociais. Journal of Science Communication 16(02)(2017)C04 TREADWELL, Donald. Content Analysis: Understanding Text and Image in Numbers. Understanding Text and Image in Numbers. In __ Introducing Communication Research: paths of Inquiry. Sage. 2014. (Capítulo sobre análise de conteúdo) 6.3.1 Sociologia Digital KATEMBERA, Serge. “Sociologia digital ou sociologia do digital?” V. 2 N. 1 (2020): Dossiê Ambiente e Sociedade. (Artigo) NASCIMENTO, Leonardo F. Sociologia digital : uma breve introdução. EDUFBA. 2020. (Livro gratuito em PDF e epub) Podcast “New Work in Digital Humanities”. Episódio: Neil Selwyn, “What is Digital Sociology?” (Polity, 2019) BERNAU, John A. Text Analysis with JSTOR Archives. November 12, 2018 https://doi.org/10.1177/2378023118809264. (Aplicação de métodos de humanidades digitais, usando a base do Jstor, para uma rápida análise de variação temática em revistas sociológicas). FUSSEY, Pete e ROTH, Silke (ed.) Digitizing Sociology. edição da revista “Sociology” da British Sociological Association. 6.3.1.1 Netnografia, etnografia digital MARKHAM, Annette N. 2013. Fieldwork in Social Media. Qualitative Communication Research 2, 4 (Dec. 2013), 434–446. https://doi.org/10.1525/qcr.2013.2.4.434 SHAFFER, David Williamson. Quantitative Etnography. Boswell Press. 2017. “This is a book about understanding why, in the digital age, the old distinctions between qualitative and quantitative research methods, between the sciences and humanities, and between numbers and understanding, limit the kinds of questions we can ask, in some cases, and lead us accept superficial answers in others. Quantitative Ethnography is a research method that goes beyond those distinctions to help us understand how to make sense of our increasingly data-rich world…\". (PDF da Introdução disponível gratuitamente) 6.4 Processamento Linguagem Natural (PLN ou NLP) JURAFSKY, D.; MARTIN, J. Speech and language processing: An introduction to speech recognition, computational linguistics and natural language processing. Upper Saddle River, NJ: Prentice Hall, 2020. link pdf dos capítulos individuais, link livro completo. (Um manual bastante extenso e mais teórico sobre PLN) BIRD, S.; KLEIN, E.; LOPER., E. Natural language processing with Python – analyzing text with the natural language toolkit. (Livro online gratuito, baseado em Python 3.) Playlist no Youtube das aulas da Stanford University sobre PLN 6.5 Revistas / Journals acadêmicos SIGCAS Computers and Society da Association for Computing Machinery Digital Scholarship in the Humanities. Revista da Universidade de Oxford. Sessão com artigos gratuitos Journal: Digital humanities Quartely International Journal of Digital Humanities Socius: Sociological Research for a Dynamic World. Special Collection: Data Visualization. Jornal de acesso livre International Journal of Humanities and Arts Computing (Universidade de Edimburgo) magazén International Journal for Digital and Public Humanities. (open access). Revista do Dipartimento di Studi Umanistici da Università Ca’ Foscari de Veneza Journal of Digital Social Research. Open Access Reviews in Digital Humanities. (O Review já não publica desde 2014) Journal of Digital Humanities. (O Journal of DH tem uma sessão de resenha de novas ferramentas disponíveis). Journal of Cultural Analytics. Department of Languages, Literatures, and Cultures at McGill University, Canada. open-access journal dedicated to the computational study of culture 6.6 Sobre Dados Abertos 5 estrelas dos dados abertos (Tim Berners-Lee, criador do termo “dados abertos”. Site explica o que são dados abertos e seus 5 níveis. Em português) Busca do Google por base de dados Abertos Dados abertos. (Se não sabe onde encontrar algum dado específico que você procura, veja aqui) SHIKIDA, Claudio D., MONASTERIO, Leonardo, NER, Pedro Fernando. Guia Brasileiro de Análise de Dados: Armadilhas e Soluções. Brasília. 2021. ( Tópicos: Causalidade, Pobreza e Desigualdade, Análise de dados em Saúde, Educação, Crimes e Violência, Macroeconomia, Mercado e Trabalho e Opinião Pública.) Dados Abertos: Fórum de discussão. Workshop do Henrique Xavier, no canal “Base dos Dados” sobre como explorar os dados do Diário Oficial da União. Como usar a biblioteca basedosdados no R. Pacote basedosdados do R para baixar diversas bases de dados. Data sets do Brasil.io, um “repositório de dados públicos disponibilizados em formato acessível” como “Eleições Brasil”, “Cursos e notas de corte do Prouni 2018”, gastos de deputados e do governo federal, salários de magistrados, etc. 6.7 Vídeos Vídeos do 1º Summerschool of Digital Humanities da Universidade de Heidelberg, Alemanha, ocorrido em 2017. link. (Em inglês) 6.8 Sites / Blogs Text analysis info Textual Analysis - University of Notre Dame Site Digital Humanities Now. (Agrega oportunidade de emprego, notícias, bolsas de pesquisa). site Go Digital Humanities. (Novidades sobre humanidades digitais, como eventos). Blog Digital Society Blog. (Blog do Institut für Internet und Gesellschaft da Alexander von Humboldt). 6.9 Organizações Alliance of Digital Humanities Organizations (ADHO). Global Network of Internet and Society Research Centers (NoC) i. (Catálogo com grupos de pesquisa sobre internet e sociedade ao redor do mundo). 6.10 Links de Podcasts sobre Digital Humanities Complexity: Peter Dodds on Text-Based Timeline Analysis &amp; New Instruments for The Science of Stories New Work in Digital Humanities. New Books Network. Interviews with digital humanists about their new work. Lista com mais podcasts dedicados às digital humanities 6.11 Links Programação 6.11.1 R introdução LENTE, Caio. Zen do R. “O objetivo deste livro é ensinar ao leitor que não costuma programar algumas formas simples de melhorar a organização de seus projetos de análise de dados em R”. (Livro online gratuito). AQUINO, Jackson A.. R para cientistas sociais. Ilhéus, BA: EDITUS, 2014. 157 p. ISBN: 978-85-7455-369-6. (PDF gratuito) FERREIRA, E.B.;. de OLIVEIRA, M.S. Introdução à Estatística com R. Unifal. 2020. (PDF gratuito). Livro Curso-R (em construção) Blog: Curso-R KUBRUSLY, Jessica. Uma Introdução à Programação com o R. (ebook online em português) FREIRE, Sergio Miranda. Introdução ao R. (Livro online, no formato bookdown) WICKHAM, Hadley. Advanced R. 2nd edition. (Livro online gratuito, no formato bookdown, em inglês, para entender melhor os conceitos do R). 6.11.2 R - tópicos específicos WICKHAM, H. GROLEMUND, Garret. R for Data Science. O’Reilly. 2017. (Livro Online, em inglês). IRIZARRY, Rafael A. Introduction to Data Science. Livro Online, em inglês, feito como notas de aulas da HarvardX Data Science Series CLARK, Michael. R &amp; Social Science. Getting Started with applied use of R in the Social Sciences. (pequeno manual em PDF da Universidade de Notre Dame) ggplot2 on-line version of work-in-progress 3rd edition of “ggplot2: elegant graphics for data analysis” SILGE, Julia, ROBINSON, David Text Mining with R. (ebook online) RYDBERG-COX, Jeff. Statistical Methods for Studying Literature Using R da Universidade Missouri-Kansas City. R-tutor- An R Introduction to Statistics. Site com tutoriais diversos sobre R. Em Inglês. Diversos links para livros gratuitos de R no bookdown. 6.11.3 Links Python HEINOLD, Brian. A Practical Introduction to Python Programming. 2012. 263 p. (PDF de introdução ao Python) Jake VanderPlas. Python Data Science Handbook: Essential Tools for Working with data. (livro online). 6.11.4 Python - tópicos específicos KARSDORP, Folgert. Python Programming for the Humanities. Livro online. Pyhton Humanities. Site com introdução às DH em Python, com alguns tutoriais. Constellate What do you want to learn today?. (Projeto Constellate da JStor junto a diversas universidades. Conta com sessão com tutoriais sobre análise textual, a maioria em Python. 6.12 Links de Cursos Online EdX, Cursos online de Universidades como Harvard, MIT, etc. gratuitos como ouvinte. Paga-se pelo certificado. Há cursos grautuitos, como “Introducing Text Analytics and Natural Language Processing with Python”, “Introduction to Digital Humanities”, “Data Science: Visualization”, “Using Python for Research”. DataCamp. Cursos mais práticos, em R, Python, SQL, com exercícios, totalmente online (não é necessário instalar nada em seu computador). As partes iniciais dos cursos costumam ser gratuitas, mas há parte paga com anuidades. Udemy Possui cursos gratuitos e cursos pagos. É possível encontrar cursos pagos por volta de R$20,00. Coursera. Possui parceria com mais de 200 universidades e empresas como o Google e IBM. Cognitive Class Da IBM, Cursos gratuitos em data science, alguns gratuitos com certificado. Cursos em inglês e espanhol. Possui cursos como “Data Visualization with R”. Big Data University. Cursos em Portugues. Curso gratuito de Estatística e probabilidade da Khan Academy, em português. Inclui teste de hipótese, regressão. 6.13 Grupos de discussão/Forum 6.13.1 Telegram Processamento de Linguagem Natural em Português PT-Br Data Science - Python R Brasil R humanidades Análise Textual-Humanidades Digitais 6.14 Links de folhas de dicas (Cheat-sheets) “Cheat sheets” seriam a tradução para “cola”, aquela feita para consulta em exames na escola. Em programação, refere-se a uma tabela muito bem resumida, com o que há de essencial em determinado assunto. Text Analysis Glossary Um glossário de termos usados na análise textual, em inglês, do projeto Constellate (JStor e diversas universidades) Lista com mais de 100 cheat-sheets em R e Python, sobre Machine Learning: link Git-GitHub, Git-Gitlab, Cheatsheets do R como: Base R, Base R em portguês, Data-table em português, Data Import, Data e horários com Lubridate, trabalhe com mais facilidade com listas e funções com o purr, stringr, dplyr, R Reference Card, Usar Python no R com reticulate, ggplot, R Markdown com o R Markdown Cheatsheet ou o Reference Guide, Regular Expressions, R Programming por Arianne Colton e Sean Chen, Uma lista com estes e outros cheatsheets aqui do Rstudio aqui. "]]
